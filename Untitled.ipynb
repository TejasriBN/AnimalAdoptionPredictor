{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-ee4bdf5a2e92>, line 4855)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-ee4bdf5a2e92>\"\u001b[1;36m, line \u001b[1;32m4855\u001b[0m\n\u001b[1;33m    coefficients_ = optR.coefficiimport json\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error,log_loss\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split,KFold,GroupKFold,StratifiedShuffleSplit\n",
    "import cv2\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression,SGDClassifier,SGDRegressor,LinearRegression,Ridge,PassiveAggressiveClassifier,PassiveAggressiveRegressor,BayesianRidge\n",
    "from wordbatch.models import FTRL,FM_FTRL\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag\n",
    "from sklearn.svm import LinearSVR,LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications import InceptionResNetV2, InceptionV3\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "from catboost import Pool, CatBoostClassifier,CatBoostRegressor\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import jieba\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack, vstack\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.sparse import hstack\n",
    "import featuretools as ft\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "from PIL import Image\n",
    "import pprint\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "import gensim\n",
    "from keras.utils import to_categorical\n",
    "from itertools import product\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(369)\n",
    "rn.seed(369)\n",
    "tf.set_random_seed(1234)\n",
    "img_size = 256\n",
    "batch_size = 16\n",
    "embed_size=128\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "stops = {x: 1 for x in stopwords.words('english')}\n",
    "start_time=time.time()\n",
    "\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        '''\n",
    "        >>>example:\n",
    "        mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['regionidcity',\n",
    "                          'regionidneighborhood', 'regionidzip'],\n",
    "                target_type='regression'\n",
    "                )\n",
    "\n",
    "        X = mean_encoder.fit_transform(X, pd.Series(y))\n",
    "        X_test = mean_encoder.transform(X_test)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "\n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new\n",
    "\n",
    "def get_embedding_matrix(word_index,embed_size=embed_size, Emed_path=\"w2v_128.txt\"):\n",
    "    embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        Emed_path, binary=False)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            count += 1\n",
    "    return embedding_matrix\n",
    "    \n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay') # decoupled weight decay (2/4)\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p # decoupled weight decay (4/4)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n",
    "    \n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.erf(x / tf.sqrt(2.0)))\n",
    "    return x * cdf\n",
    "    \n",
    "def lan_type(desc):\n",
    "    desc = str(desc)\n",
    "    if desc=='nan':\n",
    "        return 0\n",
    "    zh_model = re.compile(u'[\\u4e00-\\u9fa5]')    \n",
    "    en_model = re.compile(u'[a-zA-Z]')  \n",
    "    zh_match = zh_model.search(desc)\n",
    "    en_match = en_model.search(desc)\n",
    "    if zh_match and en_match:\n",
    "        return 3  \n",
    "    elif zh_match:\n",
    "        return 3  \n",
    "    elif en_match:\n",
    "        return 2  \n",
    "    else:\n",
    "        return 1  \n",
    "\n",
    "def malai_type(desc):\n",
    "    desc = str(desc)\n",
    "    malai = [' la x ' , ' nk ',' nie ', ' umur ', ' di ', 'teruk', ' satu ',' dh ', ' ni ',' tp ', ' yg ', 'mmg', 'msj', ' utk ' ,'neh' ]\n",
    "    for tag in malai:\n",
    "        if desc.find(tag) > -1:\n",
    "            return 1\n",
    "    \n",
    "    return  0\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().strip()\n",
    "    for s in string.punctuation:\n",
    "        text = text.replace(s, ' ')\n",
    "    text = text.strip().split(' ')\n",
    "    return u' '.join(x for x in text if len(x) > 1 and x not in stops)\n",
    "    \n",
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image\n",
    "\n",
    "def img_model():\n",
    "    K.clear_session()\n",
    "    inp = Input((img_size, img_size, 3))\n",
    "    x = DenseNet121(\n",
    "            include_top=False, \n",
    "            weights=\"../input/keras-pretrain-model-weights/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\", \n",
    "            input_shape=(img_size, img_size, 3))(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Lambda(lambda x: K.expand_dims(x, axis = -1))(x)\n",
    "    x = AveragePooling1D(4)(x)\n",
    "    out = Lambda(lambda x: x[:, :, 0])(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    return model\n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = cohen_kappa_score(y, X_p,weights='quadratic')\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y,initial_coef=[0.5, 1.5, 2.5, 3.5]):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "#         initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef,len_0):\n",
    "        X_p = np.copy(X)\n",
    "        temp = sorted(list(X_p))\n",
    "        threshold=temp[int(0.95*len_0)-1]\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < threshold:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= threshold and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "\n",
    "def get_class_bounds(y, y_pred, N=5, class0_fraction=-1):\n",
    "    \"\"\"\n",
    "    Find boundary values for y_pred to match the known y class percentiles.\n",
    "    Returns N-1 boundaries in y_pred values that separate y_pred\n",
    "    into N classes (0, 1, 2, ..., N-1) with same percentiles as y has.\n",
    "    Can adjust the fraction in Class 0 by the given factor (>=0), if desired. \n",
    "    \"\"\"\n",
    "    ysort = np.sort(y)\n",
    "    predsort = np.sort(y_pred)\n",
    "    bounds = []\n",
    "    for ibound in range(N-1):\n",
    "        iy = len(ysort[ysort <= ibound])\n",
    "        # adjust the number of class 0 predictions?\n",
    "        if (ibound == 0) and (class0_fraction >= 0.0) :\n",
    "            iy = int(class0_fraction * iy)\n",
    "        bounds.append(predsort[iy])\n",
    "    return bounds\n",
    "\n",
    "def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n",
    "    kf = StratifiedKFold(n_splits=5, random_state=1017, shuffle=True)\n",
    "#     kf = GroupKFold(n_splits=5)\n",
    "#     fold_splits = kf.split(train, target,group)\n",
    "#     kf = StratifiedShuffleSplit(n_splits=10, test_size=0.45,  random_state=1017)\n",
    "    fold_splits = kf.split(train, target)\n",
    "    folds=5\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    pred_full_test = 0\n",
    "    log_list=[]\n",
    "    pred_train = np.zeros((train.shape[0], folds))\n",
    "    all_coefficients = np.zeros((folds, 4))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print( label + ' | FOLD ' + str(i) + '/'+str(folds))\n",
    "        if isinstance(train, pd.DataFrame):\n",
    "            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        else:\n",
    "            dev_X, val_X = train[dev_index], train[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        params2 = params.copy()\n",
    "        pred_val_y, pred_test_y, importances, coefficients, qwk,log = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        all_coefficients[i-1, :] = coefficients\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            qwk_scores.append(qwk)\n",
    "            log_list.append(log)\n",
    "            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n",
    "            print(\"##\"*40)\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = train.columns.values\n",
    "        fold_importance_df['importance'] = importances\n",
    "        fold_importance_df['fold'] = i\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n",
    "        i += 1\n",
    "#     print('{} cv RMSE scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std RMSE score : {}'.format(label, np.std(cv_scores)))\n",
    "#     print('{} cv QWK scores : {}'.format(label, qwk_scores))\n",
    "    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n",
    "    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n",
    "    print('{} cv mean log_loss score : {}'.format(label, np.mean(log_list)))\n",
    "    \n",
    "    pred_full_test = pred_full_test / float(folds)\n",
    "    results = {'label': label,\n",
    "               'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores, 'qwk': qwk_scores,\n",
    "               'importance': feature_importance_df,\n",
    "               'coefficients': all_coefficients}\n",
    "    return results\n",
    "\n",
    "def get_feat1():\n",
    "    train_data = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    \n",
    "    def deal_breed(df):\n",
    "        if df['Breed1']==df['Breed2']:\n",
    "            df['Breed2']=0\n",
    "        if df['Breed1']!=307 & df['Breed2']==307:\n",
    "            temp=df[\"Breed1\"]\n",
    "            df['Breed1']=df['Breed2']\n",
    "            df['Breed2']=temp\n",
    "        return df\n",
    "    \n",
    "    train_data=train_data.apply(lambda x:deal_breed(x),1)\n",
    "    test_data=test_data.apply(lambda x:deal_breed(x),1)\n",
    "    \n",
    "    def get_purebreed_feat(df):\n",
    "        if df['Breed2']==0 and df['Breed1']!=307:\n",
    "            return 1\n",
    "        return 0\n",
    "    train_data['purebreed']=train_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    test_data['purebreed']=test_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    \n",
    "    train_data['is_group']=train_data.Gender.apply(lambda x:1 if x==3 else 0,1)\n",
    "    test_data['is_group']=test_data.Gender.apply(lambda x:1 if x==3 else 0,1)\n",
    "    \n",
    "    def get_good_cnt(df):\n",
    "        cnt=0\n",
    "        for i in ['Vaccinated',\"Dewormed\",\"Sterilized\",\"Health\"]:\n",
    "            if df[i]==1:\n",
    "                cnt+=1\n",
    "        return cnt\n",
    "    \n",
    "    train_data['good_cnt']=train_data.apply(lambda x:get_good_cnt(x),1)\n",
    "    test_data['good_cnt']=test_data.apply(lambda x:get_good_cnt(x),1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_data['lan_type'] = train_data.Description.map(lambda x:lan_type(x))\n",
    "    train_data['malai_type'] = train_data.Description.map(lambda x:malai_type(x))\n",
    "    \n",
    "    test_data['lan_type'] = test_data.Description.map(lambda x:lan_type(x))\n",
    "    test_data['malai_type'] = test_data.Description.map(lambda x:malai_type(x))\n",
    "    \n",
    "    def name_deal(df):\n",
    "        if \"No Name\" in df:\n",
    "            return np.nan\n",
    "        if df ==\"nan\":\n",
    "            return np.nan\n",
    "        else:\n",
    "            return df\n",
    "    train_data['Name'] = train_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    test_data['Name'] = test_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    \n",
    "    train_data['Name'],indexer=pd.factorize(train_data['Name'])\n",
    "    test_data['Name'] = indexer.get_indexer(test_data['Name'])\n",
    "    \n",
    "    rescuer_df=train_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    train_data=pd.merge(train_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    # train_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    rescuer_df=test_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    test_data=pd.merge(test_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    # test_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    del rescuer_df\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['rescuer_rank'] = train_data['RescuerID'].map(train_data['RescuerID'].value_counts().rank()/len(train_data['RescuerID'].unique()))\n",
    "    test_data['rescuer_rank'] = test_data['RescuerID'].map(test_data['RescuerID'].value_counts().rank()/len(test_data['RescuerID'].unique()))\n",
    "    \n",
    "    def get_res_feat(df):\n",
    "        temp=pd.DataFrame(index=range(1))\n",
    "        temp['RescuerID']=df['RescuerID'].values[0]\n",
    "        temp['res_type_cnt']=len(df['Type'].unique())\n",
    "        temp['res_breed_cnt']=len(df['Breed1'].unique())\n",
    "        temp['res_breed_mode']=df['Breed1'].mode()\n",
    "        temp['res_fee_mean']=df['Fee'].mean()\n",
    "        temp['res_Quantity_sum']=df['Quantity'].sum()\n",
    "        temp['res_MaturitySize_mean']=df['MaturitySize'].mean()\n",
    "        temp['res_Description_unique']=len(df['Description'].unique())\n",
    "        return temp\n",
    "    train_res_feat=train_data.groupby(\"RescuerID\",as_index=False).apply(lambda x:get_res_feat(x))\n",
    "    test_res_feat=test_data.groupby(\"RescuerID\",as_index=False).apply(lambda x:get_res_feat(x))\n",
    "    train_res_feat.index=range(len(train_res_feat))\n",
    "    test_res_feat.index=range(len(test_res_feat))\n",
    "    train_data = pd.merge(train_data,train_res_feat,on=\"RescuerID\",how=\"left\")\n",
    "    test_data = pd.merge(test_data,test_res_feat,on=\"RescuerID\",how=\"left\")\n",
    "    train_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    test_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    train_data['fee-mean_fee']=train_data['Fee']-train_data['res_fee_mean']\n",
    "    test_data['fee-mean_fee']=test_data['Fee']-test_data['res_fee_mean']\n",
    "    del train_res_feat,test_res_feat\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['Description'] = train_data['Description'].fillna(\"null\")\n",
    "    test_data['Description'] = test_data['Description'].fillna(\"null\")\n",
    "    \n",
    "    train_data[\"Description\"] = train_data[\"Description\"].str.lower()\n",
    "    test_data[\"Description\"] = test_data[\"Description\"].str.lower()\n",
    "    \n",
    "    puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "     '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "     '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "     '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "     '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "    def clean_text(x):\n",
    "\n",
    "        x = str(x)\n",
    "        for punct in puncts:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "        return x\n",
    "\n",
    "\n",
    "    train_data[\"Description\"] = train_data[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    test_data[\"Description\"] = test_data[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    \n",
    "\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # ## Number of words in the text ##\n",
    "    train_data[\"num_words\"] = train_data[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "    test_data[\"num_words\"] = test_data[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # ## Number of unique words in the text ##\n",
    "    train_data[\"num_unique_words\"] = train_data[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "    test_data[\"num_unique_words\"] = test_data[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    # ## Number of characters in the text ##\n",
    "    train_data[\"num_chars\"] = train_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    test_data[\"num_chars\"] = test_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    # ## Number of stopwords in the text ##\n",
    "    train_data[\"num_stopwords\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    test_data[\"num_stopwords\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    \n",
    "    # ## Number of punctuations in the text ##\n",
    "    train_data[\"num_punctuations\"] =train_data['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    test_data[\"num_punctuations\"] =test_data['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    \n",
    "    # ## Number of title case words in the text ##\n",
    "    # train_data[\"num_words_upper\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    # test_data[\"num_words_upper\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    \n",
    "    # ## Number of title case words in the text ##\n",
    "    # train_data[\"num_words_title\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    # test_data[\"num_words_title\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    \n",
    "    # ## Average length of the words in the text ##\n",
    "    train_data[\"mean_word_len\"] = train_data[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    test_data[\"mean_word_len\"] = test_data[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    # # train_data['num_vs_len']=train_data['num_punctuations']/train_data['num_chars']\n",
    "    # # test_data['num_vs_len']=test_data['num_punctuations']/test_data['num_chars']\n",
    "    \n",
    "    # # train_data['up_vs_len'] = train_data['num_words_upper'] / train_data['num_words']\n",
    "    # # test_data['up_vs_len'] = test_data['num_words_upper'] / test_data['num_words']\n",
    "    \n",
    "    # # train_data['senten_cnt']=train_data[\"Description\"].apply(lambda x:len(str(x).split(\".\")),1)\n",
    "    # # test_data['senten_cnt']=test_data[\"Description\"].apply(lambda x:len(str(x).split(\".\")),1)\n",
    "    \n",
    "    \n",
    "    def deal_desc(df):\n",
    "        if df['lan_type']==1:\n",
    "            return \"null\"\n",
    "        if df['lan_type']==3:\n",
    "            text=jieba.cut(df['Description'])\n",
    "            text=\" \".join(text)\n",
    "            text=text.replace(\"   \",\" \")\n",
    "            return text\n",
    "        else:\n",
    "            return df['Description']\n",
    "    train_data['Description']=train_data.apply(lambda x:deal_desc(x),1)\n",
    "    test_data['Description']=test_data.apply(lambda x:deal_desc(x),1)\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    train_data['doc_sent_mag'] = doc_sent_mag\n",
    "    train_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    test_data['doc_sent_mag'] = doc_sent_mag\n",
    "    test_data['doc_sent_score'] = doc_sent_score\n",
    "    del doc_sent_mag,doc_sent_score\n",
    "    gc.collect()\n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    print(nl_count)\n",
    "    train_data[ 'vertex_x'] = vertex_xs\n",
    "    train_data['vertex_y'] = vertex_ys\n",
    "    train_data['bounding_confidence'] = bounding_confidences\n",
    "    train_data['bounding_importance'] = bounding_importance_fracs\n",
    "    train_data['dominant_blue'] = dominant_blues\n",
    "    train_data['dominant_green'] = dominant_greens\n",
    "    train_data['dominant_red'] = dominant_reds\n",
    "    train_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    train_data['dominant_score'] = dominant_scores\n",
    "    train_data['label_description'] = label_descriptions\n",
    "    train_data['label_score'] = label_scores\n",
    "    \n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    test_data[ 'vertex_x'] = vertex_xs\n",
    "    test_data['vertex_y'] = vertex_ys\n",
    "    test_data['bounding_confidence'] = bounding_confidences\n",
    "    test_data['bounding_importance'] = bounding_importance_fracs\n",
    "    test_data['dominant_blue'] = dominant_blues\n",
    "    test_data['dominant_green'] = dominant_greens\n",
    "    test_data['dominant_red'] = dominant_reds\n",
    "    test_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    test_data['dominant_score'] = dominant_scores\n",
    "    test_data['label_description'] = label_descriptions\n",
    "    test_data['label_score'] = label_scores\n",
    "    \n",
    "    del  vertex_xs,vertex_ys,bounding_confidences,bounding_importance_fracs,dominant_blues,dominant_greens,dominant_reds,dominant_pixel_fracs,dominant_scores\n",
    "    del label_descriptions,label_scores\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['label_description'] =train_data['label_description'].astype(np.str)\n",
    "    train_data['label_description'] =train_data['label_description'].astype('category')\n",
    "    \n",
    "    test_data['label_description'] =test_data['label_description'].astype(np.str)\n",
    "    test_data['label_description'] =test_data['label_description'].astype('category')\n",
    "    \n",
    "    tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    print(\"TFIDF....\")\n",
    "    tfv.fit(list(train_data['Description'].values)+list(test_data['Description'].values))\n",
    "    X =  tfv.transform(train_data['Description'])\n",
    "    X_test = tfv.transform(test_data['Description'])\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=10)\n",
    "    svd.fit(X)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['svg_{}'.format(i) for i in range(10)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['svg_{}'.format(i) for i in range(10)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    breed=pd.read_csv(\"../input/petfinder-adoption-prediction/breed_labels.csv\")\n",
    "    color = pd.read_csv(\"../input/petfinder-adoption-prediction/color_labels.csv\")\n",
    "    color_dict = dict(zip(color['ColorID'].values.astype(\"str\"),color['ColorName'].values))\n",
    "    breed_dict = dict(zip(breed['BreedID'].values.astype(\"str\"),breed['BreedName'].values))\n",
    "    def get_text(df):\n",
    "        x=\"\"\n",
    "        if df['Type']==1:\n",
    "            x+=\"dog\"+\" \"\n",
    "        if df['Type']==2:\n",
    "            x+=\"cat\"+\" \"\n",
    "        for i in ['Breed1',\"Breed2\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=breed_dict[str(df[i])]+\" \"\n",
    "        for i in [\"Color1\",\"Color2\",\"Color3\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=color_dict[str(df[i])]+\" \"\n",
    "        x=x+df['Description']\n",
    "        return x\n",
    "    train_data['concat_text']=train_data.apply(lambda x:get_text(x),1)\n",
    "    test_data['concat_text']=test_data.apply(lambda x:get_text(x),1)\n",
    "    \n",
    "    train_desc=train_data['concat_text'].values\n",
    "    test_desc=test_data['concat_text'].values\n",
    "    \n",
    "    tfv.fit(list(train_data['concat_text'].values)+list(test_data['concat_text'].values))\n",
    "    X =  tfv.transform(train_data['concat_text'])\n",
    "    X_test = tfv.transform(test_data['concat_text'])\n",
    "    \n",
    "    \n",
    "\n",
    "    svd = NMF(n_components=5,random_state=100)\n",
    "    svd.fit(vstack([X,X_test]))\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['nmf_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['nmf_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    svd = LatentDirichletAllocation(n_components=5,max_iter=30, random_state=100)\n",
    "    svd.fit(vstack([X,X_test]))\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['lda_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['lda_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    tfv =  CountVectorizer(min_df=3,  \n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 5),\n",
    "        stop_words = 'english')\n",
    "    tfv.fit(list(train_data['Description'].values)+list(test_data['Description'].values))\n",
    "    X =  tfv.transform(train_data['Description'])\n",
    "    X_test = tfv.transform(test_data['Description'])\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=5)\n",
    "    svd.fit(vstack([X,X_test]))\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['nb_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['nb_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    svd = LatentDirichletAllocation(n_components=5,max_iter=30, random_state=10)\n",
    "    svd.fit(vstack([X,X_test]))\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['c_lda_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['c_lda_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    onehot_col=[\"Breed1\",\"Breed2\",\"Color1\",\"Color2\",\"Color3\",'State','Gender','MaturitySize','MaturitySize','FurLength','Vaccinated',\n",
    "           'Dewormed','Sterilized','Health']\n",
    "    data=pd.concat([train_data,test_data])\n",
    "    data.index=range(len(data))\n",
    "    onehot_df=pd.DataFrame(index=range(len(data)))\n",
    "    for i in onehot_col:\n",
    "        temp=pd.get_dummies(data[i],prefix=i)\n",
    "        onehot_df=pd.concat([onehot_df,temp],1)\n",
    "        \n",
    "    svd = TruncatedSVD(n_components=5)\n",
    "    svd.fit(onehot_df)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    oh_df = svd.transform(onehot_df)\n",
    "    oh_df = pd.DataFrame(oh_df, columns=['oh_{}'.format(i) for i in range(5)])\n",
    "    oh_df['PetID']=data['PetID']\n",
    "    \n",
    "    del X,X_test,onehot_df,data\n",
    "    gc.collect()\n",
    "    \n",
    "    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n",
    "                                                              \"hash_ngrams_weights\": [1.5, 1.0],\n",
    "                                                              \"hash_size\": 2 ** 29,\n",
    "                                                              \"norm\": None,\n",
    "                                                              \"tf\": 'binary',\n",
    "                                                              \"idf\": None,\n",
    "                                                              }), procs=8)\n",
    "    x_train = wb.fit_transform(train_data[\"Description\"])\n",
    "    x_test = wb.transform(test_data[\"Description\"])\n",
    "    mask = np.array(np.clip(x_train.getnnz(axis=0) -8 , 0, 1), dtype=bool)\n",
    "    x_train=x_train[:,mask]\n",
    "    x_test=x_test[:,mask]\n",
    "    print(x_test.shape)\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=5)\n",
    "    svd.fit(x_train)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    X_svg = svd.transform(x_train)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['wb_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(x_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['wb_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    del x_test,x_train,X_test_svg,X_svg,wb,svd\n",
    "    gc.collect()\n",
    "    \n",
    "    train_df = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "    img_size = 256\n",
    "    batch_size = 16\n",
    "    pet_ids = train_df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    extract_model = img_model()\n",
    "    img_features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = extract_model.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            img_features[pet_id] = batch_preds[i]\n",
    "    train_img_feat = pd.DataFrame.from_dict(img_features, orient='index')\n",
    "    #train_feats.to_csv('train_img_features.csv')\n",
    "    \n",
    "    test_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n",
    "    pet_ids = test_df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    img_features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = extract_model.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            img_features[pet_id] = batch_preds[i]\n",
    "            \n",
    "    test_img_feat = pd.DataFrame.from_dict(img_features, orient='index')\n",
    "    del img_features\n",
    "    gc.collect()\n",
    "    \n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(train_img_feat.values)\n",
    "    train_img=pca.transform(train_img_feat.values)\n",
    "    test_img=pca.transform(test_img_feat.values)\n",
    "    train_img = pd.DataFrame(train_img, columns=['pca_{}'.format(i) for i in range(5)])\n",
    "    test_img = pd.DataFrame(test_img, columns=['pca_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, train_img), axis=1)\n",
    "    test_data = pd.concat((test_data, test_img), axis=1)\n",
    "    del train_img,test_img,pca\n",
    "    gc.collect()\n",
    "    \n",
    "    state= pd.read_csv(\"../input/petfinder-adoption-prediction/state_labels.csv\")\n",
    "\n",
    "    state_dict={\n",
    "        \"Johor\":[3233434,19210,168],\n",
    "        \"Kedah\":[1890098,9500,199],\n",
    "        \"Kelantan\":[1459994,15099,97],\n",
    "        \"Kuala Lumpur\":[1627172,243,6696],\n",
    "        \"Labuan\":[85272,91,937],\n",
    "        \"Melaka\":[788706,1664,474],\n",
    "        \"Negeri Sembilan\":[997071,6686,149],\n",
    "        \"Pahang\":[1443365,36137,40],\n",
    "        \"Perak\":[2258428,21035,107],\n",
    "        \"Perlis\":[227025,821,277],\n",
    "        \"Pulau Pinang\":[1520143,1048,1451],\n",
    "        \"Sabah\":[3120040,73631,42],\n",
    "        \"Sarawak\":[2420009,124450,19],\n",
    "        \"Selangor\":[5411324,8104,668],\n",
    "        \"Terengganu\":[1015776,13035,78]\n",
    "    }\n",
    "    def get_state_feat(df):\n",
    "        df['state_people/area_ratio']=state_dict[df['StateName']][2]\n",
    "        return df\n",
    "    state=state.apply(lambda x:get_state_feat(x),1)\n",
    "    state['state_rank']=state['state_people/area_ratio'].rank()\n",
    "    state_ratio_dict=dict(zip(state.StateID.values,state['state_people/area_ratio'].values))\n",
    "    state_rank_dict=dict(zip(state.StateID.values,state['state_rank'].values))\n",
    "    \n",
    "    train_data['State_ratio']=train_data['State'].map(state_ratio_dict)\n",
    "    test_data['State_ratio']=test_data['State'].map(state_ratio_dict)\n",
    "    \n",
    "    train_data['State_rank']=train_data['State'].map(state_rank_dict)\n",
    "    test_data['State_rank']=test_data['State'].map(state_rank_dict)\n",
    "    \n",
    "    del state_rank_dict,state_ratio_dict,state\n",
    "    gc.collect()\n",
    "    # train_data['is_high_state_ratio']=train_data['State_ratio'].apply(lambda x:1 if x>168 else 0,1)\n",
    "    # test_data['is_high_state_ratio']=test_data['State_ratio'].apply(lambda x:1 if x>168 else 0,1)\n",
    "    \n",
    "    def get_breed(df):\n",
    "        x=\"\"\n",
    "        for i in [\"Breed1\",\"Breed2\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=breed_dict[str(df[i])]+\" \"\n",
    "        return x\n",
    "    train_data['breed']=train_data.apply(lambda x:get_breed(x),1)\n",
    "    test_data['breed']=test_data.apply(lambda x:get_breed(x),1)   \n",
    "    \n",
    "    train_data['Breed1']=train_data['Breed1'].astype(\"str\")\n",
    "    train_data['Breed1']=train_data['Breed1'].map(breed_dict)\n",
    "    train_data['Breed1']=train_data['Breed1'].replace(np.nan,\"null\")\n",
    "    train_data['Breed2']=train_data['Breed2'].astype(\"str\")\n",
    "    train_data['Breed2']=train_data['Breed2'].map(breed_dict)\n",
    "    train_data['Breed2']=train_data['Breed2'].replace(np.nan,\"null\")\n",
    "    \n",
    "    test_data['Breed1']=test_data['Breed1'].astype(\"str\")\n",
    "    test_data['Breed1']=test_data['Breed1'].map(breed_dict)\n",
    "    test_data['Breed1']=test_data['Breed1'].replace(np.nan,\"null\")\n",
    "    test_data['Breed2']=test_data['Breed2'].astype(\"str\")\n",
    "    test_data['Breed2']=test_data['Breed2'].map(breed_dict)\n",
    "    test_data['Breed2']=test_data['Breed2'].replace(np.nan,\"null\")\n",
    "    \n",
    "    def get_color_cnt(df):\n",
    "        color_list=[]\n",
    "        for i in [\"Color1\",\"Color2\",\"Color3\"]:\n",
    "            if df[i]!=0:\n",
    "                color_list.append(df[i])\n",
    "        return len(set(color_list))\n",
    "    train_data['color_cnt']=train_data.apply(lambda x:get_color_cnt(x),1)\n",
    "    test_data['color_cnt']=test_data.apply(lambda x:get_color_cnt(x),1)\n",
    "    \n",
    "    def get_color(df):\n",
    "        x=\"\"\n",
    "        for i in [\"Color1\",\"Color2\",\"Color3\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=color_dict[str(df[i])]+\" \"\n",
    "        return x\n",
    "    train_data['color']=train_data.apply(lambda x:get_color(x),1)\n",
    "    test_data['color']=test_data.apply(lambda x:get_color(x),1)\n",
    "    \n",
    "    mean_encoder = MeanEncoder( categorical_features=['Breed1', 'breed'],target_type ='regression')\n",
    "    train_data = mean_encoder.fit_transform(train_data, train_data['AdoptionSpeed'])\n",
    "    test_data = mean_encoder.transform(test_data)\n",
    "    for col in ['Breed1',\"Breed2\",\"color\",\"breed\"]:\n",
    "        lbl = LabelEncoder()\n",
    "        train_data[col]=train_data[col].fillna(0)\n",
    "        test_data[col]=test_data[col].fillna(0)\n",
    "        lbl.fit(list(train_data[col].values)+list(test_data[col].values))\n",
    "        train_data[col]=lbl.transform(train_data[col])\n",
    "        test_data[col]=lbl.transform(test_data[col])\n",
    "        \n",
    "    \n",
    "    cols = [x for x in train_data.columns if x not in ['Breed1',\"breed\",'label_description',\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "    data=pd.concat([train_data,test_data])\n",
    "    train_data[cols]=train_data[cols].fillna(0)\n",
    "    test_data[cols]=test_data[cols].fillna(0)\n",
    "    ############################ 切分数据集 ##########################\n",
    "    print('开始进行一些前期处理')\n",
    "    train_feature = train_data[cols].values\n",
    "    test_feature = test_data[cols].values\n",
    "        # 五则交叉验证\n",
    "    n_folds = 5\n",
    "    print('处理完毕')\n",
    "    df_stack2 = pd.DataFrame()\n",
    "    df_stack2['PetID']=data['PetID']\n",
    "    for label in [\"AdoptionSpeed\"]:\n",
    "        score = train_data[label]\n",
    "        \n",
    "       \n",
    "        ########################### SGD(随机梯度下降) ################################\n",
    "        print('sgd stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            sgd = SGDRegressor(random_state=1017,)\n",
    "            sgd.fit(train_feature[tr], score[tr])\n",
    "            score_va = sgd.predict(train_feature[va])\n",
    "            score_te = sgd.predict(test_feature)\n",
    "            print('得分' + str(mean_squared_error(score[va], sgd.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0]+= score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "        df_stack2['tfidf_sgd_classfiy_{}'.format(label)] = stack[:,0]\n",
    "    \n",
    "    \n",
    "        ########################### pac(PassiveAggressiveClassifier) ################################\n",
    "        print('PAC stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            pac = PassiveAggressiveRegressor(random_state=1017)\n",
    "            pac.fit(train_feature[tr], score[tr])\n",
    "            score_va = pac.predict(train_feature[va])\n",
    "            score_te = pac.predict(test_feature)\n",
    "          \n",
    "            print('得分' + str(mean_squared_error(score[va], pac.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0] += score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "        df_stack2['tfidf_pac_classfiy_{}'.format(label)] = stack[:,0]\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        ########################### FTRL ################################\n",
    "        print('MultinomialNB stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            clf = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=train_feature.shape[1], iters=50, inv_link=\"identity\", threads=1)\n",
    "            clf.fit(train_feature[tr], score[tr])\n",
    "            score_va = clf.predict(train_feature[va])\n",
    "            score_te = clf.predict(test_feature)\n",
    "          \n",
    "            print('得分' + str(mean_squared_error(score[va], clf.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0] += score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "        \n",
    "        df_stack2['tfidf_FTRL_classfiy_{}'.format(label)] = stack[:,0]\n",
    "        \n",
    "        ########################### ridge(RidgeClassfiy) ################################\n",
    "        print('RidgeClassfiy stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            ridge = Ridge(solver=\"sag\", fit_intercept=True, random_state=42, alpha=30) \n",
    "            ridge.fit(train_feature[tr], score[tr])\n",
    "            score_va = ridge.predict(train_feature[va])\n",
    "            score_te = ridge.predict(test_feature)\n",
    "           \n",
    "            print('得分' + str(mean_squared_error(score[va], ridge.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0] += score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "        df_stack2['tfidf_ridge_classfiy_{}'.format(label)] = stack[:,0]\n",
    "        \n",
    "        ############################ Linersvc(LinerSVC) ################################\n",
    "        print('LinerSVC stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            lsvc = LinearSVR(random_state=1017)\n",
    "            lsvc.fit(train_feature[tr], score[tr])\n",
    "            score_va = lsvc.predict(train_feature[va])\n",
    "            score_te = lsvc.predict(test_feature)\n",
    "           \n",
    "            print('得分' + str(mean_squared_error(score[va], lsvc.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0] += score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "        df_stack2['tfidf_lsvc_classfiy_{}'.format(label)] = stack[:,0]\n",
    "        \n",
    "    # df_stack.to_csv('graph_tfidf_classfiy.csv', index=None, encoding='utf8')\n",
    "    print('tfidf特征已保存\\n')\n",
    "    del stack,train_feature,test_feature,stack_train, stack_test,data\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    \n",
    "    train=pd.merge(train_data,oh_df,on=\"PetID\",how=\"left\")\n",
    "    test=pd.merge(test_data,oh_df,on=\"PetID\",how=\"left\")\n",
    "    \n",
    "    train=pd.merge(train,df_stack2,on=\"PetID\",how=\"left\")\n",
    "    test=pd.merge(test,df_stack2,on=\"PetID\",how=\"left\")\n",
    "    \n",
    "    del train_data,test_data,df_stack2,oh_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return train,test,train_desc,test_desc,train_img_feat,test_img_feat\n",
    "    \n",
    "def runLGB_c(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "#                       fobj=softkappaObj,\n",
    "                      verbose_eval=verbose_eval,\n",
    "#                       feval=kappa_scorer,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    log=log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "    pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "                               ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "                               ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk,log\n",
    "train,test,train_desc,test_desc,train_img_feat,test_img_feat=get_feat1()\n",
    "train1=train.copy()\n",
    "test1=test.copy()\n",
    "train1.drop(['pca_{}'.format(i) for i in range(5)],1,inplace=True)\n",
    "test1.drop(['pca_{}'.format(i) for i in range(5)],1,inplace=True)\n",
    "\n",
    "features = [x for x in train.columns if x not in ['Breed1',\"breed\",\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "\n",
    "label='AdoptionSpeed'\n",
    "###model 1\n",
    "# params = {\n",
    "# #     'application': 'regression',\n",
    "#     'objective': 'multiclass', \n",
    "#     \"num_class\":5,\n",
    "#           'boosting': 'gbdt',\n",
    "# #           'metric': 'rmse',\n",
    "#     'metric':{'multi_logloss',},\n",
    "#           'num_leaves': 80,\n",
    "#          'max_depth':9,\n",
    "#           'learning_rate': 0.01,\n",
    "#           'bagging_fraction': 0.90,\n",
    "#            \"bagging_freq\":3,\n",
    "#           'feature_fraction': 0.85,\n",
    "#           'min_split_gain': 0.01,\n",
    "#           'min_child_samples': 150,\n",
    "#           \"lambda_l1\": 0.1,\n",
    "#           'verbosity': -1,\n",
    "#           'early_stop': 100,\n",
    "#           'verbose_eval': 200,\n",
    "#           \"data_random_seed\":3,\n",
    "# #           \"random_state\":1017,\n",
    "#           'num_rounds': 10000}\n",
    "params = {\n",
    "#     'application': 'regression',\n",
    "    'objective': 'multiclass', \n",
    "    \"num_class\":5,\n",
    "          'boosting': 'gbdt',\n",
    "#           'metric': 'rmse',\n",
    "    'metric':{'multi_logloss',},\n",
    "          'num_leaves': 55,\n",
    "         'max_depth':9,\n",
    "        'max_bin': 45,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9879639408647978,\n",
    "            \"bagging_freq\":41,\n",
    "           'feature_fraction': 0.5849356442713105,\n",
    "           'min_split_gain': 0.6118528947223795,\n",
    "         'min_child_samples': 83,\n",
    "     'min_child_weight': 0.2912291401980419,\n",
    "     'lambda_l1': 0.18182496720710062,\n",
    "          'lambda_l2': 0.18340450985343382,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "          \"data_random_seed\":17,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB_c, params, rmse, 'LGB')    \n",
    "imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\n",
    "imp=imports.sort_values('importance', ascending=False)\n",
    "print(imp)\n",
    "lgb1_train=[r[0] for r in results['train']]\n",
    "lgb1_test=[r[0] for r in results['test']]\n",
    "t1=time.time()\n",
    "print(\"model1 cost:{} s\".format(t1-start_time))\n",
    "\n",
    "###model 2\n",
    "features = [x for x in train.columns if x not in ['label_description','Breed1',\"breed\",\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "def runCAT(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = Pool(train_X, label=train_y)\n",
    "    d_valid = Pool(test_X, label=test_y)\n",
    "    watchlist = (d_train, d_valid)\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = CatBoostClassifier(iterations=num_rounds, \n",
    "        learning_rate = 0.03,\n",
    "        od_type='Iter',\n",
    "         od_wait=early_stop,\n",
    "        loss_function='MultiClass',\n",
    "        eval_metric='MultiClass',\n",
    "        bagging_temperature=0.9,                   \n",
    "        random_seed = 2019,\n",
    "        task_type='GPU'\n",
    "                          )\n",
    "    model.fit(d_train,eval_set=d_valid,\n",
    "            use_best_model=True,\n",
    "            verbose=verbose_eval\n",
    "                         )\n",
    "    \n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    log=log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "    pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "                               ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 =  model.predict_proba(test_X2)\n",
    "    pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "                               ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), 0, coefficients, qwk,log\n",
    "results = run_cv_model(train[features], test[features], train[label], runCAT, params, rmse, 'CAT')\n",
    "cat1_train=[r[0] for r in results['train']]\n",
    "cat1_test=[r[0] for r in results['test']]\n",
    "t2=time.time()\n",
    "print(\"model2 cost:{} s\".format(t2-t1))\n",
    "###model 3\n",
    "features = [x for x in train.columns if x not in ['Breed1_pred',\"breed_pred\",\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "label='AdoptionSpeed'\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "         'max_depth':9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.85,\n",
    "          'min_split_gain': 0.01,\n",
    "          'min_child_samples': 150,\n",
    "          \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "           \"data_random_seed\":3,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "def runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "#                       fobj=softkappaObj,\n",
    "                      verbose_eval=verbose_eval,\n",
    "#                       feval=kappa_scorer,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    log=0#log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "#     pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "    \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "#     pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk,log\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "\n",
    "lgb2_train=[r[0] for r in results['train']]\n",
    "lgb2_test=[r[0] for r in results['test']]\n",
    "del train,test\n",
    "gc.collect()\n",
    "t3=time.time()\n",
    "print(\"model3 cost:{} s\".format(t3-t2))\n",
    "###model 4\n",
    "def get_feat2():\n",
    "    train_data = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    def get_purebreed_feat(df):\n",
    "        if df['Breed2']==0 and df['Breed1']!=307:\n",
    "            return 1\n",
    "        return 0\n",
    "    train_data['purebreed']=train_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    test_data['purebreed']=test_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    train_data['is_group']=train_data.Gender.apply(lambda x:1 if x==3 else 0,1)\n",
    "    test_data['is_group']=test_data.Gender.apply(lambda x:1 if x==3 else 0,1)\n",
    "    def name_deal(df):\n",
    "        if \"No Name\" in df:\n",
    "            return np.nan\n",
    "        if df ==\"nan\":\n",
    "            return np.nan\n",
    "        else:\n",
    "            return df\n",
    "    train_data['Name'] = train_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    test_data['Name'] = test_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    \n",
    "    train_data['Name'],indexer=pd.factorize(train_data['Name'])\n",
    "    test_data['Name'] = indexer.get_indexer(test_data['Name'])\n",
    "    group = train_data['RescuerID'].values\n",
    "    rescuer_df=train_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    train_data=pd.merge(train_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    train_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    rescuer_df=test_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    test_data=pd.merge(test_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    test_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    ## Number of words in the text ##\n",
    "    train_data[\"num_words\"] = train_data[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "    test_data[\"num_words\"] = test_data[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    ## Number of unique words in the text ##\n",
    "    train_data[\"num_unique_words\"] = train_data[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "    test_data[\"num_unique_words\"] = test_data[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    ## Number of characters in the text ##\n",
    "    train_data[\"num_chars\"] = train_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    test_data[\"num_chars\"] = test_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    ## Number of stopwords in the text ##\n",
    "    train_data[\"num_stopwords\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    test_data[\"num_stopwords\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    \n",
    "    ## Number of punctuations in the text ##\n",
    "    train_data[\"num_punctuations\"] =train_data['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    test_data[\"num_punctuations\"] =test_data['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train_data[\"num_words_upper\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    test_data[\"num_words_upper\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train_data[\"num_words_title\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    test_data[\"num_words_title\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    \n",
    "    ## Average length of the words in the text ##\n",
    "    train_data[\"mean_word_len\"] = train_data[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    test_data[\"mean_word_len\"] = test_data[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    train_data['Description'] = train_data['Description'].fillna(\"null\")\n",
    "    test_data['Description'] = test_data['Description'].fillna(\"null\")\n",
    "    data = pd.concat([train_data,test_data])\n",
    "    data.index=range(len(data))\n",
    "    data_id=data['PetID'].values\n",
    "    \n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    train_data['doc_sent_mag'] = doc_sent_mag\n",
    "    train_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    test_data['doc_sent_mag'] = doc_sent_mag\n",
    "    test_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    print(nl_count)\n",
    "    train_data[ 'vertex_x'] = vertex_xs\n",
    "    train_data['vertex_y'] = vertex_ys\n",
    "    train_data['bounding_confidence'] = bounding_confidences\n",
    "    train_data['bounding_importance'] = bounding_importance_fracs\n",
    "    train_data['dominant_blue'] = dominant_blues\n",
    "    train_data['dominant_green'] = dominant_greens\n",
    "    train_data['dominant_red'] = dominant_reds\n",
    "    train_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    train_data['dominant_score'] = dominant_scores\n",
    "    train_data['label_description'] = label_descriptions\n",
    "    train_data['label_score'] = label_scores\n",
    "    \n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    test_data[ 'vertex_x'] = vertex_xs\n",
    "    test_data['vertex_y'] = vertex_ys\n",
    "    test_data['bounding_confidence'] = bounding_confidences\n",
    "    test_data['bounding_importance'] = bounding_importance_fracs\n",
    "    test_data['dominant_blue'] = dominant_blues\n",
    "    test_data['dominant_green'] = dominant_greens\n",
    "    test_data['dominant_red'] = dominant_reds\n",
    "    test_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    test_data['dominant_score'] = dominant_scores\n",
    "    test_data['label_description'] = label_descriptions\n",
    "    test_data['label_score'] = label_scores\n",
    "    \n",
    "    del  vertex_xs,vertex_ys,bounding_confidences,bounding_importance_fracs,dominant_blues,dominant_greens,dominant_reds,dominant_pixel_fracs,dominant_scores\n",
    "    del label_descriptions,label_scores,doc_sent_mag,doc_sent_score\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['label_description'] =train_data['label_description'].astype(np.str)\n",
    "    train_data['label_description'] =train_data['label_description'].astype('category')\n",
    "    \n",
    "    test_data['label_description'] =test_data['label_description'].astype(np.str)\n",
    "    test_data['label_description'] =test_data['label_description'].astype('category')\n",
    "    \n",
    "    tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    print(\"TFIDF....\")\n",
    "    tfv.fit(list(train_data['Description'].values)+list(test_data['Description'].values))\n",
    "    X =  tfv.transform(train_data['Description'])\n",
    "    X_test = tfv.transform(test_data['Description'])\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=120)\n",
    "    svd.fit(X)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    X = svd.transform(X)\n",
    "    X = pd.DataFrame(X, columns=['nb_{}'.format(i) for i in range(120)])\n",
    "    \n",
    "    X_test = svd.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test, columns=['nb_{}'.format(i) for i in range(120)])\n",
    "    train_data = pd.concat((train_data, X), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test), axis=1)\n",
    "    \n",
    "    del X,X_test\n",
    "    gc.collect()\n",
    "    return train_data,test_data\n",
    "    \n",
    "    \n",
    "train,test=get_feat2()\n",
    "train2=train.copy()\n",
    "test2=test.copy()\n",
    "\n",
    "features = [x for x in train.columns if x not in [\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "label='AdoptionSpeed'\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "#          'max_depth':11,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.4,\n",
    "          'min_split_gain': 0.01,\n",
    "#           'min_child_samples': 30,\n",
    "#           \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "lgb3_train=[r[0] for r in results['train']]\n",
    "lgb3_test=[r[0] for r in results['test']]\n",
    "del train,test\n",
    "gc.collect()\n",
    "t4=time.time()\n",
    "print(\"model4 cost:{} s\".format(t4-t3))\n",
    "def get_feat3():\n",
    "    train_data = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    \n",
    "    # petid列\n",
    "    data = pd.concat([train_data, test_data], axis=0)\n",
    "    del data['AdoptionSpeed']\n",
    "    es = ft.EntitySet(id='data_id')\n",
    "    es = es.entity_from_dataframe(entity_id='PetID', dataframe=data,\n",
    "                                   index='PetID')\n",
    "    \n",
    "    need_deal_columns = ['Age', 'Breed1', 'Breed2', 'Color1', 'Color2', 'Color3', 'Description',\n",
    "           'Dewormed', 'Fee', 'FurLength', 'Gender', 'Health', 'MaturitySize',\n",
    "           'Name', 'PhotoAmt', 'Quantity', 'RescuerID', 'State',\n",
    "           'Sterilized', 'Type', 'Vaccinated', 'VideoAmt']\n",
    "    for i in need_deal_columns:\n",
    "        data_RescuerID = pd.DataFrame()\n",
    "        data_RescuerID[i] = list(data[i].unique())\n",
    "        es = es.entity_from_dataframe(entity_id=i, dataframe=data_RescuerID,\n",
    "                                   index=i)\n",
    "        cr = ft.Relationship( es[i][i],\n",
    "                        es['PetID'][i])\n",
    "        es = es.add_relationship(cr)\n",
    "        \n",
    "    features, feature_names = ft.dfs(entityset=es, target_entity='PetID',\n",
    "                                     max_depth=3,verbose=True)\n",
    "    \n",
    "    features = pd.merge(data[['PetID']], features.reset_index(), on='PetID', how='left')\n",
    "    label_encode = LabelEncoder()\n",
    "    for i in features.columns:\n",
    "        if features[i].dtype ==\"object\":\n",
    "            features[i] = features[i].fillna('未知')\n",
    "            features[i] = list(map(str, features[i]))\n",
    "            features[i] = label_encode.fit_transform(features[i])\n",
    "    features = features[['Breed1.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Description.MODE(PetID.VideoAmt)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Description.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Description.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'State.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'Color2.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Age.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Description.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'RescuerID.MODE(PetID.Health)',\n",
    "     'Color3.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Name)',\n",
    "     'Breed1.MODE(PetID.Type)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Color1)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'State.MODE(PetID.Type)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Health)',\n",
    "     'Color2.MODE(PetID.PhotoAmt)',\n",
    "     'Dewormed.MODE(PetID.RescuerID)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Description)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Color1.NUM_UNIQUE(PetID.State)',\n",
    "     'Color1.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Fee.MODE(PetID.Age)',\n",
    "     'Health.MODE(PetID.Description)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Health)',\n",
    "     'Breed1.MODE(PetID.Breed2)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Color3.MODE(PetID.Type)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Name)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'VideoAmt.MODE(PetID.PhotoAmt)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Name)',\n",
    "     'Health.NUM_UNIQUE(PetID.Age)',\n",
    "     'Age.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Description.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Fee.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Age.MODE(PetID.Color1)',\n",
    "     'State.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Fee)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Name)',\n",
    "     'Vaccinated.MODE(PetID.Dewormed)',\n",
    "     'Color1.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.Type)',\n",
    "     'State.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Health.NUM_UNIQUE(PetID.Breed1)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Color3.MODE(PetID.Breed1)',\n",
    "     'Age.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Color3.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Quantity.MODE(PetID.Color2)',\n",
    "     'Dewormed.MODE(PetID.Type)',\n",
    "     'Color1.MODE(PetID.Gender)',\n",
    "     'Gender.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Breed2.MODE(PetID.Color1)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'VideoAmt.COUNT(PetID)',\n",
    "     'State.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Name)',\n",
    "     'Age.MODE(PetID.FurLength)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'VideoAmt.MODE(PetID.RescuerID)',\n",
    "     'Vaccinated.MODE(PetID.Name)',\n",
    "     'Breed2.MODE(PetID.Gender)',\n",
    "     'MaturitySize.MODE(PetID.Type)',\n",
    "     'Description.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Type.COUNT(PetID)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Health)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'MaturitySize.MODE(PetID.PhotoAmt)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Name)',\n",
    "     'Description.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Description.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Sterilized.MODE(PetID.Vaccinated)',\n",
    "     'Fee.MODE(PetID.Sterilized)',\n",
    "     'Breed2.MODE(PetID.Type)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Name)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Quantity.MODE(PetID.Age)',\n",
    "     'Quantity.MODE(PetID.Dewormed)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Description)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Age)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Gender.MODE(PetID.Name)',\n",
    "     'VideoAmt.MODE(PetID.Vaccinated)',\n",
    "     'Quantity.MODE(PetID.Breed1)',\n",
    "     'Color1.MODE(PetID.Type)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Health)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.State)',\n",
    "     'Breed1.MODE(PetID.Color3)',\n",
    "     'Fee.MODE(PetID.FurLength)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Type)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Health.NUM_UNIQUE(PetID.Breed2)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Color1)',\n",
    "     'State.MODE(PetID.Dewormed)',\n",
    "     'Color1.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Description.NUM_UNIQUE(PetID.Health)',\n",
    "     'Fee.MODE(PetID.Gender)',\n",
    "     'Description.NUM_UNIQUE(PetID.State)',\n",
    "     'Color3.MODE(PetID.Dewormed)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Breed1)',\n",
    "     'PhotoAmt.MODE(PetID.State)',\n",
    "     'Type.NUM_UNIQUE(PetID.Age)',\n",
    "     'Age.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'State.NUM_UNIQUE(PetID.Health)',\n",
    "     'Breed1.MODE(PetID.State)',\n",
    "     'State.MODE(PetID.PhotoAmt)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Breed2)',\n",
    "     'Description.NUM_UNIQUE(PetID.Type)',\n",
    "     'Vaccinated.MODE(PetID.RescuerID)',\n",
    "     'Health.MODE(PetID.Name)',\n",
    "     'Color3.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Fee.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Name)',\n",
    "     'Health.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'PhotoAmt.MODE(PetID.Breed1)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Color3.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Health.NUM_UNIQUE(PetID.Description)',\n",
    "     'Age.NUM_UNIQUE(PetID.Type)',\n",
    "     'Fee.MODE(PetID.Color1)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Type.NUM_UNIQUE(PetID.Breed2)',\n",
    "     'Breed2.MODE(PetID.Dewormed)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Health.NUM_UNIQUE(PetID.Name)',\n",
    "     'Vaccinated.MODE(PetID.Type)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Description)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'State.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Age.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Type)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Type.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Age.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Color2)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Type.NUM_UNIQUE(PetID.Breed1)',\n",
    "     'Sterilized.MODE(PetID.Name)',\n",
    "     'Description.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Age.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'PhotoAmt.MODE(PetID.Vaccinated)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Age.NUM_UNIQUE(PetID.Gender)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Breed2.MODE(PetID.Color3)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.State)',\n",
    "     'Type.NUM_UNIQUE(PetID.Name)',\n",
    "     'PhotoAmt.MODE(PetID.Age)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Color2.MODE(PetID.Vaccinated)',\n",
    "     'Type.NUM_UNIQUE(PetID.Description)',\n",
    "     'Health.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Health.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Breed2.MODE(PetID.State)',\n",
    "     'MaturitySize.MODE(PetID.Gender)',\n",
    "     'PhotoAmt.MODE(PetID.Gender)',\n",
    "     'Color1.MODE(PetID.PhotoAmt)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'MaturitySize.MODE(PetID.Vaccinated)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'VideoAmt.MODE(PetID.Type)',\n",
    "     'Dewormed.MODE(PetID.Vaccinated)',\n",
    "     'FurLength.MODE(PetID.Breed1)',\n",
    "     'Health.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Type.MODE(PetID.RescuerID)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Type.MODE(PetID.Age)',\n",
    "     'Type.MODE(PetID.State)',\n",
    "     'Type.MODE(PetID.Sterilized)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Type.MODE(PetID.Breed2)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Health)',\n",
    "     'Type.MODE(PetID.Breed1)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Type)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Type.MODE(PetID.Color1)',\n",
    "     'Type.MODE(PetID.Health)',\n",
    "     'Type.MODE(PetID.MaturitySize)',\n",
    "     'Type.MODE(PetID.Gender)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.State)',\n",
    "     'Type.MODE(PetID.FurLength)',\n",
    "     'Type.MODE(PetID.Fee)',\n",
    "     'Type.MODE(PetID.VideoAmt)',\n",
    "     'Type.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Type.MODE(PetID.Name)',\n",
    "     'Type.MODE(PetID.Dewormed)',\n",
    "     'Type.MODE(PetID.PhotoAmt)',\n",
    "     'Type.MODE(PetID.Description)',\n",
    "     'Type.MODE(PetID.Color3)',\n",
    "     'Type.MODE(PetID.Quantity)',\n",
    "     'Type.MODE(PetID.Color2)',\n",
    "     'Type.MODE(PetID.Vaccinated)',\n",
    "     'Color1.MODE(PetID.Dewormed)',\n",
    "     'Type.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Type.NUM_UNIQUE(PetID.Color3)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Name)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Health)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Gender)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Fee)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Color3)',\n",
    "     'VideoAmt.MODE(PetID.Sterilized)',\n",
    "     'VideoAmt.MODE(PetID.State)',\n",
    "     'VideoAmt.MODE(PetID.Quantity)',\n",
    "     'VideoAmt.MODE(PetID.MaturitySize)',\n",
    "     'VideoAmt.MODE(PetID.Health)',\n",
    "     'VideoAmt.MODE(PetID.Gender)',\n",
    "     'VideoAmt.MODE(PetID.FurLength)',\n",
    "     'VideoAmt.MODE(PetID.Fee)',\n",
    "     'VideoAmt.MODE(PetID.Dewormed)',\n",
    "     'VideoAmt.MODE(PetID.Color3)',\n",
    "     'VideoAmt.MODE(PetID.Color2)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.State)',\n",
    "     'Breed1.MODE(PetID.Fee)',\n",
    "     'Age.MODE(PetID.Breed2)',\n",
    "     'Age.MODE(PetID.Color3)',\n",
    "     'Age.MODE(PetID.Fee)',\n",
    "     'Age.MODE(PetID.Health)',\n",
    "     'Age.MODE(PetID.MaturitySize)',\n",
    "     'Age.MODE(PetID.Quantity)',\n",
    "     'Age.MODE(PetID.State)',\n",
    "     'Age.MODE(PetID.VideoAmt)',\n",
    "     'Breed1.MODE(PetID.Health)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Breed1.MODE(PetID.Quantity)',\n",
    "     'Breed1.MODE(PetID.VideoAmt)',\n",
    "     'Breed2.MODE(PetID.Fee)',\n",
    "     'Breed2.MODE(PetID.Health)',\n",
    "     'Breed2.MODE(PetID.Quantity)',\n",
    "     'Breed2.MODE(PetID.VideoAmt)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Type)',\n",
    "     'VideoAmt.MODE(PetID.Color1)',\n",
    "     'VideoAmt.MODE(PetID.Breed2)',\n",
    "     'VideoAmt.MODE(PetID.Breed1)',\n",
    "     'Type.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Vaccinated.MODE(PetID.Color3)',\n",
    "     'Vaccinated.MODE(PetID.Color2)',\n",
    "     'Vaccinated.MODE(PetID.Color1)',\n",
    "     'Vaccinated.MODE(PetID.Breed2)',\n",
    "     'Vaccinated.MODE(PetID.Breed1)',\n",
    "     'Vaccinated.MODE(PetID.Age)',\n",
    "     'Type.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Type.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Type.NUM_UNIQUE(PetID.State)',\n",
    "     'Vaccinated.MODE(PetID.Fee)',\n",
    "     'Type.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Type.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'Type.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Type.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Type.NUM_UNIQUE(PetID.Health)',\n",
    "     'Type.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Type.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Type.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Vaccinated.MODE(PetID.Description)',\n",
    "     'Vaccinated.MODE(PetID.FurLength)',\n",
    "     'VideoAmt.MODE(PetID.Age)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Type)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.State)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Health)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Vaccinated.MODE(PetID.Gender)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Vaccinated.MODE(PetID.VideoAmt)',\n",
    "     'Vaccinated.MODE(PetID.Sterilized)',\n",
    "     'Vaccinated.MODE(PetID.State)',\n",
    "     'Vaccinated.MODE(PetID.Quantity)',\n",
    "     'Vaccinated.MODE(PetID.PhotoAmt)',\n",
    "     'Vaccinated.MODE(PetID.MaturitySize)',\n",
    "     'Vaccinated.MODE(PetID.Health)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Color2)',\n",
    "     'PhotoAmt.MODE(PetID.Breed2)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'FurLength.MODE(PetID.Age)',\n",
    "     'Fee.MODE(PetID.Health)',\n",
    "     'Fee.MODE(PetID.MaturitySize)',\n",
    "     'Fee.MODE(PetID.Quantity)',\n",
    "     'Fee.MODE(PetID.State)',\n",
    "     'Fee.MODE(PetID.VideoAmt)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Gender)',\n",
    "     'FurLength.MODE(PetID.Breed2)',\n",
    "     'FurLength.MODE(PetID.MaturitySize)',\n",
    "     'FurLength.MODE(PetID.Color1)',\n",
    "     'FurLength.MODE(PetID.Color2)',\n",
    "     'FurLength.MODE(PetID.Color3)',\n",
    "     'FurLength.MODE(PetID.Dewormed)',\n",
    "     'FurLength.MODE(PetID.Fee)',\n",
    "     'FurLength.MODE(PetID.Gender)',\n",
    "     'Fee.MODE(PetID.Dewormed)',\n",
    "     'Fee.MODE(PetID.Color3)',\n",
    "     'Fee.MODE(PetID.Breed2)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Type)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.State)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Health)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Dewormed.MODE(PetID.VideoAmt)',\n",
    "     'Dewormed.MODE(PetID.Sterilized)',\n",
    "     'Dewormed.MODE(PetID.State)',\n",
    "     'FurLength.MODE(PetID.Health)',\n",
    "     'FurLength.MODE(PetID.PhotoAmt)',\n",
    "     'Dewormed.MODE(PetID.PhotoAmt)',\n",
    "     'Gender.MODE(PetID.Fee)',\n",
    "     'Gender.MODE(PetID.Breed2)',\n",
    "     'Gender.MODE(PetID.Color1)',\n",
    "     'Gender.MODE(PetID.Color2)',\n",
    "     'Gender.MODE(PetID.Color3)',\n",
    "     'Gender.MODE(PetID.Description)',\n",
    "     'Gender.MODE(PetID.Dewormed)',\n",
    "     'Gender.MODE(PetID.FurLength)',\n",
    "     'FurLength.MODE(PetID.Quantity)',\n",
    "     'Gender.MODE(PetID.Health)',\n",
    "     'Gender.MODE(PetID.MaturitySize)',\n",
    "     'Gender.MODE(PetID.PhotoAmt)',\n",
    "     'Gender.MODE(PetID.Quantity)',\n",
    "     'Gender.MODE(PetID.RescuerID)',\n",
    "     'Gender.MODE(PetID.State)',\n",
    "     'Gender.MODE(PetID.Breed1)',\n",
    "     'Gender.MODE(PetID.Age)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Type)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Health)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Gender)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Color3)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Color2)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Color1)',\n",
    "     'FurLength.MODE(PetID.VideoAmt)',\n",
    "     'FurLength.MODE(PetID.Vaccinated)',\n",
    "     'FurLength.MODE(PetID.Type)',\n",
    "     'FurLength.MODE(PetID.Sterilized)',\n",
    "     'FurLength.MODE(PetID.State)',\n",
    "     'Dewormed.MODE(PetID.Quantity)',\n",
    "     'Dewormed.MODE(PetID.Name)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Color2.MODE(PetID.MaturitySize)',\n",
    "     'Color2.MODE(PetID.Color3)',\n",
    "     'Color2.MODE(PetID.Dewormed)',\n",
    "     'Color2.MODE(PetID.Fee)',\n",
    "     'Color2.MODE(PetID.FurLength)',\n",
    "     'Color2.MODE(PetID.Gender)',\n",
    "     'Color2.MODE(PetID.Health)',\n",
    "     'Color2.MODE(PetID.Quantity)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Color2.MODE(PetID.State)',\n",
    "     'Color2.MODE(PetID.Sterilized)',\n",
    "     'Color2.MODE(PetID.VideoAmt)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Color2.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Color2.MODE(PetID.Breed2)',\n",
    "     'Color2.MODE(PetID.Age)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Type)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Color1.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Health)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Color1.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Color1.MODE(PetID.VideoAmt)',\n",
    "     'Color1.MODE(PetID.Sterilized)',\n",
    "     'Color1.MODE(PetID.State)',\n",
    "     'Color1.MODE(PetID.Quantity)',\n",
    "     'Color1.MODE(PetID.MaturitySize)',\n",
    "     'Color1.MODE(PetID.Health)',\n",
    "     'Color1.MODE(PetID.FurLength)',\n",
    "     'Color2.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Type)',\n",
    "     'Dewormed.MODE(PetID.MaturitySize)',\n",
    "     'Dewormed.MODE(PetID.Breed2)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Type)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Color1.MODE(PetID.Color3)',\n",
    "     'Dewormed.MODE(PetID.Age)',\n",
    "     'Dewormed.MODE(PetID.Breed1)',\n",
    "     'Dewormed.MODE(PetID.Color1)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Dewormed.MODE(PetID.Color3)',\n",
    "     'Dewormed.MODE(PetID.Description)',\n",
    "     'Dewormed.MODE(PetID.Fee)',\n",
    "     'Dewormed.MODE(PetID.FurLength)',\n",
    "     'Dewormed.MODE(PetID.Gender)',\n",
    "     'Dewormed.MODE(PetID.Health)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Color3.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Color3.MODE(PetID.VideoAmt)',\n",
    "     'Color3.MODE(PetID.Vaccinated)',\n",
    "     'Color3.MODE(PetID.Sterilized)',\n",
    "     'Color3.MODE(PetID.State)',\n",
    "     'Color3.MODE(PetID.Quantity)',\n",
    "     'Color3.MODE(PetID.MaturitySize)',\n",
    "     'Color3.MODE(PetID.Health)',\n",
    "     'Color3.MODE(PetID.Gender)',\n",
    "     'Color3.MODE(PetID.FurLength)',\n",
    "     'Color3.MODE(PetID.Fee)',\n",
    "     'Color3.MODE(PetID.Color2)',\n",
    "     'Color3.MODE(PetID.Color1)',\n",
    "     'Color3.MODE(PetID.Breed2)',\n",
    "     'Color3.MODE(PetID.Age)',\n",
    "     'Gender.MODE(PetID.Sterilized)',\n",
    "     'Gender.MODE(PetID.Type)',\n",
    "     'Gender.MODE(PetID.Vaccinated)',\n",
    "     'Quantity.MODE(PetID.VideoAmt)',\n",
    "     'Quantity.MODE(PetID.Gender)',\n",
    "     'Quantity.MODE(PetID.Health)',\n",
    "     'Quantity.MODE(PetID.MaturitySize)',\n",
    "     'Quantity.MODE(PetID.State)',\n",
    "     'Quantity.MODE(PetID.Sterilized)',\n",
    "     'Quantity.MODE(PetID.Vaccinated)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'State.MODE(PetID.Color3)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Type)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'RescuerID.NUM_UNIQUE(PetID.State)',\n",
    "     'State.MODE(PetID.Breed2)',\n",
    "     'State.MODE(PetID.Color1)',\n",
    "     'Quantity.MODE(PetID.FurLength)',\n",
    "     'Quantity.MODE(PetID.Fee)',\n",
    "     'Quantity.MODE(PetID.Color1)',\n",
    "     'Quantity.MODE(PetID.Breed2)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Type)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Gender)',\n",
    "     'PhotoAmt.MODE(PetID.VideoAmt)',\n",
    "     'PhotoAmt.MODE(PetID.Sterilized)',\n",
    "     'PhotoAmt.MODE(PetID.Quantity)',\n",
    "     'PhotoAmt.MODE(PetID.MaturitySize)',\n",
    "     'PhotoAmt.MODE(PetID.Health)',\n",
    "     'PhotoAmt.MODE(PetID.FurLength)',\n",
    "     'PhotoAmt.MODE(PetID.Fee)',\n",
    "     'PhotoAmt.MODE(PetID.Dewormed)',\n",
    "     'PhotoAmt.MODE(PetID.Color3)',\n",
    "     'PhotoAmt.MODE(PetID.Color1)',\n",
    "     'State.MODE(PetID.Color2)',\n",
    "     'State.MODE(PetID.Fee)',\n",
    "     'Gender.MODE(PetID.VideoAmt)',\n",
    "     'Sterilized.MODE(PetID.MaturitySize)',\n",
    "     'Sterilized.MODE(PetID.Color3)',\n",
    "     'Sterilized.MODE(PetID.Dewormed)',\n",
    "     'Sterilized.MODE(PetID.Fee)',\n",
    "     'Sterilized.MODE(PetID.FurLength)',\n",
    "     'Sterilized.MODE(PetID.Gender)',\n",
    "     'Sterilized.MODE(PetID.Health)',\n",
    "     'Sterilized.MODE(PetID.PhotoAmt)',\n",
    "     'State.MODE(PetID.FurLength)',\n",
    "     'Sterilized.MODE(PetID.Quantity)',\n",
    "     'Sterilized.MODE(PetID.State)',\n",
    "     'Sterilized.MODE(PetID.Type)',\n",
    "     'Sterilized.MODE(PetID.VideoAmt)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Sterilized.MODE(PetID.Color2)',\n",
    "     'Sterilized.MODE(PetID.Color1)',\n",
    "     'Sterilized.MODE(PetID.Breed2)',\n",
    "     'Sterilized.MODE(PetID.Breed1)',\n",
    "     'State.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'State.NUM_UNIQUE(PetID.Type)',\n",
    "     'State.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'State.NUM_UNIQUE(PetID.Gender)',\n",
    "     'State.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'State.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'State.NUM_UNIQUE(PetID.Color2)',\n",
    "     'State.MODE(PetID.VideoAmt)',\n",
    "     'State.MODE(PetID.Sterilized)',\n",
    "     'State.MODE(PetID.Quantity)',\n",
    "     'State.MODE(PetID.MaturitySize)',\n",
    "     'State.MODE(PetID.Health)',\n",
    "     'State.MODE(PetID.Gender)',\n",
    "     'Color1.MODE(PetID.Fee)',\n",
    "     'Color1.MODE(PetID.Age)',\n",
    "     'Color1.MODE(PetID.Breed2)',\n",
    "     'Health.MODE(PetID.Quantity)',\n",
    "     'Health.MODE(PetID.Dewormed)',\n",
    "     'Health.MODE(PetID.Fee)',\n",
    "     'Health.MODE(PetID.FurLength)',\n",
    "     'Health.MODE(PetID.Gender)',\n",
    "     'Health.MODE(PetID.MaturitySize)',\n",
    "     'Health.MODE(PetID.PhotoAmt)',\n",
    "     'Health.MODE(PetID.RescuerID)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Health.MODE(PetID.State)',\n",
    "     'Health.MODE(PetID.Sterilized)',\n",
    "     'Health.MODE(PetID.Type)',\n",
    "     'Health.MODE(PetID.Vaccinated)',\n",
    "     'Health.MODE(PetID.VideoAmt)',\n",
    "     'Health.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Health.MODE(PetID.Color3)',\n",
    "     'Health.MODE(PetID.Color2)',\n",
    "     'Health.MODE(PetID.Color1)',\n",
    "     'Health.MODE(PetID.Breed2)',\n",
    "     'Health.MODE(PetID.Breed1)',\n",
    "     'Health.MODE(PetID.Age)',\n",
    "     'Gender.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Type)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Gender.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Health)',\n",
    "     'Gender.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Health.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Health.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Health.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'MaturitySize.MODE(PetID.Health)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Type)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.State)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Health)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Gender)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Color3)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Color2)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Color1)',\n",
    "     'MaturitySize.MODE(PetID.Sterilized)',\n",
    "     'MaturitySize.MODE(PetID.State)',\n",
    "     'MaturitySize.MODE(PetID.Quantity)',\n",
    "     'MaturitySize.MODE(PetID.FurLength)',\n",
    "     'Health.NUM_UNIQUE(PetID.Gender)',\n",
    "     'MaturitySize.MODE(PetID.Fee)',\n",
    "     'MaturitySize.MODE(PetID.Dewormed)',\n",
    "     'MaturitySize.MODE(PetID.Color3)',\n",
    "     'MaturitySize.MODE(PetID.Color2)',\n",
    "     'MaturitySize.MODE(PetID.Color1)',\n",
    "     'MaturitySize.MODE(PetID.Breed2)',\n",
    "     'MaturitySize.MODE(PetID.Age)',\n",
    "     'Health.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Health.NUM_UNIQUE(PetID.Type)',\n",
    "     'Health.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Health.NUM_UNIQUE(PetID.State)',\n",
    "     'Health.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Health.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'Health.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'MaturitySize.MODE(PetID.VideoAmt)']]\n",
    "    \n",
    "    new_columns = []\n",
    "    for i in features.columns:\n",
    "        new_columns.append('featuretools_' + i)\n",
    "    features.columns = new_columns\n",
    "    del data\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data_temp = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data_temp = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    temp_data = pd.concat([train_data_temp, test_data_temp], axis=0)\n",
    "    features['PetID'] = list(temp_data['PetID'])\n",
    "    del train_data_temp,test_data_temp,temp_data\n",
    "    gc.collect()\n",
    "    return features\n",
    "    \n",
    "\n",
    "nurbs=get_feat3()\n",
    "train = pd.merge(train1,nurbs, on='PetID', how='left')\n",
    "test = pd.merge(test1, nurbs, on='PetID', how='left')\n",
    "features = [x for x in train.columns if x not in ['Breed1',\"breed\",\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "\n",
    "label='AdoptionSpeed'\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "#          'max_depth':11,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.4,\n",
    "          'min_split_gain': 0.01,\n",
    "#           'min_child_samples': 30,\n",
    "#           \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "\n",
    "\n",
    "def runCAT(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = Pool(train_X, label=train_y)\n",
    "    d_valid = Pool(test_X, label=test_y)\n",
    "    watchlist = (d_train, d_valid)\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = CatBoostRegressor(iterations=num_rounds, \n",
    "        learning_rate = 0.03,\n",
    "        od_type='Iter',\n",
    "         od_wait=early_stop,\n",
    "        loss_function='RMSE',\n",
    "        eval_metric='RMSE',\n",
    "        bagging_temperature=0.9,                   \n",
    "        random_seed = 2019,\n",
    "        task_type='GPU'\n",
    "                          )\n",
    "    model.fit(d_train,eval_set=d_valid,\n",
    "            use_best_model=True,\n",
    "            verbose=verbose_eval\n",
    "                         )\n",
    "    \n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    log=0#log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "#     pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2)\n",
    "#     pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), 0, coefficients, qwk,log\n",
    "###model 5\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "lgb5_train=[r[0] for r in results['train']]\n",
    "lgb5_test=[r[0] for r in results['test']]\n",
    "t5=time.time()\n",
    "print(\"model5 cost:{} s\".format(t5-t4))\n",
    "\n",
    "train = pd.merge(train2, nurbs, on='PetID', how='left')\n",
    "test = pd.merge(test2, nurbs, on='PetID', how='left')\n",
    "\n",
    "del nurbs\n",
    "gc.collect()\n",
    "###model 6\n",
    "features = [x for x in train.columns if x not in ['label_description',\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "results = run_cv_model(train[features], test[features], train[label], runCAT, params, rmse, 'CAT')\n",
    "\n",
    "cat2_train=[r[0] for r in results['train']]\n",
    "cat2_test=[r[0] for r in results['test']]\n",
    "t6=time.time()\n",
    "print(\"model6 cost:{} s\".format(t6-t5))\n",
    "\n",
    "\n",
    "####model 7\n",
    "features = [x for x in train.columns if x not in [\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "#          'max_depth':11,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.4,\n",
    "          'min_split_gain': 0.01,\n",
    "#           'min_child_samples': 30,\n",
    "#           \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "lgb6_train=[r[0] for r in results['train']]\n",
    "lgb6_test=[r[0] for r in results['test']]\n",
    "del train,test,results\n",
    "gc.collect()\n",
    "t7=time.time()\n",
    "print(\"model7 cost:{} s\".format(t7-t6))\n",
    "\n",
    "def nn1_model(train_img_feat=train_img_feat,test_img_feat=test_img_feat):\n",
    "    train_data = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    \n",
    "    breed=pd.read_csv(\"../input/petfinder-adoption-prediction/breed_labels.csv\")\n",
    "    color = pd.read_csv(\"../input/petfinder-adoption-prediction/color_labels.csv\")\n",
    "    \n",
    "    color_dict = dict(zip(color['ColorID'].values.astype(\"str\"),color['ColorName'].values))\n",
    "    breed_dict = dict(zip(breed['BreedID'].values.astype(\"str\"),breed['BreedName'].values))\n",
    "    \n",
    "    def get_purebreed_feat(df):\n",
    "        if df['Breed2']==0 and df['Breed1']!=307:\n",
    "            return 1\n",
    "        return 0\n",
    "    train_data['purebreed']=train_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    test_data['purebreed']=test_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    \n",
    "    train_data['mixed']=train_data['Gender'].apply(lambda x:1 if x==3 else 0)\n",
    "    test_data['mixed']=test_data['Gender'].apply(lambda x:1 if x==3 else 0)\n",
    "    \n",
    "    \n",
    "    train_data['lan_type'] = train_data.Description.map(lambda x:lan_type(x))\n",
    "    train_data['malai_type'] = train_data.Description.map(lambda x:malai_type(x))\n",
    "    \n",
    "    test_data['lan_type'] = test_data.Description.map(lambda x:lan_type(x))\n",
    "    test_data['malai_type'] = test_data.Description.map(lambda x:malai_type(x))\n",
    "    \n",
    "    def name_deal(df):\n",
    "        if \"No Name\" in df:\n",
    "            return np.nan\n",
    "        if df ==\"nan\":\n",
    "            return np.nan\n",
    "        else:\n",
    "            return df\n",
    "    train_data['Name'] = train_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    test_data['Name'] = test_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    \n",
    "    train_data['Name'],indexer=pd.factorize(train_data['Name'])\n",
    "    test_data['Name'] = indexer.get_indexer(test_data['Name'])\n",
    "    \n",
    "    group = train_data['RescuerID'].values\n",
    "    \n",
    "    rescuer_df=train_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    train_data=pd.merge(train_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    train_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    rescuer_df=test_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    test_data=pd.merge(test_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    test_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    train_data[\"num_chars\"] = train_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    test_data[\"num_chars\"] = test_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    train_data['Description'] = train_data['Description'].fillna(\"None\")\n",
    "    test_data['Description'] = test_data['Description'].fillna(\"None\")\n",
    "    \n",
    "    train_data[\"Description\"] = train_data[\"Description\"].str.lower()\n",
    "    test_data[\"Description\"] = test_data[\"Description\"].str.lower()\n",
    "    \n",
    "    puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "     '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "     '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "     '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "     '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "    def clean_text(x):\n",
    "        for punct in puncts:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    train_data[\"Description\"] = train_data[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    test_data[\"Description\"] = test_data[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "\n",
    "    def deal_desc(df):\n",
    "        if df['lan_type']==1:\n",
    "            return \"null\"\n",
    "        if df['lan_type']==3:\n",
    "            text=jieba.cut(df['Description'])\n",
    "            text=\" \".join(text)\n",
    "            text=text.replace(\"   \",\" \")\n",
    "            return text\n",
    "        else:\n",
    "            return df['Description']\n",
    "    train_data['Description']=train_data.apply(lambda x:deal_desc(x), 1)\n",
    "    test_data['Description']=test_data.apply(lambda x:deal_desc(x), 1)\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    train_data['doc_sent_mag'] = doc_sent_mag\n",
    "    train_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    test_data['doc_sent_mag'] = doc_sent_mag\n",
    "    test_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    print(nl_count)\n",
    "    train_data[ 'vertex_x'] = vertex_xs\n",
    "    train_data['vertex_y'] = vertex_ys\n",
    "    train_data['bounding_confidence'] = bounding_confidences\n",
    "    train_data['bounding_importance'] = bounding_importance_fracs\n",
    "    train_data['dominant_blue'] = dominant_blues\n",
    "    train_data['dominant_green'] = dominant_greens\n",
    "    train_data['dominant_red'] = dominant_reds\n",
    "    train_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    train_data['dominant_score'] = dominant_scores\n",
    "    train_data['label_description'] = label_descriptions\n",
    "    train_data['label_score'] = label_scores\n",
    "    \n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    test_data[ 'vertex_x'] = vertex_xs\n",
    "    test_data['vertex_y'] = vertex_ys\n",
    "    test_data['bounding_confidence'] = bounding_confidences\n",
    "    test_data['bounding_importance'] = bounding_importance_fracs\n",
    "    test_data['dominant_blue'] = dominant_blues\n",
    "    test_data['dominant_green'] = dominant_greens\n",
    "    test_data['dominant_red'] = dominant_reds\n",
    "    test_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    test_data['dominant_score'] = dominant_scores\n",
    "    test_data['label_description'] = label_descriptions\n",
    "    test_data['label_score'] = label_scores\n",
    "    \n",
    "    del  vertex_xs,vertex_ys,bounding_confidences,bounding_importance_fracs,dominant_blues,dominant_greens,dominant_reds,dominant_pixel_fracs,dominant_scores\n",
    "    del label_descriptions,label_scores,doc_sent_mag,doc_sent_score\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['label_description'] =train_data['label_description'].astype(np.str)\n",
    "    test_data['label_description'] =test_data['label_description'].astype(np.str)\n",
    "    \n",
    "    def get_text(df):\n",
    "        x=\"\"\n",
    "        if df['Type']==1:\n",
    "            x+=\"dog\"+\" \"\n",
    "        if df['Type']==2:\n",
    "            x+=\"cat\"+\" \"\n",
    "        for i in ['Breed1',\"Breed2\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=breed_dict[str(df[i])]+\" \"\n",
    "        for i in [\"Color1\",\"Color2\",\"Color3\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=color_dict[str(df[i])]+\" \"\n",
    "        x+=df['label_description']+\" \"\n",
    "        x=x+df['Description']\n",
    "        return x\n",
    "    train_data['Description']=train_data.apply(lambda x:get_text(x),1)\n",
    "    test_data['Description']=test_data.apply(lambda x:get_text(x),1)\n",
    "    \n",
    "    text_list = train_data['Description'].values.tolist()\n",
    "    text_list.extend(test_data['Description'].values.tolist())\n",
    "    \n",
    "    documents = text_list\n",
    "    texts = [[word for word in str(document).split(' ') ] for document in documents]\n",
    "    \n",
    "    \n",
    "    \n",
    "    w2v = Word2Vec(texts, size=128, window=7, iter=8, seed=10, workers=2, min_count=3)\n",
    "    w2v.wv.save_word2vec_format('w2v_128.txt')\n",
    "    print(\"w2v model done\")\n",
    "    del w2v\n",
    "    gc.collect()\n",
    "    embed_size = 128 # how big is each word vector\n",
    "    max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
    "    maxlen = 230 # max number of words in a question to use\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    train_X = train_data[\"Description\"].values\n",
    "    test_X = test_data[\"Description\"].values\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "    tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_y = train_data['AdoptionSpeed'].values\n",
    "    \n",
    "    word_index=tokenizer.word_index\n",
    "    features = [x for x in train_data.columns if x not in [\"num_words\",\"num_unique_words\",\"num_stopwords\",\"num_punctuations\",\"mean_word_len\",'label_description',\"Name\", 'PetID', \"Description\", 'AdoptionSpeed']]\n",
    "    \n",
    "    cate_col=['Breed1', 'Breed2', 'Color1', 'Color2', 'Color3', 'State']\n",
    "    onehot_col=['Type','Gender','MaturitySize','MaturitySize','FurLength','Vaccinated',\n",
    "               'Dewormed','Sterilized','Health','purebreed','Color1', 'Color2', 'Color3', ]\n",
    "    num_col=features\n",
    "    \n",
    "   \n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    data = pd.concat([train_data, test_data])\n",
    "    sc.fit(data[num_col])\n",
    "    del data\n",
    "    gc.collect()\n",
    "    train_data[num_col] = sc.transform(train_data[num_col])\n",
    "    test_data[num_col] = sc.transform(test_data[num_col])\n",
    "    train_num_feat = train_data[num_col]\n",
    "    test_num_feat = test_data[num_col]\n",
    "    \n",
    "    train_img_feat.reset_index(inplace=True)\n",
    "    test_img_feat.reset_index(inplace=True)\n",
    "    train_img_feat.columns = [\"PetID\"]+[\"img_\"+str(i) for i in range(train_img_feat.shape[1]-1)]\n",
    "    test_img_feat.columns = [\"PetID\"]+[\"img_\"+str(i) for i in range(train_img_feat.shape[1]-1)]\n",
    "    del train_img_feat['PetID'], test_img_feat['PetID']\n",
    "    train_num_feat = pd.concat([train_num_feat, train_img_feat], axis=1).values\n",
    "    test_num_feat = pd.concat([test_num_feat, test_img_feat], axis=1).values\n",
    "    \n",
    "    embedding_matrix=get_embedding_matrix(word_index)\n",
    "    \n",
    "    def hybrid_model(embedding_matrix):\n",
    "        K.clear_session()\n",
    "        inp_text = Input(shape=(maxlen, ))\n",
    "        emb = Embedding(\n",
    "            input_dim=embedding_matrix.shape[0],\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=False)(inp_text)\n",
    "        x = SpatialDropout1D(rate=0.22)(emb)\n",
    "        x = Bidirectional(CuDNNLSTM(120, return_sequences=True, kernel_initializer=glorot_uniform(seed=123)))(x)  \n",
    "        x1 = Conv1D(filters=100, kernel_size=1, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x2 = Conv1D(filters=90, kernel_size=2, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x3 = Conv1D(filters=30, kernel_size=3, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x4 = Conv1D(filters=10, kernel_size=5, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "    \n",
    "        x1 = GlobalMaxPool1D()(x1)\n",
    "        x2 = GlobalMaxPool1D()(x2)\n",
    "        x3 = GlobalMaxPool1D()(x3)\n",
    "        x4 = GlobalMaxPool1D()(x4)\n",
    "        x5 = AttentionWeightedAverage()(x)\n",
    "        \n",
    "        inp_num = Input(shape=(293, ))\n",
    "        x = concatenate([x1, x2, x3, x4, x5, inp_num])\n",
    "        x = Dense(200, kernel_initializer='glorot_uniform', activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.22)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(200, kernel_initializer='glorot_uniform', activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.22)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        out = Dense(1, kernel_initializer=glorot_uniform(seed=123))(x)\n",
    "    \n",
    "        model = Model(inputs=[inp_text, inp_num], outputs=out)\n",
    "        model.compile(loss='mean_squared_error', optimizer=AdamW(weight_decay=0.02))\n",
    "        return model\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=1017, shuffle=True)\n",
    "    pred_oof=np.zeros((train_X.shape[0], ))\n",
    "    y_test = np.zeros((test_X.shape[0], ))\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    all_coefficients = np.zeros((5, 4))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(train_X, train_y)):\n",
    "        print(\"FOLD | {}/{}\".format(i+1,5))\n",
    "        X_tr, X_vl, X_tr2, X_vl2, y_tr, y_vl = train_X[train_index], train_X[test_index], train_num_feat[\n",
    "            train_index], train_num_feat[test_index], train_y[train_index], train_y[test_index]\n",
    "        #X_tr0 = get_keras_data(X_trall,  cate_col)\n",
    "        #X_tr0['text']=X_tr\n",
    "        #X_tr0['num']=X_tr2\n",
    "        #X_vl0 = get_keras_data(X_vlall,  cate_col)\n",
    "        #X_vl0['text']=X_vl\n",
    "        #X_vl0['num']=X_vl2\n",
    "        filepath=\"weights_best.h5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=0.00001, verbose=2)\n",
    "        earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=2, mode='auto')\n",
    "        callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "        model = hybrid_model(embedding_matrix)\n",
    "        if i == 0:print(model.summary()) \n",
    "        model.fit([X_tr, X_tr2], y_tr, batch_size=128, epochs=20, validation_data=([X_vl, X_vl2], y_vl), verbose=2, callbacks=callbacks,)\n",
    "        model.load_weights(filepath)    \n",
    "        y_pred = np.squeeze(model.predict([X_vl, X_vl2], batch_size=256, verbose=2))\n",
    "        pred_oof[test_index] = y_pred\n",
    "        y_test += np.squeeze(model.predict([test_X, test_num_feat], batch_size=256, verbose=2))/5\n",
    "        optR = OptimizedRounder()\n",
    "        optR.fit(y_pred, y_vl)\n",
    "        len_0 = sum([1 for i in y_vl if i==0])\n",
    "        coefficients = optR.coefficients()\n",
    "        pred_test_y_k = optR.predict(y_pred, coefficients, len_0)\n",
    "        print(\"Valid Counts = \", Counter(y_vl))\n",
    "        print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "        print(\"Coefficients = \", coefficients)\n",
    "        \n",
    "        qwk = cohen_kappa_score(y_vl, pred_test_y_k,weights='quadratic')\n",
    "        cv_score = rmse(y_vl, y_pred)\n",
    "        cv_scores.append(cv_score)\n",
    "        qwk_scores.append(qwk)\n",
    "        all_coefficients[i, :] = coefficients\n",
    "        print( ' cv score {}: RMSE {} QWK {}'.format(i+1, cv_score, qwk))\n",
    "        print(\"##\"*40)\n",
    "        \n",
    "    print('cv mean RMSE score : {}'.format( np.mean(cv_scores)))\n",
    "    print('cv std RMSE score : {}'.format( np.std(cv_scores)))\n",
    "    print('cv mean QWK score : {}'.format( np.mean(qwk_scores)))\n",
    "    print('cv std QWK score : {}'.format( np.std(qwk_scores)))  \n",
    "    \n",
    "    del train_num_feat,test_num_feat,train_X,test_X\n",
    "    gc.collect()\n",
    "    \n",
    "    nn1_train = [r for r in pred_oof]\n",
    "    nn1_test = [r for r in y_test]\n",
    "    \n",
    "    return nn1_train,nn1_test,embedding_matrix,train_img_feat,test_img_feat,train_data,test_data\n",
    "\n",
    "###model 8\n",
    "###nn1\n",
    "nn1_train,nn1_test,embedding_matrix,train_img_feat,test_img_feat,train_data,test_data=nn1_model()\n",
    "t8=time.time()\n",
    "print(\"model8 cost:{} s\".format(t8-t7))\n",
    "####model 9\n",
    "###nn2\n",
    "def nn2_model(train,test,embedding_matrix,train_img_feat,test_img_feat):\n",
    "    \n",
    "    embed_size = 128 # how big is each word vector\n",
    "    max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
    "    maxlen = 220 # max number of words in a question to use\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    train_X = train[\"concat_text\"].values\n",
    "    test_X = test[\"concat_text\"].values\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "    tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_y = train['AdoptionSpeed'].values\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    features = [x for x in train.columns if x not in [\"num_words\",\"num_unique_words\",\"num_stopwords\",\"num_punctuations\",\"mean_word_len\",'Breed1',\"breed\",\"color\",\"Breed2\",\"State\",\"concat_text\",'label_description',\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "\n",
    "    num_col = features\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    data = pd.concat([train, test])\n",
    "    sc.fit(data[num_col])\n",
    "    del data\n",
    "    gc.collect()\n",
    "    train[num_col] = sc.transform(train[num_col])\n",
    "    test[num_col] = sc.transform(test[num_col])\n",
    "    train_num_feat = train[num_col]\n",
    "    test_num_feat = test[num_col]\n",
    "    \n",
    "    train_num_feat = pd.concat([train_num_feat, train_img_feat], axis=1).values\n",
    "    test_num_feat = pd.concat([test_num_feat, test_img_feat], axis=1).values\n",
    "    \n",
    "    \n",
    "    def hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.01):\n",
    "        K.clear_session()\n",
    "        inp_text = Input(shape=(maxlen, ))\n",
    "        emb = Embedding(\n",
    "            input_dim=embedding_matrix.shape[0],\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=False)(inp_text)\n",
    "        x = SpatialDropout1D(rate=sp, seed=1024)(emb)\n",
    "        x = Bidirectional(CuDNNLSTM(128, return_sequences=True, kernel_initializer=glorot_uniform(seed=123), \n",
    "                                    recurrent_initializer=orthogonal(gain=1.0, seed=10000)))(x)\n",
    "        #xx = Bidirectional(CuDNNGRU(60, return_sequences=False, kernel_initializer=glorot_uniform(seed=123)))(x)\n",
    "        #x1 = Conv1D(filters=filters[0], kernel_size=1, kernel_initializer=glorot_uniform(seed=123),\n",
    "        #               padding='same', activation='relu')(x)\n",
    "        c = Conv1D(filters=filters[1], kernel_size=2, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        #x3 = Conv1D(filters=filters[2], kernel_size=3, kernel_initializer=glorot_uniform(seed=123),\n",
    "        #               padding='same', activation='relu')(x)\n",
    "        #x4 = Conv1D(filters=10, kernel_size=5, kernel_initializer=glorot_uniform(seed=123),\n",
    "        #               padding='same', activation='relu')(x)\n",
    "    \n",
    "        #x1 = GlobalMaxPool1D()(x1)\n",
    "        x2 = GlobalMaxPool1D()(c)\n",
    "        x3 = GlobalAvgPool1D()(c)\n",
    "        #x3 = GlobalMaxPool1D()(x3)\n",
    "        #x4 = GlobalMaxPool1D()(x4)\n",
    "        x5 = AttentionWeightedAverage()(x)\n",
    "        \n",
    "        inp_num = Input(shape=(test_num_feat.shape[1], ))\n",
    "        x = concatenate([x2, x3, x5, inp_num])\n",
    "        x = Dense(200, kernel_initializer=glorot_uniform(seed=123), activation=gelu\n",
    "                 )(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.23, seed=1024)(x)\n",
    "        #x = BatchNormalization()(x)\n",
    "        #x = Dense(200, kernel_initializer=glorot_uniform(seed=123), activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        #x = Dropout(0.23, seed=1024)(x)\n",
    "        #x = BatchNormalization()(x)\n",
    "        out = Dense(1, kernel_initializer=glorot_uniform(seed=123))(x)\n",
    "    \n",
    "        model = Model(inputs=[inp_text, inp_num], outputs=out)\n",
    "        model.compile(loss='mean_squared_error', optimizer=AdamW(weight_decay=weight_decay))\n",
    "        #model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "        return model\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=1017, shuffle=True)\n",
    "    pred_oof=np.zeros((train_X.shape[0], ))\n",
    "    y_test = np.zeros((test_X.shape[0], ))\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    all_coefficients = np.zeros((5, 4))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(train_X, train_y)):\n",
    "        print(\"FOLD | {}/{}\".format(i+1,5))\n",
    "        X_tr, X_vl, X_tr2, X_vl2, y_tr, y_vl = train_X[train_index], train_X[test_index], train_num_feat[\n",
    "            train_index], train_num_feat[test_index], train_y[train_index], train_y[test_index]\n",
    "        \n",
    "        filepath=\"weights_best.h5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=0.00001, verbose=2)\n",
    "        earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=2, mode='auto')\n",
    "        callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "        if i == 0:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        elif i == 1:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        elif i == 2:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        elif i == 3:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        elif i == 4:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        if i == 0:print(model.summary()) \n",
    "        model.fit([X_tr, X_tr2], y_tr, batch_size=128, epochs=20, validation_data=([X_vl, X_vl2], y_vl), verbose=2, callbacks=callbacks,)\n",
    "        model.load_weights(filepath)    \n",
    "        y_pred = np.squeeze(model.predict([X_vl, X_vl2], batch_size=256, verbose=2))\n",
    "        pred_oof[test_index] = y_pred\n",
    "        y_test += np.squeeze(model.predict([test_X, test_num_feat], batch_size=256, verbose=2))/5\n",
    "        optR = OptimizedRounder()\n",
    "        optR.fit(y_pred, y_vl)\n",
    "        len_0 = sum([1 for i in y_vl if i==0])\n",
    "        coefficients = optR.coefficients()\n",
    "        pred_test_y_k = optR.predict(y_pred, coefficients, len_0)\n",
    "        print(\"Valid Counts = \", Counter(y_vl))\n",
    "        print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "        print(\"Coefficients = \", coefficients)\n",
    "        qwk = cohen_kappa_score(y_vl, pred_test_y_k,weights='quadratic')\n",
    "        cv_score = rmse(y_vl, y_pred)\n",
    "        cv_scores.append(cv_score)\n",
    "        qwk_scores.append(qwk)\n",
    "        all_coefficients[i, :] = coefficients\n",
    "        print( ' cv score {}: RMSE {} QWK {}'.format(i+1, cv_score, qwk))\n",
    "        print(\"##\"*40)\n",
    "        \n",
    "    print('cv mean RMSE score : {}'.format( np.mean(cv_scores)))\n",
    "    print('cv std RMSE score : {}'.format( np.std(cv_scores)))\n",
    "    print('cv mean QWK score : {}'.format( np.mean(qwk_scores)))\n",
    "    print('cv std QWK score : {}'.format( np.std(qwk_scores)))\n",
    "    \n",
    "    nn2_train = [r for r in pred_oof]\n",
    "    nn2_test = [r for r in y_test]\n",
    "    del train_X,test_X\n",
    "    gc.collect()\n",
    "    \n",
    "    return nn2_train,nn2_test,train_num_feat,test_num_feat\n",
    "nn2_train,nn2_test,train_num_feat,test_num_feat=nn2_model(train1,test1,embedding_matrix,train_img_feat,test_img_feat)\n",
    "del train_img_feat,test_img_feat\n",
    "gc.collect()\n",
    "t9=time.time()\n",
    "print(\"model9 cost:{} s\".format(t9-t8))\n",
    "####model 10\n",
    "###nn3\n",
    "\n",
    "def nn3_model(train,test,embedding_matrix,train_num_feat,test_num_feat):\n",
    "    maxlen = 200\n",
    "    max_features = None \n",
    "    train_X = train[\"concat_text\"].values\n",
    "    test_X = test[\"concat_text\"].values\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "    tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "     ## Get the target values\n",
    "    train_y = train['AdoptionSpeed'].values\n",
    "    \n",
    "    def hybrid_model(embedding_matrix):\n",
    "        K.clear_session()\n",
    "        inp_text = Input(shape=(maxlen, ))\n",
    "        emb = Embedding(\n",
    "            input_dim=embedding_matrix.shape[0],\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=False)(inp_text)\n",
    "        x = SpatialDropout1D(rate=0.22)(emb)\n",
    "        x = Bidirectional(CuDNNLSTM(120, return_sequences=True, kernel_initializer=glorot_uniform(seed=123)))(x)  \n",
    "        x1 = Conv1D(filters=96, kernel_size=1, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x2 = Conv1D(filters=90, kernel_size=2, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x3 = Conv1D(filters=30, kernel_size=3, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x4 = Conv1D(filters=10, kernel_size=5, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "    \n",
    "        x1 = GlobalMaxPool1D()(x1)\n",
    "        x2 = GlobalMaxPool1D()(x2)\n",
    "        x3 = GlobalMaxPool1D()(x3)\n",
    "        x4 = GlobalMaxPool1D()(x4)\n",
    "        x5 = AttentionWeightedAverage()(x)\n",
    "        \n",
    "        inp_num = Input(shape=(test_num_feat.shape[1], ))\n",
    "        x = concatenate([x1, x2, x3, x5, inp_num])\n",
    "        x = Dense(200, kernel_initializer='glorot_uniform', activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.22)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(200, kernel_initializer='glorot_uniform', activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.22)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        out = Dense(5, activation=\"softmax\",kernel_initializer=glorot_uniform(seed=123))(x)\n",
    "    \n",
    "        model = Model(inputs=[inp_text, inp_num], outputs=out)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=AdamW(weight_decay=0.02))\n",
    "        return model\n",
    "    \n",
    "   \n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=1017, shuffle=True)\n",
    "    pred_oof=np.zeros((train_X.shape[0], ))\n",
    "    y_test = np.zeros((test_X.shape[0],))\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    all_coefficients = np.zeros((5, 4))\n",
    "    \n",
    "    y_label= to_categorical(train['AdoptionSpeed'])\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(train_X, train_y)):\n",
    "        print(\"FOLD | {}/{}\".format(i+1,5))\n",
    "        X_tr, X_vl, X_tr2, X_vl2, y_tr, y_vl = train_X[train_index], train_X[test_index], train_num_feat[\n",
    "            train_index], train_num_feat[test_index], y_label[train_index], y_label[test_index]\n",
    "        \n",
    "        filepath=\"weights_best.h5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=0.00001, verbose=2)\n",
    "        earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=2, mode='auto')\n",
    "        callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "        model = hybrid_model(embedding_matrix)\n",
    "        if i == 0:print(model.summary()) \n",
    "        model.fit([X_tr, X_tr2], y_tr, batch_size=128, epochs=20, validation_data=([X_vl, X_vl2], y_vl), verbose=2, callbacks=callbacks,)\n",
    "        model.load_weights(filepath)  \n",
    "        class_list=[0,1,2,3,4]\n",
    "        y_pred = np.squeeze(model.predict([X_vl, X_vl2], batch_size=256, verbose=2))\n",
    "        y_pred=np.array([sum(y_pred[ix]*class_list) for\n",
    "                                   ix in range(len(y_pred[:,0]))]) \n",
    "        pred_oof[test_index] = y_pred\n",
    "        test_temp = np.squeeze(model.predict([test_X, test_num_feat], batch_size=256, verbose=2))\n",
    "    \n",
    "        \n",
    "        test_temp=np.array([sum(test_temp[ix]*class_list) for\n",
    "                                   ix in range(len(test_temp[:,0]))]) \n",
    "        y_test+=np.squeeze(test_temp)/5\n",
    "        y_vl= train_y[test_index]\n",
    "        optR = OptimizedRounder()\n",
    "        optR.fit(y_pred, y_vl)\n",
    "        len_0 = sum([1 for i in y_vl if i==0])\n",
    "        coefficients = optR.coefficients()\n",
    "        pred_test_y_k = optR.predict(y_pred, coefficients, len_0)\n",
    "        print(\"Valid Counts = \", Counter(y_vl))\n",
    "        print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "        print(\"Coefficients = \", coefficients)\n",
    "        qwk = cohen_kappa_score(y_vl, pred_test_y_k,weights='quadratic')\n",
    "        cv_score = rmse(y_vl, y_pred)\n",
    "        cv_scores.append(cv_score)\n",
    "        qwk_scores.append(qwk)\n",
    "        all_coefficients[i, :] = coefficients\n",
    "        print( ' cv score {}: RMSE {} QWK {}'.format(i+1, cv_score, qwk))\n",
    "        print(\"##\"*40)\n",
    "        \n",
    "    print('cv mean RMSE score : {}'.format( np.mean(cv_scores)))\n",
    "    print('cv std RMSE score : {}'.format( np.std(cv_scores)))\n",
    "    print('cv mean QWK score : {}'.format( np.mean(qwk_scores)))\n",
    "    print('cv std QWK score : {}'.format( np.std(qwk_scores)))\n",
    "    \n",
    "    nn3_train = [r for r in pred_oof]\n",
    "    nn3_test = [r for r in y_test]\n",
    "    \n",
    "    del train_X,test_X\n",
    "    gc.collect()\n",
    "    return  nn3_train,nn3_test \n",
    "nn3_train,nn3_test=nn3_model(train1,test1,embedding_matrix,train_num_feat,test_num_feat)\n",
    "del embedding_matrix,train_num_feat,test_num_feat\n",
    "gc.collect()\n",
    "t10=time.time()\n",
    "print(\"model10 cost:{} s\".format(t10-t9))\n",
    "\n",
    "######weak model###############################################\n",
    "data = pd.concat([train_data,test_data])\n",
    "data.index=range(len(data))\n",
    "data_id=data['PetID'].values\n",
    "\n",
    "del train_desc,test_desc\n",
    "gc.collect()  \n",
    "\n",
    "cols = [x for x in train1.columns if x not in ['Breed1',\"breed\",\"color\",\"Breed2\",\"State\",'label_description',\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "\n",
    "train1[cols]=train1[cols].fillna(0)\n",
    "test1[cols]=test1[cols].fillna(0)\n",
    "############################ 切分数据集 ##########################\n",
    "print('开始进行一些前期处理')\n",
    "train_feature = train1[cols].values\n",
    "test_feature = test1[cols].values\n",
    "    # 五则交叉验证\n",
    "n_folds = 5\n",
    "print('处理完毕')\n",
    "df_stack3 = pd.DataFrame()\n",
    "df_stack3['PetID']=data['PetID']\n",
    "for label in [\"AdoptionSpeed\"]:\n",
    "    score = train_data[label]\n",
    "    \n",
    "   \n",
    "    ########################### SGD(随机梯度下降) ################################\n",
    "    # print('sgd stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     sgd = SGDRegressor(random_state=1017,)\n",
    "    #     sgd.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = sgd.predict(train_feature[va])\n",
    "    #     score_te = sgd.predict(test_feature)\n",
    "    #     print('得分' + str(mean_squared_error(score[va], sgd.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0]+= score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "#     df_stack3['tfidf_sgd_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "\n",
    "\n",
    "    ########################### pac(PassiveAggressiveClassifier) ################################\n",
    "    # print('PAC stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     pac = PassiveAggressiveRegressor(random_state=1017)\n",
    "    #     pac.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = pac.predict(train_feature[va])\n",
    "    #     score_te = pac.predict(test_feature)\n",
    "      \n",
    "    #     print('得分' + str(mean_squared_error(score[va], pac.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0] += score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "#     df_stack3['tfidf_pac_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ########################### FTRL ################################\n",
    "    print('MultinomialNB stacking')\n",
    "    stack_train = np.zeros((len(train_data),1))\n",
    "    stack_test = np.zeros((len(test_data),1))\n",
    "    score_va = 0\n",
    "\n",
    "    sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "        clf = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=train_feature.shape[1], iters=50, inv_link=\"identity\", threads=1)\n",
    "        clf.fit(train_feature[tr], score[tr])\n",
    "        score_va = clf.predict(train_feature[va])\n",
    "        score_te = clf.predict(test_feature)\n",
    "      \n",
    "        print('得分' + str(mean_squared_error(score[va], clf.predict(train_feature[va]))))\n",
    "        stack_train[va,0] = score_va\n",
    "        stack_test[:,0] += score_te\n",
    "    stack_test /= n_folds\n",
    "    stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "    df_stack3['tfidf_FTRL_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "    \n",
    "    ########################### ridge(RidgeClassfiy) ################################\n",
    "    print('RidgeClassfiy stacking')\n",
    "    stack_train = np.zeros((len(train_data),1))\n",
    "    stack_test = np.zeros((len(test_data),1))\n",
    "    score_va = 0\n",
    "\n",
    "    sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "        ridge = Ridge(solver=\"sag\", fit_intercept=True, random_state=42, alpha=30) \n",
    "        ridge.fit(train_feature[tr], score[tr])\n",
    "        score_va = ridge.predict(train_feature[va])\n",
    "        score_te = ridge.predict(test_feature)\n",
    "       \n",
    "        print('得分' + str(mean_squared_error(score[va], ridge.predict(train_feature[va]))))\n",
    "        stack_train[va,0] = score_va\n",
    "        stack_test[:,0] += score_te\n",
    "    stack_test /= n_folds\n",
    "    stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "    df_stack3['tfidf_ridge_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "    \n",
    "    ############################ Linersvc(LinerSVC) ################################\n",
    "    # print('LinerSVC stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     lsvc = LinearSVR(random_state=1017)\n",
    "    #     lsvc.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = lsvc.predict(train_feature[va])\n",
    "    #     score_te = lsvc.predict(test_feature)\n",
    "       \n",
    "    #     print('得分' + str(mean_squared_error(score[va], lsvc.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0] += score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "#     df_stack3['tfidf_lsvc_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "del stack,stack_train, stack_test,train_feature,test_feature\n",
    "gc.collect()   \n",
    "# df_stack.to_csv('graph_tfidf_classfiy.csv', index=None, encoding='utf8')\n",
    "print('tfidf特征已保存\\n')\n",
    "del train1,test1\n",
    "gc.collect()\n",
    "\n",
    "cols = [x for x in train2.columns if x not in ['label_description',\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "train2[cols]=train2[cols].fillna(0)\n",
    "test2[cols]=test2[cols].fillna(0)\n",
    "############################ 切分数据集 ##########################\n",
    "print('开始进行一些前期处理')\n",
    "train_feature = train2[cols].values\n",
    "test_feature = test2[cols].values\n",
    "    # 五则交叉验证\n",
    "n_folds = 5\n",
    "print('处理完毕')\n",
    "df_stack4 = pd.DataFrame()\n",
    "df_stack4['PetID']=data['PetID']\n",
    "for label in [\"AdoptionSpeed\"]:\n",
    "    score = train_data[label]\n",
    "    \n",
    "   \n",
    "    ########################### SGD(随机梯度下降) ################################\n",
    "    # print('sgd stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     sgd = SGDRegressor(random_state=1017,)\n",
    "    #     sgd.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = sgd.predict(train_feature[va])\n",
    "    #     score_te = sgd.predict(test_feature)\n",
    "    #     print('得分' + str(mean_squared_error(score[va], sgd.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0]+= score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "#     df_stack4['tfidf_sgd_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "\n",
    "\n",
    "    ########################### pac(PassiveAggressiveClassifier) ################################\n",
    "    # print('PAC stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     pac = PassiveAggressiveRegressor(random_state=1017)\n",
    "    #     pac.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = pac.predict(train_feature[va])\n",
    "    #     score_te = pac.predict(test_feature)\n",
    "      \n",
    "    #     print('得分' + str(mean_squared_error(score[va], pac.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0] += score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "#     df_stack4['tfidf_pac_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ########################### FTRL ################################\n",
    "    print('MultinomialNB stacking')\n",
    "    stack_train = np.zeros((len(train_data),1))\n",
    "    stack_test = np.zeros((len(test_data),1))\n",
    "    score_va = 0\n",
    "\n",
    "    sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "        clf = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=train_feature.shape[1], iters=50, inv_link=\"identity\", threads=1)\n",
    "        clf.fit(train_feature[tr], score[tr])\n",
    "        score_va = clf.predict(train_feature[va])\n",
    "        score_te = clf.predict(test_feature)\n",
    "      \n",
    "        print('得分' + str(mean_squared_error(score[va], clf.predict(train_feature[va]))))\n",
    "        stack_train[va,0] = score_va\n",
    "        stack_test[:,0] += score_te\n",
    "    stack_test /= n_folds\n",
    "    stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "    df_stack4['tfidf_FTRL_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "    \n",
    "    ########################### ridge(RidgeClassfiy) ################################\n",
    "    print('RidgeClassfiy stacking')\n",
    "    stack_train = np.zeros((len(train_data),1))\n",
    "    stack_test = np.zeros((len(test_data),1))\n",
    "    score_va = 0\n",
    "\n",
    "    sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "        ridge = Ridge(solver=\"sag\", fit_intercept=True, random_state=42, alpha=30) \n",
    "        ridge.fit(train_feature[tr], score[tr])\n",
    "        score_va = ridge.predict(train_feature[va])\n",
    "        score_te = ridge.predict(test_feature)\n",
    "       \n",
    "        print('得分' + str(mean_squared_error(score[va], ridge.predict(train_feature[va]))))\n",
    "        stack_train[va,0] = score_va\n",
    "        stack_test[:,0] += score_te\n",
    "    stack_test /= n_folds\n",
    "    stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "    df_stack4['tfidf_ridge_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "    \n",
    "    ############################ Linersvc(LinerSVC) ################################\n",
    "    # print('LinerSVC stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     lsvc = LinearSVR(random_state=1017)\n",
    "    #     lsvc.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = lsvc.predict(train_feature[va])\n",
    "    #     score_te = lsvc.predict(test_feature)\n",
    "       \n",
    "    #     print('得分' + str(mean_squared_error(score[va], lsvc.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0] += score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "#     df_stack4['tfidf_lsvc_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "del stack,stack_train, stack_test,train_feature,test_feature\n",
    "gc.collect()   \n",
    "# df_stack.to_csv('graph_tfidf_classfiy.csv', index=None, encoding='utf8')\n",
    "print('tfidf特征已保存\\n')\n",
    "\n",
    "# wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n",
    "#                                                               \"hash_ngrams_weights\": [1.5, 1.0],\n",
    "#                                                               \"hash_size\": 2 ** 29,\n",
    "#                                                               \"norm\": None,\n",
    "#                                                               \"tf\": 'binary',\n",
    "#                                                               \"idf\": None,\n",
    "#                                                               }), procs=8)\n",
    "# x_train = wb.fit_transform(train2['Description'])\n",
    "# x_test = wb.transform(test2['Description'])\n",
    "\n",
    "################\n",
    "# Remove features with document frequency <=100\n",
    "#@eg:1)\n",
    "#mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 100, 0, 1), dtype=bool)\n",
    "#sparse_merge = sparse_merge[:, mask]\n",
    "#@eg:2)\n",
    "#mask\n",
    "#mask = np.where(X_param1_train.getnnz(axis=0) > 3)[0]\n",
    "#X_param1_train = X_param1_train[:, mask]\n",
    "################\n",
    "# mask = np.array(np.clip(x_train.getnnz(axis=0) - 3, 0, 1), dtype=bool)\n",
    "# x_train=x_train[:,mask]\n",
    "# x_test=x_test[:,mask]\n",
    "\n",
    "# sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# print(\"FTRL...\")\n",
    "# n_fold=5\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf=  FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=x_train.shape[1], iters=50, inv_link=\"identity\", threads=1)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['FTRL_pred']=stack_train\n",
    "# test_data['FTRL_pred']=stack_test\n",
    "# print(\"Ridge...\")\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf= Ridge(solver=\"sag\", fit_intercept=True, random_state=42, alpha=30)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['ridge_pred']=stack_train\n",
    "# test_data['ridge_pred']=stack_test\n",
    "\n",
    "# print(\"LinearSVR...\")\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf= LinearSVR(random_state=1017)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['svr_pred']=stack_train\n",
    "# test_data['svr_pred']=stack_test\n",
    "\n",
    "# print(\"pac...\")\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf= PassiveAggressiveRegressor(random_state=1017)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['pac_pred']=stack_train\n",
    "# test_data['pac_pred']=stack_test\n",
    "\n",
    "# print(\"sgd...\")\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf= SGDRegressor(random_state=1017,)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['sgd_pred']=stack_train\n",
    "# test_data['sgd_pred']=stack_test\n",
    "\n",
    "# del x_train,x_test,stack_train,stack_test\n",
    "del train2,test2\n",
    "gc.collect()\n",
    "\n",
    "##3sigma \n",
    "def feat4_model():\n",
    "\n",
    "    def read_sent_json(pet_id,data_source='train'):\n",
    "    \n",
    "        doc_sent_mag = []\n",
    "        doc_sent_score = []\n",
    "        nf_count = 0\n",
    "        for pet in pet_id:\n",
    "            try:\n",
    "                with open(data_source + '_sentiment/' + pet + '.json', 'r') as f:\n",
    "                    sentiment = json.load(f)\n",
    "                doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "                doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "            except FileNotFoundError:\n",
    "                nf_count += 1\n",
    "                doc_sent_mag.append(-np.nan)\n",
    "                doc_sent_score.append(-np.nan)\n",
    "        return doc_sent_mag,doc_sent_score\n",
    "    \n",
    "    def read_meta_json(data_source='train'):\n",
    "    \n",
    "        vertex_xs = []\n",
    "        vertex_ys = []\n",
    "        bounding_confidences = []\n",
    "        bounding_importance_fracs = []\n",
    "        dominant_blues = []\n",
    "        dominant_greens = []\n",
    "        dominant_reds = []\n",
    "        dominant_pixel_fracs = []\n",
    "        dominant_scores = []\n",
    "        label_descriptions = []\n",
    "        label_all_descriptions = []\n",
    "        label_scores = []\n",
    "        file_name = []\n",
    "        nl_count = 0\n",
    "        file_list = os.listdir(data_source+'_metadata/')\n",
    "        for file_i in file_list:\n",
    "            file_name.append(file_i) \n",
    "            with open(data_source + '_metadata/' + file_i, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_all_description = ' '.join([i['description'].replace(' ','_') for i in data['labelAnnotations']])\n",
    "                label_descriptions.append(label_description)\n",
    "                label_all_descriptions.append(label_all_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_all_descriptions.append('nothing')\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        out_df = pd.DataFrame({'file_name':file_name,\n",
    "                               'vertex_xs':vertex_xs,\n",
    "                               'vertex_ys':vertex_ys,\n",
    "                               'bounding_confidences':bounding_confidences,\n",
    "                               'bounding_importance_fracs':bounding_importance_fracs,\n",
    "                               'dominant_blues':dominant_blues,\n",
    "                               'dominant_greens':dominant_greens,\n",
    "                               'dominant_reds':dominant_reds,\n",
    "                               'dominant_pixel_fracs':dominant_pixel_fracs,\n",
    "                               'dominant_scores':dominant_scores,\n",
    "                               'label_descriptions':label_descriptions,\n",
    "                               'label_all_descriptions':label_all_descriptions,\n",
    "                               'label_scores':label_scores\n",
    "                              })\n",
    "        out_df['PetID'] = out_df['file_name'].str.split('-').str[0]\n",
    "        out_df['PicID'] =  out_df['file_name'].str.split('[-|.]').str[1]\n",
    "        \n",
    "        return out_df\n",
    "    \n",
    "    breed = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')\n",
    "    state = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')\n",
    "    color = pd.read_csv('../input/petfinder-adoption-prediction/color_labels.csv')\n",
    "    \n",
    "    data_tr = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "    data_te = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n",
    "    \n",
    "    def deal_breed(df):\n",
    "        if df['Breed1']==df['Breed2']:\n",
    "            df['Breed2']=0\n",
    "        if df['Breed1']!=307 & df['Breed2']==307:\n",
    "            temp=df[\"Breed1\"]\n",
    "            df['Breed1']=df['Breed2']\n",
    "            df['Breed2']=temp\n",
    "        return df\n",
    "    data_tr=data_tr.apply(lambda x:deal_breed(x),1)\n",
    "    data_te=data_te.apply(lambda x:deal_breed(x),1)\n",
    "    \n",
    "    breed_dict = dict(zip(breed['BreedID'].values.astype(\"str\"),breed['BreedName'].values))\n",
    "    def get_breed(df):\n",
    "        \n",
    "        x=\"\"\n",
    "        for i in [\"Breed1\",\"Breed2\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=breed_dict[str(df[i])]+\" \"\n",
    "        return x\n",
    "    data_tr['breed']=data_tr.apply(lambda x:get_breed(x),1)\n",
    "    data_te['breed']=data_te.apply(lambda x:get_breed(x),1)\n",
    "    \n",
    "    data_tr['Breed1_str']=data_tr['Breed1'].astype(\"str\")\n",
    "    data_tr['Breed1_str']=data_tr['Breed1'].map(breed_dict)\n",
    "    data_tr['Breed1_str']=data_tr['Breed1'].replace(np.nan,\"null\")\n",
    "    \n",
    "    data_te['Breed1_str']=data_te['Breed1'].astype(\"str\")\n",
    "    data_te['Breed1_str']=data_te['Breed1'].map(breed_dict)\n",
    "    data_te['Breed1_str']=data_te['Breed1'].replace(np.nan,\"null\")\n",
    "    \n",
    "    mean_encoder = MeanEncoder( categorical_features=['Breed1_str', 'breed'],target_type ='regression')\n",
    "    \n",
    "    data_tr = mean_encoder.fit_transform(data_tr, data_tr['AdoptionSpeed'])\n",
    "    data_te = mean_encoder.transform(data_te)\n",
    "    \n",
    "    breed_encode_tr = data_tr[['Breed1_str_pred','breed_pred']]\n",
    "    breed_encode_te = data_te[['Breed1_str_pred','breed_pred']]\n",
    "    \n",
    "    data_tr['doc_sent_mag'],data_tr['doc_sent_score'] = read_sent_json(data_tr['PetID'],data_source='../input/petfinder-adoption-prediction/train')\n",
    "    data_te['doc_sent_mag'],data_te['doc_sent_score'] = read_sent_json(data_te['PetID'],data_source='../input/petfinder-adoption-prediction/test')\n",
    "    \n",
    "    meta_tr = read_meta_json(data_source='../input/petfinder-adoption-prediction/train')\n",
    "    meta_te = read_meta_json(data_source='../input/petfinder-adoption-prediction/test')\n",
    "    \n",
    "    num_cols_0 = ['Age','MaturitySize','FurLength','Health','Quantity','Fee','VideoAmt','PhotoAmt']\n",
    "    sent_cols_0 = ['doc_sent_mag','doc_sent_score']\n",
    "    meta_cols_0 = ['vertex_xs', 'vertex_ys', 'bounding_confidences',\n",
    "           'bounding_importance_fracs', 'dominant_blues', 'dominant_greens',\n",
    "           'dominant_reds', 'dominant_pixel_fracs', 'dominant_scores','label_scores'] #'label_descriptions'\n",
    "    cat_cols_0 = ['Type','Gender','Vaccinated','Dewormed','Sterilized']\n",
    "    join_cols_0 = ['Breed1','Breed2','Color1','Color2','Color3','State']\n",
    "    \n",
    "    n_tr = data_tr.shape[0]\n",
    "    n_te = data_te.shape[0]\n",
    "    \n",
    "    def data_process(data,meta_data,data_source='train'):\n",
    "        fea_data = data[['PetID']+num_cols_0+sent_cols_0+cat_cols_0].copy()\n",
    "        \n",
    "        meta_1 = meta_data.query('PicID ==\"1\"')\n",
    "        temp = data[['PetID']].merge(meta_1,how='left',on='PetID')[meta_cols_0].fillna(-1)\n",
    "        fea_data = pd.concat([fea_data,temp],axis=1)[['PetID']+num_cols_0+sent_cols_0+meta_cols_0+cat_cols_0]\n",
    "        \n",
    "    #     temp = meta_1.pivot(index='PetID',columns='label_descriptions',values='label_scores')\n",
    "    #     temp = meta_data.loc[meta_data['PicID'].isin(['1']),:].groupby(['PetID','label_descriptions'])['label_scores'].max().reset_index()\n",
    "    #     temp = temp.pivot(index='PetID',columns='label_descriptions',values='label_scores')\n",
    "    #     temp.columns = [i.replace(' ','_') + '_scores' for i in temp.columns]\n",
    "    #     fea_data = fea_data.merge(temp,how='left',on='PetID')\n",
    "        \n",
    "        \n",
    "        for i in ['Age','MaturitySize','FurLength','Health',\n",
    "                  'Fee','VideoAmt','PhotoAmt',\n",
    "                  'Breed2','Color2','Color3']:\n",
    "            fea_data[i+'_is_0'] = data[i] == 0\n",
    "        for i in ['Fee','VideoAmt','PhotoAmt']:\n",
    "            fea_data[i+'_avg'] = data[i] / data['Quantity']\n",
    "        \n",
    "        dummy_fea = pd.get_dummies(data[cat_cols_0],columns=cat_cols_0)\n",
    "        fea_data = pd.concat([fea_data,dummy_fea],axis=1)\n",
    "        fea_data['color_cnt'] = (data[['Color1','Color2','Color3']] != 0 ).sum(axis=1)\n",
    "        fea_data['no_name'] = data['Name'].isnull() | data['Name'].str.contains('No Name')\n",
    "        \n",
    "        # Description\n",
    "        temp = data['Description']\n",
    "        fea_data['desc_ch'] = temp.str.count(u'[\\u4e00-\\u9fa5]')\n",
    "        fea_data['desc_count'] = temp.str.count(u'[\\u4e00-\\u9fa5]') + temp.replace(u'[\\u4e00-\\u9fa5]','').str.count(' ')\n",
    "        fea_data['desc_ch_rate'] = fea_data['desc_ch'] / fea_data['desc_count']\n",
    "        \n",
    "        # RescuerID\n",
    "        fea_data['rescuer_cnt'] = data.groupby('RescuerID')['PetID'].transform('count')\n",
    "        \n",
    "        temp = data.groupby(['RescuerID','Type'])['PetID'].count().rename('rescuer_type_cnt').reset_index()\n",
    "        temp = temp.pivot(index='RescuerID',columns='Type',values='rescuer_type_cnt').fillna(0)\n",
    "        temp.columns = ['rescuer_type1_cnt','rescuer_type2_cnt']\n",
    "        temp = data[['RescuerID']].merge(temp,how='left',on='RescuerID')\n",
    "        fea_data[['rescuer_type1_cnt','rescuer_type2_cnt']] = temp[['rescuer_type1_cnt','rescuer_type2_cnt']] \n",
    "        fea_data['rescuer_type1_rate']= fea_data['rescuer_type1_cnt'] / fea_data['rescuer_cnt']\n",
    "        \n",
    "    # #     temp = data.groupby(['RescuerID','State'])['PetID'].count() / data_tr.groupby(['State'])['PetID'].count()\n",
    "    # #     temp = temp.rename('Rescuer_State_rate').reset_index().drop('State',axis=1)\n",
    "    # #     fea_data['Rescuer_State_rate']= data_tr[['RescuerID']].merge(temp,how='left',on='RescuerID')['Rescuer_State_rate']\n",
    "        \n",
    "        # Breed\n",
    "        temp = data_tr.groupby(['Breed1'])['PetID'].count().rename('Breed1_cnt').reset_index()\n",
    "        temp = data[['Breed1']].merge(temp,how='left',on='Breed1')   \n",
    "        fea_data['Breed1_cnt'] = temp['Breed1_cnt']\n",
    "        \n",
    "        \n",
    "        data_tr['Fee_avg'] = data_tr['Fee'] / data_tr['Quantity']\n",
    "        temp = data_tr.groupby(['Breed1'])['Fee_avg'].mean().rename('Breed1_Fee_avg').reset_index()\n",
    "        temp = data[['Breed1']].merge(temp,how='left',on='Breed1') \n",
    "        fea_data['Breed1_Fee_avg'] = temp['Breed1_Fee_avg']\n",
    "        fea_data['Breed1_Fee_avg_diff'] = fea_data['Fee_avg'] - fea_data['Breed1_Fee_avg']\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # State\n",
    "        \n",
    "        temp = data_tr.groupby(['State','Type'])['PetID'].count().rename('state_type_cnt').reset_index()\n",
    "        temp = temp.pivot(index='State',columns='Type',values='state_type_cnt').fillna(0)\n",
    "        # temp.columns = ['state_type1_cnt','state_type2_cnt']\n",
    "        temp.columns = ['state_type1_cnt_rank','state_type2_cnt_rank']\n",
    "        temp['state_type1_cnt_rank'] = temp['state_type1_cnt_rank'].rank()\n",
    "        temp['state_type2_cnt_rank'] = temp['state_type2_cnt_rank'].rank()\n",
    "        temp = data[['State']].merge(temp,how='left',on='State')\n",
    "        fea_data[['state_type1_cnt_rank','state_type2_cnt_rank']] = temp[['state_type1_cnt_rank','state_type2_cnt_rank']] \n",
    "        \n",
    "        \n",
    "        \n",
    "        if data_source == 'train':\n",
    "            label = data['AdoptionSpeed'].values\n",
    "        else:\n",
    "            label = None\n",
    "        return fea_data,label\n",
    "    \n",
    "    fea_tr,label_tr = data_process(data_tr,meta_tr,data_source='train')\n",
    "    fea_te,_        = data_process(data_te,meta_te,data_source='test')\n",
    "    \n",
    "    base_fea_cols = [i for i in fea_te.columns if i in fea_tr.columns]\n",
    "    fea_tr = fea_tr[base_fea_cols]\n",
    "    fea_te = fea_te[base_fea_cols]\n",
    "    \n",
    "    print('base_fea_size_tr:',fea_tr.shape)\n",
    "    print('base_fea_size_te:',fea_te.shape)\n",
    "    \n",
    "    def data_process_lr(data,meta_data,data_source='train'):\n",
    "        fea_data = data[['PetID']+num_cols_0+sent_cols_0].copy()\n",
    "        \n",
    "        meta_1 = meta_data.query('PicID ==\"1\"')\n",
    "        temp = data[['PetID']].merge(meta_1,how='left',on='PetID')[meta_cols_0].fillna(0)\n",
    "        fea_data = pd.concat([fea_data,temp],axis=1)[['PetID']+num_cols_0+sent_cols_0+meta_cols_0]\n",
    "        \n",
    "        for i in ['Fee','VideoAmt','PhotoAmt']:\n",
    "            fea_data[i+'_avg'] = data[i] / data['Quantity']\n",
    "        \n",
    "    \n",
    "        fea_data['color_cnt'] = (data[['Color1','Color2','Color3']] != 0 ).sum(axis=1)\n",
    "        \n",
    "        # Description\n",
    "        temp = data['Description']\n",
    "        fea_data['desc_count'] = temp.str.count(u'[\\u4e00-\\u9fa5]') + temp.replace(u'[\\u4e00-\\u9fa5]','').str.count(' ')\n",
    "        \n",
    "        # RescuerID\n",
    "        fea_data['rescuer_cnt'] = data.groupby('RescuerID')['PetID'].transform('count')\n",
    "        \n",
    "        temp = data.groupby(['RescuerID','Type'])['PetID'].count().rename('rescuer_type_cnt').reset_index()\n",
    "        temp = temp.pivot(index='RescuerID',columns='Type',values='rescuer_type_cnt').fillna(0)\n",
    "        temp.columns = ['rescuer_type1_cnt','rescuer_type2_cnt']\n",
    "        temp = data[['RescuerID']].merge(temp,how='left',on='RescuerID')\n",
    "        fea_data[['rescuer_type1_cnt','rescuer_type2_cnt']] = temp[['rescuer_type1_cnt','rescuer_type2_cnt']] \n",
    "        fea_data['rescuer_type1_rate']= fea_data['rescuer_type1_cnt'] / fea_data['rescuer_cnt']\n",
    "        \n",
    "      \n",
    "        # Breed\n",
    "        temp = data_tr.groupby(['Breed1'])['PetID'].count().rename('Breed1_cnt').reset_index()\n",
    "        temp = data[['Breed1']].merge(temp,how='left',on='Breed1')   \n",
    "        fea_data['Breed1_cnt'] = temp['Breed1_cnt']\n",
    "        \n",
    "        \n",
    "        data_tr['Fee_avg'] = data_tr['Fee'] / data_tr['Quantity']\n",
    "        temp = data_tr.groupby(['Breed1'])['Fee_avg'].mean().rename('Breed1_Fee_avg').reset_index()\n",
    "        temp = data[['Breed1']].merge(temp,how='left',on='Breed1') \n",
    "        fea_data['Breed1_Fee_avg'] = temp['Breed1_Fee_avg']\n",
    "        fea_data['Breed1_Fee_avg_diff'] = fea_data['Fee_avg'] - fea_data['Breed1_Fee_avg']\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # State\n",
    "        \n",
    "        temp = data_tr.groupby(['State','Type'])['PetID'].count().rename('state_type_cnt').reset_index()\n",
    "        temp = temp.pivot(index='State',columns='Type',values='state_type_cnt').fillna(0)\n",
    "        # temp.columns = ['state_type1_cnt','state_type2_cnt']\n",
    "        temp.columns = ['state_type1_cnt_rank','state_type2_cnt_rank']\n",
    "        temp['state_type1_cnt_rank'] = temp['state_type1_cnt_rank'].rank()\n",
    "        temp['state_type2_cnt_rank'] = temp['state_type2_cnt_rank'].rank()\n",
    "        temp = data[['State']].merge(temp,how='left',on='State')\n",
    "        fea_data[['state_type1_cnt_rank','state_type2_cnt_rank']] = temp[['state_type1_cnt_rank','state_type2_cnt_rank']] \n",
    "        \n",
    "        dummy_fea = pd.get_dummies(data[cat_cols_0],columns=cat_cols_0)\n",
    "        fea_data = pd.concat([fea_data,dummy_fea],axis=1)   \n",
    "        fea_data['no_name'] = (data['Name'].isnull() | data['Name'].str.contains('No Name')).fillna(1).astype(int)\n",
    "        \n",
    "        \n",
    "        if data_source == 'train':\n",
    "            label = data['AdoptionSpeed'].values\n",
    "        else:\n",
    "            label = None\n",
    "        return fea_data,label\n",
    "    \n",
    "    lr_fea_tr,label_tr = data_process_lr(data_tr,meta_tr,data_source='train')\n",
    "    lr_fea_te,_        = data_process_lr(data_te,meta_te,data_source='test')\n",
    "    \n",
    "    lr_base_fea_cols = [i for i in lr_fea_te.columns if i in lr_fea_tr.columns]\n",
    "    lr_fea_tr = lr_fea_tr[lr_base_fea_cols]\n",
    "    lr_fea_te = lr_fea_te[lr_base_fea_cols]\n",
    "    replace_cols_0 = [\n",
    "     'bounding_importance_fracs',\n",
    "     'dominant_blues',\n",
    "     'dominant_greens',\n",
    "     'dominant_reds',\n",
    "    'label_scores',]\n",
    "    \n",
    "    \n",
    "    lr_fea_tr[replace_cols_0] = lr_fea_tr[replace_cols_0].replace(-1,0)\n",
    "    lr_fea_te[replace_cols_0] = lr_fea_te[replace_cols_0].replace(-1,0)\n",
    "    # fillna log\n",
    "    for i in lr_base_fea_cols[1:35]:\n",
    "        if i in ['doc_sent_mag','doc_sent_score','Breed1_Fee_avg_diff']:\n",
    "            lr_fea_tr[i] = lr_fea_tr[i].fillna(0)\n",
    "            lr_fea_te[i] = lr_fea_te[i].fillna(0)\n",
    "        else:\n",
    "            lr_fea_tr[i] = np.log(lr_fea_tr[i].fillna(0)+1e-5)\n",
    "            lr_fea_te[i] = np.log(lr_fea_te[i].fillna(0)+1e-5)\n",
    "    # standarscaler\n",
    "    scaler = StandardScaler()\n",
    "    # scaler = MinMaxScaler()\n",
    "    lr_fea_tr[lr_base_fea_cols[1:35]] = scaler.fit_transform(lr_fea_tr[lr_base_fea_cols[1:35]])\n",
    "    lr_fea_te[lr_base_fea_cols[1:35]] = scaler.transform(lr_fea_te[lr_base_fea_cols[1:35]])\n",
    "    \n",
    "    \n",
    "    ##############img_feat###############################\n",
    "    train_df = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "    pet_ids = train_df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    \n",
    "    def img_model():\n",
    "        K.clear_session()\n",
    "        inp = Input((img_size, img_size, 3))\n",
    "        x = DenseNet121(\n",
    "                include_top=False, \n",
    "                weights=\"../input/keras-pretrain-model-weights/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\", \n",
    "                input_shape=(img_size, img_size, 3))(inp)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Lambda(lambda x: K.expand_dims(x, axis = -1))(x)\n",
    "        x = AveragePooling1D()(x)\n",
    "        out = Lambda(lambda x: x[:, :, 0])(x)\n",
    "    \n",
    "        model = Model(inp, out)\n",
    "        return model\n",
    "    \n",
    "    extract_model = img_model()\n",
    "    img_features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = extract_model.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            img_features[pet_id] = batch_preds[i]\n",
    "    img_tr = pd.DataFrame.from_dict(img_features, orient='index')\n",
    "    del img_features\n",
    "    gc.collect()\n",
    "    \n",
    "    pca1 = PCA(n_components=30,random_state=42)\n",
    "    image_pca_all_tr = pca1.fit_transform(img_tr)\n",
    "    pca2 = PCA(n_components=80,random_state=42)\n",
    "    \n",
    "    image_pca_lr_tr = pca2.fit_transform(img_tr)\n",
    "    del img_tr\n",
    "    gc.collect()\n",
    "    \n",
    "    test_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n",
    "    pet_ids = test_df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    img_features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = extract_model.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            img_features[pet_id] = batch_preds[i]\n",
    "            \n",
    "    img_te = pd.DataFrame.from_dict(img_features, orient='index')\n",
    "    del img_features,train_df,test_df,pet_ids,extract_model\n",
    "    gc.collect()\n",
    "    image_pca_all_te = pca1.transform(img_te)\n",
    "    image_pca_lr_te = pca2.transform(img_te)\n",
    "    del img_te,pca1,pca2\n",
    "    gc.collect()\n",
    "    \n",
    "    #####################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    desc_vec = TfidfVectorizer(ngram_range=(1,4),\n",
    "                               min_df=3, max_df=0.9, \n",
    "                               strip_accents='unicode', \n",
    "                               use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "    \n",
    "    \n",
    "    desc_tfidf_all_tr = desc_vec.fit_transform(data_tr['Description'].fillna('null').tolist())\n",
    "    desc_tfidf_all_te = desc_vec.transform(data_te['Description'].fillna('null').tolist())\n",
    "    \n",
    "    # tfidf select by chi2 top 10k\n",
    "    tfidf_select = chi2(desc_tfidf_all_tr,label_tr)\n",
    "    tfidf_select_index = (-tfidf_select[0]).argsort()[:10000]\n",
    "    \n",
    "    desc_tfidf_tr = desc_tfidf_all_tr[:,tfidf_select_index]\n",
    "    desc_tfidf_te = desc_tfidf_all_te[:,tfidf_select_index]\n",
    "    \n",
    "    # SVD\n",
    "    svd = TruncatedSVD(n_components=200,random_state=42)\n",
    "    tfidf_svd_tr = svd.fit_transform(desc_tfidf_all_tr)\n",
    "    tfidf_svd_te = svd.transform(desc_tfidf_all_te)\n",
    "    \n",
    "    tfidf_select = chi2(desc_tfidf_all_tr,label_tr)\n",
    "    tfidf_select_index = (-tfidf_select[0]).argsort()[:50000]\n",
    "    \n",
    "    lr_desc_tfidf_tr = desc_tfidf_all_tr[:,tfidf_select_index]\n",
    "    lr_desc_tfidf_te = desc_tfidf_all_te[:,tfidf_select_index]\n",
    "    \n",
    "    join_data_all = pd.concat([data_tr[join_cols_0],data_te[join_cols_0]],axis=0)\n",
    "    # join_data_all = pd.concat([data_tr[cat_cols_0+join_cols_0],data_te[cat_cols_0+join_cols_0]],axis=0)\n",
    "    # for i in cat_cols_0:\n",
    "    #     join_data_all[i] = join_data_all[i].astype(str)+'_'+i\n",
    "    for i in join_cols_0:\n",
    "        if i != 'State':\n",
    "            join_data_all[i] = join_data_all[i].astype(str)+'_'+i[:-1]\n",
    "        else:\n",
    "            join_data_all[i] = join_data_all[i].astype(str)+'_'+i\n",
    "    \n",
    "    join_data_all = join_data_all.apply(lambda x: ' '.join(x), axis=1).tolist()\n",
    "    \n",
    "    # tfidf\n",
    "    join_vec = TfidfVectorizer(ngram_range=(1,1),\n",
    "                               min_df=3, max_df=0.9, \n",
    "                               strip_accents='unicode', \n",
    "                               use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "    # join_all = join_vec.fit_transform(join_data_all)\n",
    "    join_tr = join_vec.fit_transform(join_data_all[:n_tr])\n",
    "    join_te = join_vec.transform(join_data_all[n_tr:])\n",
    "    \n",
    "    # SVD\n",
    "    svd = TruncatedSVD(n_components=50,random_state=42)\n",
    "    # svd_join = svd.fit_transform(join_all)\n",
    "    \n",
    "    join_svd_tr = svd.fit_transform(join_tr)\n",
    "    join_svd_te = svd.transform(join_te)\n",
    "    \n",
    "    meta_desc_all = meta_tr[['PetID','label_all_descriptions']].append(meta_te[['PetID','label_all_descriptions']])\n",
    "    \n",
    "    meta_desc_all = meta_desc_all.groupby('PetID')['label_all_descriptions']\\\n",
    "                    .apply(lambda x: ' '.join(x)).to_frame().reset_index()\n",
    "    \n",
    "    meta_desc_all = data_tr[['PetID']].append(data_te[['PetID']]).merge(meta_desc_all,how='left',on='PetID')['label_all_descriptions']\n",
    "    \n",
    "    meta_desc_vec = TfidfVectorizer(ngram_range=(1,1),\n",
    "                               min_df=10, max_df=0.9, \n",
    "                               strip_accents='unicode', \n",
    "                               use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "    # meta_desc_tfidf = meta_desc_vec.fit_transform(meta_desc_all.fillna('nan'))\n",
    "    # meta_desc_tfidf_tr = meta_desc_tfidf[:n_tr,:]\n",
    "    # meta_desc_tfidf_te = meta_desc_tfidf[n_tr:,:]\n",
    "    meta_desc_tfidf_tr = meta_desc_vec.fit_transform(meta_desc_all[:n_tr].fillna('nan'))\n",
    "    meta_desc_tfidf_te = meta_desc_vec.transform(meta_desc_all[n_tr:].fillna('nan'))\n",
    "     \n",
    "    del meta_desc_all,join_data_all\n",
    "    gc.collect()\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    gpf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    lr_tr_fea_all = hstack([lr_fea_tr.values[:,1:].astype(float),lr_desc_tfidf_tr,join_tr,meta_desc_tfidf_tr,image_pca_lr_tr]).tocsr()\n",
    "    lr_te_fea_all = hstack([lr_fea_te.values[:,1:].astype(float),lr_desc_tfidf_te,join_te,meta_desc_tfidf_te,image_pca_lr_te]).tocsr()\n",
    "    del lr_desc_tfidf_tr,lr_desc_tfidf_te,lr_fea_tr,lr_fea_te,image_pca_lr_tr,image_pca_lr_te\n",
    "    gc.collect()\n",
    "    lr_train_predictions = np.zeros((n_tr, 1))\n",
    "    lr_test_predictions = np.zeros((n_te, 1))\n",
    "    lr_train = np.zeros((n_tr, ))\n",
    "    lr_test = np.zeros((n_te, ))\n",
    "    qwk = []\n",
    "    rmse_list = []\n",
    "    for k,(tr_idx,val_idx) in enumerate(gpf.split(data_tr,label_tr,data_tr['RescuerID'])):\n",
    "    # for k,(tr_idx,val_idx) in enumerate(skf.split(data_tr,label_tr)):\n",
    "        if k >= 0:\n",
    "            print('fold_' + str(k) + ' ...')\n",
    "            \n",
    "            fold_tr_fea,fold_tr_label = lr_tr_fea_all[tr_idx,:],label_tr[tr_idx]\n",
    "            fold_val_fea,fold_val_label = lr_tr_fea_all[val_idx,:],label_tr[val_idx]\n",
    "            \n",
    "            for i in [2]:\n",
    "                ridge_model = Ridge(alpha=i,random_state=42)\n",
    "                ridge_model.fit(fold_tr_fea,fold_tr_label)\n",
    "    \n",
    "                pred_val  = ridge_model.predict(fold_val_fea)\n",
    "    \n",
    "                optR = OptimizedRounder()\n",
    "                optR.fit(pred_val, fold_val_label)\n",
    "                valid_p = optR.predict(pred_val, optR.coefficients(),384).astype(int)  \n",
    "                qwk_i = cohen_kappa_score(fold_val_label, valid_p,weights='quadratic')\n",
    "                rmse_i = rmse(fold_val_label,pred_val)\n",
    "                qwk.append(qwk_i)\n",
    "                rmse_list.append(rmse_i)\n",
    "                print('alpha:',i)\n",
    "                print('qwk:',qwk_i)\n",
    "                print(optR.coefficients())\n",
    "                print('train_rmse:',rmse(fold_tr_label,ridge_model.predict(fold_tr_fea)))\n",
    "                print('val_rmse:',rmse_i)\n",
    "                print('----'*5)\n",
    "                \n",
    "                lr_train_predictions[val_idx] = pred_val.reshape(-1,1)\n",
    "                lr_train[val_idx] = np.squeeze(pred_val)\n",
    "                test_preds = ridge_model.predict(lr_te_fea_all)\n",
    "                lr_test_predictions +=test_preds.reshape(-1,1)\n",
    "                lr_test += np.squeeze(test_preds)/5\n",
    "    lr_test_predictions =lr_test_predictions/5\n",
    "    print(qwk,np.mean(qwk),np.std(qwk))\n",
    "    print(rmse_list,np.mean(rmse_list),np.std(rmse_list))\n",
    "    del lr_tr_fea_all,lr_te_fea_all\n",
    "    gc.collect()\n",
    "    params = {'application': 'regression',\n",
    "              'boosting': 'gbdt',\n",
    "              'metric': 'rmse',\n",
    "              'num_leaves': 70,\n",
    "              'max_depth': 8,\n",
    "              'learning_rate': 0.01,\n",
    "              'bagging_fraction': 0.85,\n",
    "              'feature_fraction': 0.8,\n",
    "              'min_split_gain': 0.02,\n",
    "              'min_child_samples': 150,\n",
    "              'min_child_weight': 0.2,\n",
    "              'lambda_l2': 0.05,\n",
    "              'verbosity': -1,\n",
    "              'seed':24}\n",
    "    \n",
    "    tr_fea_all = hstack([fea_tr.values[:,1:].astype(float),desc_tfidf_tr,tfidf_svd_tr,join_tr,join_svd_tr,meta_desc_tfidf_tr,image_pca_all_tr,lr_train_predictions,breed_encode_tr]).tocsr()\n",
    "    te_fea_all = hstack([fea_te.values[:,1:].astype(float),desc_tfidf_te,tfidf_svd_te,join_te,join_svd_te,meta_desc_tfidf_te,image_pca_all_te,lr_test_predictions,breed_encode_te]).tocsr()\n",
    "    del fea_tr,fea_te,desc_tfidf_tr,tfidf_svd_tr,join_tr,join_svd_tr,meta_desc_tfidf_tr,image_pca_all_tr,desc_tfidf_te,tfidf_svd_te,join_te,join_svd_te,meta_desc_tfidf_te,image_pca_all_te,breed_encode_tr,breed_encode_te\n",
    "    gc.collect()\n",
    "    qwk = []\n",
    "    rmse_list = []\n",
    "    # fea_imp = []\n",
    "    train_predictions = np.zeros((n_tr, ))\n",
    "    test_predictions = np.zeros((n_te, ))\n",
    "    \n",
    "    for k,(tr_idx,val_idx) in enumerate(skf.split(data_tr,label_tr)):\n",
    "    # for k,(tr_idx,val_idx) in enumerate(gpf.split(data_tr,label_tr,data_tr['RescuerID'])):\n",
    "        if k >= 0:\n",
    "            print('fold_' + str(k) + ' ...')\n",
    "            \n",
    "            \n",
    "            fold_tr_fea,fold_tr_label = tr_fea_all[tr_idx,:],label_tr[tr_idx]\n",
    "            fold_val_fea,fold_val_label = tr_fea_all[val_idx,:],label_tr[val_idx]\n",
    "            \n",
    "            d_fold_tr = lgb.Dataset(fold_tr_fea, label=fold_tr_label)\n",
    "            d_fold_val = lgb.Dataset(fold_val_fea, label=fold_val_label)\n",
    "            watchlist = [d_fold_tr, d_fold_val]\n",
    "            num_rounds = 10000\n",
    "            verbose_eval = 100\n",
    "            early_stop = 100\n",
    "            model = lgb.train(params,\n",
    "                              train_set=d_fold_tr,\n",
    "                              num_boost_round=num_rounds,\n",
    "                              valid_sets=watchlist,\n",
    "                              verbose_eval=verbose_eval,\n",
    "                              early_stopping_rounds=early_stop)    \n",
    "            pred_val = model.predict(fold_val_fea, num_iteration=model.best_iteration)\n",
    "            optR = OptimizedRounder()\n",
    "            optR.fit(pred_val, fold_val_label)\n",
    "            valid_p = optR.predict(pred_val, optR.coefficients(),384).astype(int)     \n",
    "            qwk_i = cohen_kappa_score(fold_val_label, valid_p,weights='quadratic')\n",
    "            rmse_i = model.best_score['valid_1']['rmse']\n",
    "            qwk.append(qwk_i)\n",
    "            rmse_list.append(rmse_i)\n",
    "            print('qwk:',qwk_i)\n",
    "            print('rmse:',rmse_i)\n",
    "            print('----'*5)\n",
    "            \n",
    "            train_predictions[val_idx] = np.squeeze(pred_val)\n",
    "            test_preds = model.predict(te_fea_all, num_iteration=model.best_iteration)\n",
    "            test_predictions += np.squeeze(test_preds)/5\n",
    "    lgb7_test = [r for r in test_predictions]\n",
    "    \n",
    "    lgb7_train = [r for r in train_predictions]\n",
    "    \n",
    "    \n",
    "    del tr_fea_all,te_fea_all \n",
    "    del data_tr,meta_tr,data_te,meta_te\n",
    "    gc.collect()\n",
    "    \n",
    "    return lr_train,lr_test,lgb7_train,lgb7_test\n",
    "lr_train,lr_test,lgb7_train,lgb7_test = feat4_model()\n",
    "t11=time.time()\n",
    "print(\"model11 cost:{} s\".format(t11-t10))\n",
    "\n",
    "import psutil\n",
    "info = psutil.virtual_memory()\n",
    "print(\"memery used rate:\",info.percent)\n",
    "\n",
    "vv=pd.DataFrame(index=range(len(train_data)))\n",
    "vv['PetID']=train_data['PetID']\n",
    "vv['lgb1']=lgb1_train\n",
    "vv['lgb2']=lgb2_train\n",
    "vv['lgb3']=lgb3_train\n",
    "vv['lr']=lr_train\n",
    "vv['lgb5']=lgb5_train\n",
    "vv['lgb6']=lgb6_train\n",
    "vv['lgb7']=lgb7_train\n",
    "vv['cat1']=cat1_train\n",
    "vv['cat2']=cat2_train\n",
    "vv['nn1']=nn1_train\n",
    "vv['nn2']=nn2_train\n",
    "vv['nn3']=nn3_train\n",
    "# vv['ridge_pred']=train_data['ridge_pred']\n",
    "# vv['FTRL_pred']=train_data['FTRL_pred']\n",
    "# vv['svr_pred']=train_data['svr_pred']\n",
    "# vv['pac_pred']=train_data['pac_pred']\n",
    "# vv['sgd_pred']=train_data['sgd_pred']\n",
    "# vv['breed1_pred']=breed_encode_tr['Breed1_str_pred']\n",
    "# vv['breed_pred']=breed_encode_tr['breed_pred']\n",
    "# vv = pd.merge(vv,df_stack1,on='PetID',how=\"left\")\n",
    "# vv = pd.merge(vv,df_stack2,on='PetID',how=\"left\")\n",
    "vv = pd.merge(vv,df_stack3,on='PetID',how=\"left\")\n",
    "vv = pd.merge(vv,df_stack4,on='PetID',how=\"left\")\n",
    "vv['AdoptionSpeed']=train_data['AdoptionSpeed']\n",
    "\n",
    "col = [x for x in vv.columns if x not in ['PetID','AdoptionSpeed']]\n",
    "print(vv[col].corr())\n",
    "\n",
    "sub=pd.DataFrame(index=range(len(test_data)))\n",
    "sub['PetID']=test_data['PetID']\n",
    "sub['lgb1']=lgb1_test\n",
    "sub['lgb2']=lgb2_test\n",
    "sub['lgb3']=lgb3_test\n",
    "sub['lr']=lr_test\n",
    "sub['lgb5']=lgb5_test\n",
    "sub['lgb6']=lgb6_test\n",
    "sub['lgb7']=lgb7_test\n",
    "sub['cat1']=cat1_test\n",
    "sub['cat2']=cat2_test\n",
    "sub['nn1']=nn1_test\n",
    "sub['nn2']=nn2_test\n",
    "sub['nn3']=nn3_test\n",
    "# sub['ridge_pred']=test_data['ridge_pred']\n",
    "# sub['FTRL_pred']=test_data['FTRL_pred']\n",
    "# sub['svr_pred']=test_data['svr_pred']\n",
    "# sub['pac_pred']=test_data['pac_pred']\n",
    "# sub['sgd_pred']=test_data['sgd_pred']\n",
    "# sub['breed1_pred']=breed_encode_te['Breed1_str_pred']\n",
    "# sub['breed_pred']=breed_encode_te['breed_pred']\n",
    "# sub = pd.merge(sub,df_stack1,on='PetID',how=\"left\")\n",
    "# sub = pd.merge(sub,df_stack2,on='PetID',how=\"left\")\n",
    "sub = pd.merge(sub,df_stack3,on='PetID',how=\"left\")\n",
    "sub = pd.merge(sub,df_stack4,on='PetID',how=\"left\")\n",
    "\n",
    "print(sub[col].corr())\n",
    "\n",
    "del train_data,test_data,df_stack3,df_stack4\n",
    "gc.collect()\n",
    "\n",
    "train=vv\n",
    "test=sub\n",
    "\n",
    "vote = pd.DataFrame(index=range(len(test)))\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "train_predictions = np.array(lgb1_train)*0.3+np.array(lgb7_train)*0.3+np.array(lgb3_train)*0.2+np.array(nn3_train)*0.2\n",
    "blend_train=train_predictions.copy()\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "print(coefficients_)\n",
    "\n",
    "test_predictions =  np.array(lgb1_test)*0.3+np.array(lgb7_test)*0.3+np.array(lgb3_test)*0.2+np.array(nn3_test)*0.2\n",
    "blend_test=test_predictions.copy()\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['blend1']=test_predictions\n",
    "\n",
    "features = [x for x in train.columns if x not in ['PetID','AdoptionSpeed']]\n",
    "\n",
    "label='AdoptionSpeed'\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "         'max_depth':9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.9,\n",
    "          'min_split_gain': 0.01,\n",
    "          'min_child_samples': 150,\n",
    "          \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "           \"data_random_seed\":3,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "\n",
    "imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\n",
    "imp=imports.sort_values('importance', ascending=False)\n",
    "print(imp)\n",
    "\n",
    "lgb_train = [r[0] for r in results['train']]\n",
    "lgb_test = [r[0] for r in results['test']]\n",
    "optR = OptimizedRounder()\n",
    "train_predictions =lgb_train\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "\n",
    "test_predictions =  lgb_test\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['lgb']=test_predictions\n",
    "\n",
    "def runBR(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "\n",
    "\n",
    "    model = BayesianRidge()\n",
    "    model.fit(train_X,train_y)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    log=0#log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "#     pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2)\n",
    "#     pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y2[:,0]))]) \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), 0, coefficients, qwk,log\n",
    "results = run_cv_model(train[features], test[features], train[label], runBR, params, rmse, 'LR')\n",
    "\n",
    "br_train = [r[0] for r in results['train']]\n",
    "br_test = [r[0] for r in results['test']]\n",
    "optR = OptimizedRounder()\n",
    "train_predictions =br_train\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "\n",
    "test_predictions =  br_test\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['br']=test_predictions\n",
    "\n",
    "params={\n",
    "\t'booster':'gbtree',\n",
    "\t'objective': 'reg:linear',\n",
    "    \"tree_method\":\"gpu_hist\",\n",
    "            \"gpu_id\":0,\n",
    "#      'is_unbalance':'True',\n",
    "# \t'scale_pos_weight': 1500.0/13458.0,\n",
    "        'eval_metric': \"rmse\",\n",
    "\t'gamma':0.2,#0.2 is ok\n",
    "\t'max_depth':7,\n",
    "# \t'lambda':20,\n",
    "    # \"alpha\":5,\n",
    "        'subsample':0.9,\n",
    "        'colsample_bytree':0.9 ,\n",
    "        'min_child_weight':3, \n",
    "        'eta': 0.01,\n",
    "    # 'learning_rate':0.01,\n",
    "    \"silent\":1,\n",
    "\t'seed':1024,\n",
    "\t'nthread':12,\n",
    "     'num_rounds': 5000,\n",
    "    'verbose_eval': 200,\n",
    "    'early_stop':100,\n",
    "\n",
    "   \n",
    "    }\n",
    "def runXGB(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = xgb.DMatrix(train_X, label=train_y)\n",
    "    d_valid = xgb.DMatrix(test_X, label=test_y)\n",
    "    watchlist = [(d_train,'train'),\n",
    "    (d_valid,'val')\n",
    "             ]\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = xgb.train(params,\n",
    "                      d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      evals=watchlist,\n",
    "#                       fobj=softkappaObj,\n",
    "                      verbose_eval=verbose_eval,\n",
    "#                       feval=kappa_scorer,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(xgb.DMatrix(test_X),ntree_limit=model.best_ntree_limit)\n",
    "    log=0#log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "#     pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(xgb.DMatrix(test_X2),ntree_limit=model.best_ntree_limit)\n",
    "#     pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.get_fscore(), coefficients, qwk,log\n",
    "results = run_cv_model(train[features], test[features], train[label], runXGB, params, rmse, 'XGB')\n",
    "\n",
    "xgb_train = [r[0] for r in results['train']]\n",
    "xgb_test = [r[0] for r in results['test']]\n",
    "optR = OptimizedRounder()\n",
    "train_predictions =xgb_train\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "\n",
    "test_predictions =  xgb_test\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['xgb']=test_predictions\n",
    "\n",
    "\n",
    "results = run_cv_model(train[features], test[features], train[label], runCAT, params, rmse, 'CAT')\n",
    "cat_train = [r[0] for r in results['train']]\n",
    "cat_test = [r[0] for r in results['test']]\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "train_predictions =cat_train\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "\n",
    "test_predictions =  cat_test\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['cat']=test_predictions\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "train_predictions = (np.array(lgb_train)+np.array(xgb_train)+np.array(cat_train)+np.array(br_train))/4.0#[r[0] for r in results['train']]##np.array(lgb1_train)*0.7+np.array(lgb2_train)*0.3\n",
    "stack_train = train_predictions.copy()\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficiimport json\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error,log_loss\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split,KFold,GroupKFold,StratifiedShuffleSplit\n",
    "import cv2\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.linear_model import RidgeClassifier,LogisticRegression,SGDClassifier,SGDRegressor,LinearRegression,Ridge,PassiveAggressiveClassifier,PassiveAggressiveRegressor,BayesianRidge\n",
    "from wordbatch.models import FTRL,FM_FTRL\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag\n",
    "from sklearn.svm import LinearSVR,LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications import InceptionResNetV2, InceptionV3\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "from catboost import Pool, CatBoostClassifier,CatBoostRegressor\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import jieba\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import hstack, vstack\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.sparse import hstack\n",
    "import featuretools as ft\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "from PIL import Image\n",
    "import pprint\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "import gensim\n",
    "from keras.utils import to_categorical\n",
    "from itertools import product\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(369)\n",
    "rn.seed(369)\n",
    "tf.set_random_seed(1234)\n",
    "img_size = 256\n",
    "batch_size = 16\n",
    "embed_size=128\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "stops = {x: 1 for x in stopwords.words('english')}\n",
    "start_time=time.time()\n",
    "\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        '''\n",
    "        >>>example:\n",
    "        mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['regionidcity',\n",
    "                          'regionidneighborhood', 'regionidzip'],\n",
    "                target_type='regression'\n",
    "                )\n",
    "\n",
    "        X = mean_encoder.fit_transform(X, pd.Series(y))\n",
    "        X_test = mean_encoder.transform(X_test)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "\n",
    "        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new\n",
    "\n",
    "def get_embedding_matrix(word_index,embed_size=embed_size, Emed_path=\"w2v_128.txt\"):\n",
    "    embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        Emed_path, binary=False)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            count += 1\n",
    "    return embedding_matrix\n",
    "    \n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, weight_decay=1e-4,  # decoupled weight decay (1/4)\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        super(AdamW, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.wd = K.variable(weight_decay, name='weight_decay') # decoupled weight decay (2/4)\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "        wd = self.wd # decoupled weight decay (3/4)\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon) - lr * wd * p # decoupled weight decay (4/4)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'weight_decay': float(K.get_value(self.wd)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(AdamW, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n",
    "    \n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + tf.erf(x / tf.sqrt(2.0)))\n",
    "    return x * cdf\n",
    "    \n",
    "def lan_type(desc):\n",
    "    desc = str(desc)\n",
    "    if desc=='nan':\n",
    "        return 0\n",
    "    zh_model = re.compile(u'[\\u4e00-\\u9fa5]')    \n",
    "    en_model = re.compile(u'[a-zA-Z]')  \n",
    "    zh_match = zh_model.search(desc)\n",
    "    en_match = en_model.search(desc)\n",
    "    if zh_match and en_match:\n",
    "        return 3  \n",
    "    elif zh_match:\n",
    "        return 3  \n",
    "    elif en_match:\n",
    "        return 2  \n",
    "    else:\n",
    "        return 1  \n",
    "\n",
    "def malai_type(desc):\n",
    "    desc = str(desc)\n",
    "    malai = [' la x ' , ' nk ',' nie ', ' umur ', ' di ', 'teruk', ' satu ',' dh ', ' ni ',' tp ', ' yg ', 'mmg', 'msj', ' utk ' ,'neh' ]\n",
    "    for tag in malai:\n",
    "        if desc.find(tag) > -1:\n",
    "            return 1\n",
    "    \n",
    "    return  0\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower().strip()\n",
    "    for s in string.punctuation:\n",
    "        text = text.replace(s, ' ')\n",
    "    text = text.strip().split(' ')\n",
    "    return u' '.join(x for x in text if len(x) > 1 and x not in stops)\n",
    "    \n",
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image\n",
    "\n",
    "def img_model():\n",
    "    K.clear_session()\n",
    "    inp = Input((img_size, img_size, 3))\n",
    "    x = DenseNet121(\n",
    "            include_top=False, \n",
    "            weights=\"../input/keras-pretrain-model-weights/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\", \n",
    "            input_shape=(img_size, img_size, 3))(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Lambda(lambda x: K.expand_dims(x, axis = -1))(x)\n",
    "    x = AveragePooling1D(4)(x)\n",
    "    out = Lambda(lambda x: x[:, :, 0])(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    return model\n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = cohen_kappa_score(y, X_p,weights='quadratic')\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y,initial_coef=[0.5, 1.5, 2.5, 3.5]):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "#         initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef,len_0):\n",
    "        X_p = np.copy(X)\n",
    "        temp = sorted(list(X_p))\n",
    "        threshold=temp[int(0.95*len_0)-1]\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < threshold:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= threshold and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "\n",
    "def get_class_bounds(y, y_pred, N=5, class0_fraction=-1):\n",
    "    \"\"\"\n",
    "    Find boundary values for y_pred to match the known y class percentiles.\n",
    "    Returns N-1 boundaries in y_pred values that separate y_pred\n",
    "    into N classes (0, 1, 2, ..., N-1) with same percentiles as y has.\n",
    "    Can adjust the fraction in Class 0 by the given factor (>=0), if desired. \n",
    "    \"\"\"\n",
    "    ysort = np.sort(y)\n",
    "    predsort = np.sort(y_pred)\n",
    "    bounds = []\n",
    "    for ibound in range(N-1):\n",
    "        iy = len(ysort[ysort <= ibound])\n",
    "        # adjust the number of class 0 predictions?\n",
    "        if (ibound == 0) and (class0_fraction >= 0.0) :\n",
    "            iy = int(class0_fraction * iy)\n",
    "        bounds.append(predsort[iy])\n",
    "    return bounds\n",
    "\n",
    "def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n",
    "    kf = StratifiedKFold(n_splits=5, random_state=1017, shuffle=True)\n",
    "#     kf = GroupKFold(n_splits=5)\n",
    "#     fold_splits = kf.split(train, target,group)\n",
    "#     kf = StratifiedShuffleSplit(n_splits=10, test_size=0.45,  random_state=1017)\n",
    "    fold_splits = kf.split(train, target)\n",
    "    folds=5\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    pred_full_test = 0\n",
    "    log_list=[]\n",
    "    pred_train = np.zeros((train.shape[0], folds))\n",
    "    all_coefficients = np.zeros((folds, 4))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print( label + ' | FOLD ' + str(i) + '/'+str(folds))\n",
    "        if isinstance(train, pd.DataFrame):\n",
    "            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        else:\n",
    "            dev_X, val_X = train[dev_index], train[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        params2 = params.copy()\n",
    "        pred_val_y, pred_test_y, importances, coefficients, qwk,log = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        all_coefficients[i-1, :] = coefficients\n",
    "        if eval_fn is not None:\n",
    "            cv_score = eval_fn(val_y, pred_val_y)\n",
    "            cv_scores.append(cv_score)\n",
    "            qwk_scores.append(qwk)\n",
    "            log_list.append(log)\n",
    "            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n",
    "            print(\"##\"*40)\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] = train.columns.values\n",
    "        fold_importance_df['importance'] = importances\n",
    "        fold_importance_df['fold'] = i\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n",
    "        i += 1\n",
    "#     print('{} cv RMSE scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std RMSE score : {}'.format(label, np.std(cv_scores)))\n",
    "#     print('{} cv QWK scores : {}'.format(label, qwk_scores))\n",
    "    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n",
    "    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n",
    "    print('{} cv mean log_loss score : {}'.format(label, np.mean(log_list)))\n",
    "    \n",
    "    pred_full_test = pred_full_test / float(folds)\n",
    "    results = {'label': label,\n",
    "               'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores, 'qwk': qwk_scores,\n",
    "               'importance': feature_importance_df,\n",
    "               'coefficients': all_coefficients}\n",
    "    return results\n",
    "\n",
    "def get_feat1():\n",
    "    train_data = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    \n",
    "    def deal_breed(df):\n",
    "        if df['Breed1']==df['Breed2']:\n",
    "            df['Breed2']=0\n",
    "        if df['Breed1']!=307 & df['Breed2']==307:\n",
    "            temp=df[\"Breed1\"]\n",
    "            df['Breed1']=df['Breed2']\n",
    "            df['Breed2']=temp\n",
    "        return df\n",
    "    \n",
    "    train_data=train_data.apply(lambda x:deal_breed(x),1)\n",
    "    test_data=test_data.apply(lambda x:deal_breed(x),1)\n",
    "    \n",
    "    def get_purebreed_feat(df):\n",
    "        if df['Breed2']==0 and df['Breed1']!=307:\n",
    "            return 1\n",
    "        return 0\n",
    "    train_data['purebreed']=train_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    test_data['purebreed']=test_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    \n",
    "    train_data['is_group']=train_data.Gender.apply(lambda x:1 if x==3 else 0,1)\n",
    "    test_data['is_group']=test_data.Gender.apply(lambda x:1 if x==3 else 0,1)\n",
    "    \n",
    "    def get_good_cnt(df):\n",
    "        cnt=0\n",
    "        for i in ['Vaccinated',\"Dewormed\",\"Sterilized\",\"Health\"]:\n",
    "            if df[i]==1:\n",
    "                cnt+=1\n",
    "        return cnt\n",
    "    \n",
    "    train_data['good_cnt']=train_data.apply(lambda x:get_good_cnt(x),1)\n",
    "    test_data['good_cnt']=test_data.apply(lambda x:get_good_cnt(x),1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_data['lan_type'] = train_data.Description.map(lambda x:lan_type(x))\n",
    "    train_data['malai_type'] = train_data.Description.map(lambda x:malai_type(x))\n",
    "    \n",
    "    test_data['lan_type'] = test_data.Description.map(lambda x:lan_type(x))\n",
    "    test_data['malai_type'] = test_data.Description.map(lambda x:malai_type(x))\n",
    "    \n",
    "    def name_deal(df):\n",
    "        if \"No Name\" in df:\n",
    "            return np.nan\n",
    "        if df ==\"nan\":\n",
    "            return np.nan\n",
    "        else:\n",
    "            return df\n",
    "    train_data['Name'] = train_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    test_data['Name'] = test_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    \n",
    "    train_data['Name'],indexer=pd.factorize(train_data['Name'])\n",
    "    test_data['Name'] = indexer.get_indexer(test_data['Name'])\n",
    "    \n",
    "    rescuer_df=train_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    train_data=pd.merge(train_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    # train_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    rescuer_df=test_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    test_data=pd.merge(test_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    # test_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    del rescuer_df\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['rescuer_rank'] = train_data['RescuerID'].map(train_data['RescuerID'].value_counts().rank()/len(train_data['RescuerID'].unique()))\n",
    "    test_data['rescuer_rank'] = test_data['RescuerID'].map(test_data['RescuerID'].value_counts().rank()/len(test_data['RescuerID'].unique()))\n",
    "    \n",
    "    def get_res_feat(df):\n",
    "        temp=pd.DataFrame(index=range(1))\n",
    "        temp['RescuerID']=df['RescuerID'].values[0]\n",
    "        temp['res_type_cnt']=len(df['Type'].unique())\n",
    "        temp['res_breed_cnt']=len(df['Breed1'].unique())\n",
    "        temp['res_breed_mode']=df['Breed1'].mode()\n",
    "        temp['res_fee_mean']=df['Fee'].mean()\n",
    "        temp['res_Quantity_sum']=df['Quantity'].sum()\n",
    "        temp['res_MaturitySize_mean']=df['MaturitySize'].mean()\n",
    "        temp['res_Description_unique']=len(df['Description'].unique())\n",
    "        return temp\n",
    "    train_res_feat=train_data.groupby(\"RescuerID\",as_index=False).apply(lambda x:get_res_feat(x))\n",
    "    test_res_feat=test_data.groupby(\"RescuerID\",as_index=False).apply(lambda x:get_res_feat(x))\n",
    "    train_res_feat.index=range(len(train_res_feat))\n",
    "    test_res_feat.index=range(len(test_res_feat))\n",
    "    train_data = pd.merge(train_data,train_res_feat,on=\"RescuerID\",how=\"left\")\n",
    "    test_data = pd.merge(test_data,test_res_feat,on=\"RescuerID\",how=\"left\")\n",
    "    train_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    test_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    train_data['fee-mean_fee']=train_data['Fee']-train_data['res_fee_mean']\n",
    "    test_data['fee-mean_fee']=test_data['Fee']-test_data['res_fee_mean']\n",
    "    del train_res_feat,test_res_feat\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['Description'] = train_data['Description'].fillna(\"null\")\n",
    "    test_data['Description'] = test_data['Description'].fillna(\"null\")\n",
    "    \n",
    "    train_data[\"Description\"] = train_data[\"Description\"].str.lower()\n",
    "    test_data[\"Description\"] = test_data[\"Description\"].str.lower()\n",
    "    \n",
    "    puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "     '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "     '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "     '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "     '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "    def clean_text(x):\n",
    "\n",
    "        x = str(x)\n",
    "        for punct in puncts:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "        return x\n",
    "\n",
    "\n",
    "    train_data[\"Description\"] = train_data[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    test_data[\"Description\"] = test_data[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    \n",
    "\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # ## Number of words in the text ##\n",
    "    train_data[\"num_words\"] = train_data[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "    test_data[\"num_words\"] = test_data[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # ## Number of unique words in the text ##\n",
    "    train_data[\"num_unique_words\"] = train_data[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "    test_data[\"num_unique_words\"] = test_data[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    # ## Number of characters in the text ##\n",
    "    train_data[\"num_chars\"] = train_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    test_data[\"num_chars\"] = test_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    # ## Number of stopwords in the text ##\n",
    "    train_data[\"num_stopwords\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    test_data[\"num_stopwords\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    \n",
    "    # ## Number of punctuations in the text ##\n",
    "    train_data[\"num_punctuations\"] =train_data['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    test_data[\"num_punctuations\"] =test_data['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    \n",
    "    # ## Number of title case words in the text ##\n",
    "    # train_data[\"num_words_upper\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    # test_data[\"num_words_upper\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    \n",
    "    # ## Number of title case words in the text ##\n",
    "    # train_data[\"num_words_title\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    # test_data[\"num_words_title\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    \n",
    "    # ## Average length of the words in the text ##\n",
    "    train_data[\"mean_word_len\"] = train_data[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    test_data[\"mean_word_len\"] = test_data[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    # # train_data['num_vs_len']=train_data['num_punctuations']/train_data['num_chars']\n",
    "    # # test_data['num_vs_len']=test_data['num_punctuations']/test_data['num_chars']\n",
    "    \n",
    "    # # train_data['up_vs_len'] = train_data['num_words_upper'] / train_data['num_words']\n",
    "    # # test_data['up_vs_len'] = test_data['num_words_upper'] / test_data['num_words']\n",
    "    \n",
    "    # # train_data['senten_cnt']=train_data[\"Description\"].apply(lambda x:len(str(x).split(\".\")),1)\n",
    "    # # test_data['senten_cnt']=test_data[\"Description\"].apply(lambda x:len(str(x).split(\".\")),1)\n",
    "    \n",
    "    \n",
    "    def deal_desc(df):\n",
    "        if df['lan_type']==1:\n",
    "            return \"null\"\n",
    "        if df['lan_type']==3:\n",
    "            text=jieba.cut(df['Description'])\n",
    "            text=\" \".join(text)\n",
    "            text=text.replace(\"   \",\" \")\n",
    "            return text\n",
    "        else:\n",
    "            return df['Description']\n",
    "    train_data['Description']=train_data.apply(lambda x:deal_desc(x),1)\n",
    "    test_data['Description']=test_data.apply(lambda x:deal_desc(x),1)\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    train_data['doc_sent_mag'] = doc_sent_mag\n",
    "    train_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    test_data['doc_sent_mag'] = doc_sent_mag\n",
    "    test_data['doc_sent_score'] = doc_sent_score\n",
    "    del doc_sent_mag,doc_sent_score\n",
    "    gc.collect()\n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    print(nl_count)\n",
    "    train_data[ 'vertex_x'] = vertex_xs\n",
    "    train_data['vertex_y'] = vertex_ys\n",
    "    train_data['bounding_confidence'] = bounding_confidences\n",
    "    train_data['bounding_importance'] = bounding_importance_fracs\n",
    "    train_data['dominant_blue'] = dominant_blues\n",
    "    train_data['dominant_green'] = dominant_greens\n",
    "    train_data['dominant_red'] = dominant_reds\n",
    "    train_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    train_data['dominant_score'] = dominant_scores\n",
    "    train_data['label_description'] = label_descriptions\n",
    "    train_data['label_score'] = label_scores\n",
    "    \n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    test_data[ 'vertex_x'] = vertex_xs\n",
    "    test_data['vertex_y'] = vertex_ys\n",
    "    test_data['bounding_confidence'] = bounding_confidences\n",
    "    test_data['bounding_importance'] = bounding_importance_fracs\n",
    "    test_data['dominant_blue'] = dominant_blues\n",
    "    test_data['dominant_green'] = dominant_greens\n",
    "    test_data['dominant_red'] = dominant_reds\n",
    "    test_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    test_data['dominant_score'] = dominant_scores\n",
    "    test_data['label_description'] = label_descriptions\n",
    "    test_data['label_score'] = label_scores\n",
    "    \n",
    "    del  vertex_xs,vertex_ys,bounding_confidences,bounding_importance_fracs,dominant_blues,dominant_greens,dominant_reds,dominant_pixel_fracs,dominant_scores\n",
    "    del label_descriptions,label_scores\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['label_description'] =train_data['label_description'].astype(np.str)\n",
    "    train_data['label_description'] =train_data['label_description'].astype('category')\n",
    "    \n",
    "    test_data['label_description'] =test_data['label_description'].astype(np.str)\n",
    "    test_data['label_description'] =test_data['label_description'].astype('category')\n",
    "    \n",
    "    tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    print(\"TFIDF....\")\n",
    "    tfv.fit(list(train_data['Description'].values)+list(test_data['Description'].values))\n",
    "    X =  tfv.transform(train_data['Description'])\n",
    "    X_test = tfv.transform(test_data['Description'])\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=10)\n",
    "    svd.fit(X)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['svg_{}'.format(i) for i in range(10)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['svg_{}'.format(i) for i in range(10)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    breed=pd.read_csv(\"../input/petfinder-adoption-prediction/breed_labels.csv\")\n",
    "    color = pd.read_csv(\"../input/petfinder-adoption-prediction/color_labels.csv\")\n",
    "    color_dict = dict(zip(color['ColorID'].values.astype(\"str\"),color['ColorName'].values))\n",
    "    breed_dict = dict(zip(breed['BreedID'].values.astype(\"str\"),breed['BreedName'].values))\n",
    "    def get_text(df):\n",
    "        x=\"\"\n",
    "        if df['Type']==1:\n",
    "            x+=\"dog\"+\" \"\n",
    "        if df['Type']==2:\n",
    "            x+=\"cat\"+\" \"\n",
    "        for i in ['Breed1',\"Breed2\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=breed_dict[str(df[i])]+\" \"\n",
    "        for i in [\"Color1\",\"Color2\",\"Color3\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=color_dict[str(df[i])]+\" \"\n",
    "        x=x+df['Description']\n",
    "        return x\n",
    "    train_data['concat_text']=train_data.apply(lambda x:get_text(x),1)\n",
    "    test_data['concat_text']=test_data.apply(lambda x:get_text(x),1)\n",
    "    \n",
    "    train_desc=train_data['concat_text'].values\n",
    "    test_desc=test_data['concat_text'].values\n",
    "    \n",
    "    tfv.fit(list(train_data['concat_text'].values)+list(test_data['concat_text'].values))\n",
    "    X =  tfv.transform(train_data['concat_text'])\n",
    "    X_test = tfv.transform(test_data['concat_text'])\n",
    "    \n",
    "    \n",
    "\n",
    "    svd = NMF(n_components=5,random_state=100)\n",
    "    svd.fit(vstack([X,X_test]))\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['nmf_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['nmf_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    svd = LatentDirichletAllocation(n_components=5,max_iter=30, random_state=100)\n",
    "    svd.fit(vstack([X,X_test]))\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['lda_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['lda_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    tfv =  CountVectorizer(min_df=3,  \n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 5),\n",
    "        stop_words = 'english')\n",
    "    tfv.fit(list(train_data['Description'].values)+list(test_data['Description'].values))\n",
    "    X =  tfv.transform(train_data['Description'])\n",
    "    X_test = tfv.transform(test_data['Description'])\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=5)\n",
    "    svd.fit(vstack([X,X_test]))\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['nb_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['nb_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    svd = LatentDirichletAllocation(n_components=5,max_iter=30, random_state=10)\n",
    "    svd.fit(vstack([X,X_test]))\n",
    "    X_svg = svd.transform(X)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['c_lda_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(X_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['c_lda_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    \n",
    "    onehot_col=[\"Breed1\",\"Breed2\",\"Color1\",\"Color2\",\"Color3\",'State','Gender','MaturitySize','MaturitySize','FurLength','Vaccinated',\n",
    "           'Dewormed','Sterilized','Health']\n",
    "    data=pd.concat([train_data,test_data])\n",
    "    data.index=range(len(data))\n",
    "    onehot_df=pd.DataFrame(index=range(len(data)))\n",
    "    for i in onehot_col:\n",
    "        temp=pd.get_dummies(data[i],prefix=i)\n",
    "        onehot_df=pd.concat([onehot_df,temp],1)\n",
    "        \n",
    "    svd = TruncatedSVD(n_components=5)\n",
    "    svd.fit(onehot_df)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    oh_df = svd.transform(onehot_df)\n",
    "    oh_df = pd.DataFrame(oh_df, columns=['oh_{}'.format(i) for i in range(5)])\n",
    "    oh_df['PetID']=data['PetID']\n",
    "    \n",
    "    del X,X_test,onehot_df,data\n",
    "    gc.collect()\n",
    "    \n",
    "    wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n",
    "                                                              \"hash_ngrams_weights\": [1.5, 1.0],\n",
    "                                                              \"hash_size\": 2 ** 29,\n",
    "                                                              \"norm\": None,\n",
    "                                                              \"tf\": 'binary',\n",
    "                                                              \"idf\": None,\n",
    "                                                              }), procs=8)\n",
    "    x_train = wb.fit_transform(train_data[\"Description\"])\n",
    "    x_test = wb.transform(test_data[\"Description\"])\n",
    "    mask = np.array(np.clip(x_train.getnnz(axis=0) -8 , 0, 1), dtype=bool)\n",
    "    x_train=x_train[:,mask]\n",
    "    x_test=x_test[:,mask]\n",
    "    print(x_test.shape)\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=5)\n",
    "    svd.fit(x_train)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    X_svg = svd.transform(x_train)\n",
    "    X_svg = pd.DataFrame(X_svg, columns=['wb_{}'.format(i) for i in range(5)])\n",
    "    \n",
    "    X_test_svg = svd.transform(x_test)\n",
    "    X_test_svg = pd.DataFrame(X_test_svg, columns=['wb_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, X_svg), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test_svg), axis=1)\n",
    "    del x_test,x_train,X_test_svg,X_svg,wb,svd\n",
    "    gc.collect()\n",
    "    \n",
    "    train_df = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "    img_size = 256\n",
    "    batch_size = 16\n",
    "    pet_ids = train_df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    extract_model = img_model()\n",
    "    img_features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = extract_model.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            img_features[pet_id] = batch_preds[i]\n",
    "    train_img_feat = pd.DataFrame.from_dict(img_features, orient='index')\n",
    "    #train_feats.to_csv('train_img_features.csv')\n",
    "    \n",
    "    test_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n",
    "    pet_ids = test_df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    img_features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = extract_model.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            img_features[pet_id] = batch_preds[i]\n",
    "            \n",
    "    test_img_feat = pd.DataFrame.from_dict(img_features, orient='index')\n",
    "    del img_features\n",
    "    gc.collect()\n",
    "    \n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(train_img_feat.values)\n",
    "    train_img=pca.transform(train_img_feat.values)\n",
    "    test_img=pca.transform(test_img_feat.values)\n",
    "    train_img = pd.DataFrame(train_img, columns=['pca_{}'.format(i) for i in range(5)])\n",
    "    test_img = pd.DataFrame(test_img, columns=['pca_{}'.format(i) for i in range(5)])\n",
    "    train_data = pd.concat((train_data, train_img), axis=1)\n",
    "    test_data = pd.concat((test_data, test_img), axis=1)\n",
    "    del train_img,test_img,pca\n",
    "    gc.collect()\n",
    "    \n",
    "    state= pd.read_csv(\"../input/petfinder-adoption-prediction/state_labels.csv\")\n",
    "\n",
    "    state_dict={\n",
    "        \"Johor\":[3233434,19210,168],\n",
    "        \"Kedah\":[1890098,9500,199],\n",
    "        \"Kelantan\":[1459994,15099,97],\n",
    "        \"Kuala Lumpur\":[1627172,243,6696],\n",
    "        \"Labuan\":[85272,91,937],\n",
    "        \"Melaka\":[788706,1664,474],\n",
    "        \"Negeri Sembilan\":[997071,6686,149],\n",
    "        \"Pahang\":[1443365,36137,40],\n",
    "        \"Perak\":[2258428,21035,107],\n",
    "        \"Perlis\":[227025,821,277],\n",
    "        \"Pulau Pinang\":[1520143,1048,1451],\n",
    "        \"Sabah\":[3120040,73631,42],\n",
    "        \"Sarawak\":[2420009,124450,19],\n",
    "        \"Selangor\":[5411324,8104,668],\n",
    "        \"Terengganu\":[1015776,13035,78]\n",
    "    }\n",
    "    def get_state_feat(df):\n",
    "        df['state_people/area_ratio']=state_dict[df['StateName']][2]\n",
    "        return df\n",
    "    state=state.apply(lambda x:get_state_feat(x),1)\n",
    "    state['state_rank']=state['state_people/area_ratio'].rank()\n",
    "    state_ratio_dict=dict(zip(state.StateID.values,state['state_people/area_ratio'].values))\n",
    "    state_rank_dict=dict(zip(state.StateID.values,state['state_rank'].values))\n",
    "    \n",
    "    train_data['State_ratio']=train_data['State'].map(state_ratio_dict)\n",
    "    test_data['State_ratio']=test_data['State'].map(state_ratio_dict)\n",
    "    \n",
    "    train_data['State_rank']=train_data['State'].map(state_rank_dict)\n",
    "    test_data['State_rank']=test_data['State'].map(state_rank_dict)\n",
    "    \n",
    "    del state_rank_dict,state_ratio_dict,state\n",
    "    gc.collect()\n",
    "    # train_data['is_high_state_ratio']=train_data['State_ratio'].apply(lambda x:1 if x>168 else 0,1)\n",
    "    # test_data['is_high_state_ratio']=test_data['State_ratio'].apply(lambda x:1 if x>168 else 0,1)\n",
    "    \n",
    "    def get_breed(df):\n",
    "        x=\"\"\n",
    "        for i in [\"Breed1\",\"Breed2\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=breed_dict[str(df[i])]+\" \"\n",
    "        return x\n",
    "    train_data['breed']=train_data.apply(lambda x:get_breed(x),1)\n",
    "    test_data['breed']=test_data.apply(lambda x:get_breed(x),1)   \n",
    "    \n",
    "    train_data['Breed1']=train_data['Breed1'].astype(\"str\")\n",
    "    train_data['Breed1']=train_data['Breed1'].map(breed_dict)\n",
    "    train_data['Breed1']=train_data['Breed1'].replace(np.nan,\"null\")\n",
    "    train_data['Breed2']=train_data['Breed2'].astype(\"str\")\n",
    "    train_data['Breed2']=train_data['Breed2'].map(breed_dict)\n",
    "    train_data['Breed2']=train_data['Breed2'].replace(np.nan,\"null\")\n",
    "    \n",
    "    test_data['Breed1']=test_data['Breed1'].astype(\"str\")\n",
    "    test_data['Breed1']=test_data['Breed1'].map(breed_dict)\n",
    "    test_data['Breed1']=test_data['Breed1'].replace(np.nan,\"null\")\n",
    "    test_data['Breed2']=test_data['Breed2'].astype(\"str\")\n",
    "    test_data['Breed2']=test_data['Breed2'].map(breed_dict)\n",
    "    test_data['Breed2']=test_data['Breed2'].replace(np.nan,\"null\")\n",
    "    \n",
    "    def get_color_cnt(df):\n",
    "        color_list=[]\n",
    "        for i in [\"Color1\",\"Color2\",\"Color3\"]:\n",
    "            if df[i]!=0:\n",
    "                color_list.append(df[i])\n",
    "        return len(set(color_list))\n",
    "    train_data['color_cnt']=train_data.apply(lambda x:get_color_cnt(x),1)\n",
    "    test_data['color_cnt']=test_data.apply(lambda x:get_color_cnt(x),1)\n",
    "    \n",
    "    def get_color(df):\n",
    "        x=\"\"\n",
    "        for i in [\"Color1\",\"Color2\",\"Color3\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=color_dict[str(df[i])]+\" \"\n",
    "        return x\n",
    "    train_data['color']=train_data.apply(lambda x:get_color(x),1)\n",
    "    test_data['color']=test_data.apply(lambda x:get_color(x),1)\n",
    "    \n",
    "    mean_encoder = MeanEncoder( categorical_features=['Breed1', 'breed'],target_type ='regression')\n",
    "    train_data = mean_encoder.fit_transform(train_data, train_data['AdoptionSpeed'])\n",
    "    test_data = mean_encoder.transform(test_data)\n",
    "    for col in ['Breed1',\"Breed2\",\"color\",\"breed\"]:\n",
    "        lbl = LabelEncoder()\n",
    "        train_data[col]=train_data[col].fillna(0)\n",
    "        test_data[col]=test_data[col].fillna(0)\n",
    "        lbl.fit(list(train_data[col].values)+list(test_data[col].values))\n",
    "        train_data[col]=lbl.transform(train_data[col])\n",
    "        test_data[col]=lbl.transform(test_data[col])\n",
    "        \n",
    "    \n",
    "    cols = [x for x in train_data.columns if x not in ['Breed1',\"breed\",'label_description',\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "    data=pd.concat([train_data,test_data])\n",
    "    train_data[cols]=train_data[cols].fillna(0)\n",
    "    test_data[cols]=test_data[cols].fillna(0)\n",
    "    ############################ 切分数据集 ##########################\n",
    "    print('开始进行一些前期处理')\n",
    "    train_feature = train_data[cols].values\n",
    "    test_feature = test_data[cols].values\n",
    "        # 五则交叉验证\n",
    "    n_folds = 5\n",
    "    print('处理完毕')\n",
    "    df_stack2 = pd.DataFrame()\n",
    "    df_stack2['PetID']=data['PetID']\n",
    "    for label in [\"AdoptionSpeed\"]:\n",
    "        score = train_data[label]\n",
    "        \n",
    "       \n",
    "        ########################### SGD(随机梯度下降) ################################\n",
    "        print('sgd stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            sgd = SGDRegressor(random_state=1017,)\n",
    "            sgd.fit(train_feature[tr], score[tr])\n",
    "            score_va = sgd.predict(train_feature[va])\n",
    "            score_te = sgd.predict(test_feature)\n",
    "            print('得分' + str(mean_squared_error(score[va], sgd.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0]+= score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "        df_stack2['tfidf_sgd_classfiy_{}'.format(label)] = stack[:,0]\n",
    "    \n",
    "    \n",
    "        ########################### pac(PassiveAggressiveClassifier) ################################\n",
    "        print('PAC stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            pac = PassiveAggressiveRegressor(random_state=1017)\n",
    "            pac.fit(train_feature[tr], score[tr])\n",
    "            score_va = pac.predict(train_feature[va])\n",
    "            score_te = pac.predict(test_feature)\n",
    "          \n",
    "            print('得分' + str(mean_squared_error(score[va], pac.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0] += score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "        df_stack2['tfidf_pac_classfiy_{}'.format(label)] = stack[:,0]\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        ########################### FTRL ################################\n",
    "        print('MultinomialNB stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            clf = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=train_feature.shape[1], iters=50, inv_link=\"identity\", threads=1)\n",
    "            clf.fit(train_feature[tr], score[tr])\n",
    "            score_va = clf.predict(train_feature[va])\n",
    "            score_te = clf.predict(test_feature)\n",
    "          \n",
    "            print('得分' + str(mean_squared_error(score[va], clf.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0] += score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "        \n",
    "        df_stack2['tfidf_FTRL_classfiy_{}'.format(label)] = stack[:,0]\n",
    "        \n",
    "        ########################### ridge(RidgeClassfiy) ################################\n",
    "        print('RidgeClassfiy stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            ridge = Ridge(solver=\"sag\", fit_intercept=True, random_state=42, alpha=30) \n",
    "            ridge.fit(train_feature[tr], score[tr])\n",
    "            score_va = ridge.predict(train_feature[va])\n",
    "            score_te = ridge.predict(test_feature)\n",
    "           \n",
    "            print('得分' + str(mean_squared_error(score[va], ridge.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0] += score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "        df_stack2['tfidf_ridge_classfiy_{}'.format(label)] = stack[:,0]\n",
    "        \n",
    "        ############################ Linersvc(LinerSVC) ################################\n",
    "        print('LinerSVC stacking')\n",
    "        stack_train = np.zeros((len(train_data),1))\n",
    "        stack_test = np.zeros((len(test_data),1))\n",
    "        score_va = 0\n",
    "    \n",
    "        sk = StratifiedKFold( n_splits=5, random_state=1017)\n",
    "        for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "            print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "            lsvc = LinearSVR(random_state=1017)\n",
    "            lsvc.fit(train_feature[tr], score[tr])\n",
    "            score_va = lsvc.predict(train_feature[va])\n",
    "            score_te = lsvc.predict(test_feature)\n",
    "           \n",
    "            print('得分' + str(mean_squared_error(score[va], lsvc.predict(train_feature[va]))))\n",
    "            stack_train[va,0] = score_va\n",
    "            stack_test[:,0] += score_te\n",
    "        stack_test /= n_folds\n",
    "        stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "        df_stack2['tfidf_lsvc_classfiy_{}'.format(label)] = stack[:,0]\n",
    "        \n",
    "    # df_stack.to_csv('graph_tfidf_classfiy.csv', index=None, encoding='utf8')\n",
    "    print('tfidf特征已保存\\n')\n",
    "    del stack,train_feature,test_feature,stack_train, stack_test,data\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    \n",
    "    train=pd.merge(train_data,oh_df,on=\"PetID\",how=\"left\")\n",
    "    test=pd.merge(test_data,oh_df,on=\"PetID\",how=\"left\")\n",
    "    \n",
    "    train=pd.merge(train,df_stack2,on=\"PetID\",how=\"left\")\n",
    "    test=pd.merge(test,df_stack2,on=\"PetID\",how=\"left\")\n",
    "    \n",
    "    del train_data,test_data,df_stack2,oh_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return train,test,train_desc,test_desc,train_img_feat,test_img_feat\n",
    "    \n",
    "def runLGB_c(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "#                       fobj=softkappaObj,\n",
    "                      verbose_eval=verbose_eval,\n",
    "#                       feval=kappa_scorer,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    log=log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "    pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "                               ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "                               ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk,log\n",
    "train,test,train_desc,test_desc,train_img_feat,test_img_feat=get_feat1()\n",
    "train1=train.copy()\n",
    "test1=test.copy()\n",
    "train1.drop(['pca_{}'.format(i) for i in range(5)],1,inplace=True)\n",
    "test1.drop(['pca_{}'.format(i) for i in range(5)],1,inplace=True)\n",
    "\n",
    "features = [x for x in train.columns if x not in ['Breed1',\"breed\",\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "\n",
    "label='AdoptionSpeed'\n",
    "###model 1\n",
    "# params = {\n",
    "# #     'application': 'regression',\n",
    "#     'objective': 'multiclass', \n",
    "#     \"num_class\":5,\n",
    "#           'boosting': 'gbdt',\n",
    "# #           'metric': 'rmse',\n",
    "#     'metric':{'multi_logloss',},\n",
    "#           'num_leaves': 80,\n",
    "#          'max_depth':9,\n",
    "#           'learning_rate': 0.01,\n",
    "#           'bagging_fraction': 0.90,\n",
    "#            \"bagging_freq\":3,\n",
    "#           'feature_fraction': 0.85,\n",
    "#           'min_split_gain': 0.01,\n",
    "#           'min_child_samples': 150,\n",
    "#           \"lambda_l1\": 0.1,\n",
    "#           'verbosity': -1,\n",
    "#           'early_stop': 100,\n",
    "#           'verbose_eval': 200,\n",
    "#           \"data_random_seed\":3,\n",
    "# #           \"random_state\":1017,\n",
    "#           'num_rounds': 10000}\n",
    "params = {\n",
    "#     'application': 'regression',\n",
    "    'objective': 'multiclass', \n",
    "    \"num_class\":5,\n",
    "          'boosting': 'gbdt',\n",
    "#           'metric': 'rmse',\n",
    "    'metric':{'multi_logloss',},\n",
    "          'num_leaves': 55,\n",
    "         'max_depth':9,\n",
    "        'max_bin': 45,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9879639408647978,\n",
    "            \"bagging_freq\":41,\n",
    "           'feature_fraction': 0.5849356442713105,\n",
    "           'min_split_gain': 0.6118528947223795,\n",
    "         'min_child_samples': 83,\n",
    "     'min_child_weight': 0.2912291401980419,\n",
    "     'lambda_l1': 0.18182496720710062,\n",
    "          'lambda_l2': 0.18340450985343382,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "          \"data_random_seed\":17,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB_c, params, rmse, 'LGB')    \n",
    "imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\n",
    "imp=imports.sort_values('importance', ascending=False)\n",
    "print(imp)\n",
    "lgb1_train=[r[0] for r in results['train']]\n",
    "lgb1_test=[r[0] for r in results['test']]\n",
    "t1=time.time()\n",
    "print(\"model1 cost:{} s\".format(t1-start_time))\n",
    "\n",
    "###model 2\n",
    "features = [x for x in train.columns if x not in ['label_description','Breed1',\"breed\",\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "def runCAT(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = Pool(train_X, label=train_y)\n",
    "    d_valid = Pool(test_X, label=test_y)\n",
    "    watchlist = (d_train, d_valid)\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = CatBoostClassifier(iterations=num_rounds, \n",
    "        learning_rate = 0.03,\n",
    "        od_type='Iter',\n",
    "         od_wait=early_stop,\n",
    "        loss_function='MultiClass',\n",
    "        eval_metric='MultiClass',\n",
    "        bagging_temperature=0.9,                   \n",
    "        random_seed = 2019,\n",
    "        task_type='GPU'\n",
    "                          )\n",
    "    model.fit(d_train,eval_set=d_valid,\n",
    "            use_best_model=True,\n",
    "            verbose=verbose_eval\n",
    "                         )\n",
    "    \n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    log=log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "    pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "                               ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 =  model.predict_proba(test_X2)\n",
    "    pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "                               ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), 0, coefficients, qwk,log\n",
    "results = run_cv_model(train[features], test[features], train[label], runCAT, params, rmse, 'CAT')\n",
    "cat1_train=[r[0] for r in results['train']]\n",
    "cat1_test=[r[0] for r in results['test']]\n",
    "t2=time.time()\n",
    "print(\"model2 cost:{} s\".format(t2-t1))\n",
    "###model 3\n",
    "features = [x for x in train.columns if x not in ['Breed1_pred',\"breed_pred\",\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "label='AdoptionSpeed'\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "         'max_depth':9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.85,\n",
    "          'min_split_gain': 0.01,\n",
    "          'min_child_samples': 150,\n",
    "          \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "           \"data_random_seed\":3,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "def runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "#                       fobj=softkappaObj,\n",
    "                      verbose_eval=verbose_eval,\n",
    "#                       feval=kappa_scorer,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    log=0#log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "#     pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "    \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "#     pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(), coefficients, qwk,log\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "\n",
    "lgb2_train=[r[0] for r in results['train']]\n",
    "lgb2_test=[r[0] for r in results['test']]\n",
    "del train,test\n",
    "gc.collect()\n",
    "t3=time.time()\n",
    "print(\"model3 cost:{} s\".format(t3-t2))\n",
    "###model 4\n",
    "def get_feat2():\n",
    "    train_data = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    def get_purebreed_feat(df):\n",
    "        if df['Breed2']==0 and df['Breed1']!=307:\n",
    "            return 1\n",
    "        return 0\n",
    "    train_data['purebreed']=train_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    test_data['purebreed']=test_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    train_data['is_group']=train_data.Gender.apply(lambda x:1 if x==3 else 0,1)\n",
    "    test_data['is_group']=test_data.Gender.apply(lambda x:1 if x==3 else 0,1)\n",
    "    def name_deal(df):\n",
    "        if \"No Name\" in df:\n",
    "            return np.nan\n",
    "        if df ==\"nan\":\n",
    "            return np.nan\n",
    "        else:\n",
    "            return df\n",
    "    train_data['Name'] = train_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    test_data['Name'] = test_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    \n",
    "    train_data['Name'],indexer=pd.factorize(train_data['Name'])\n",
    "    test_data['Name'] = indexer.get_indexer(test_data['Name'])\n",
    "    group = train_data['RescuerID'].values\n",
    "    rescuer_df=train_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    train_data=pd.merge(train_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    train_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    rescuer_df=test_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    test_data=pd.merge(test_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    test_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    ## Number of words in the text ##\n",
    "    train_data[\"num_words\"] = train_data[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "    test_data[\"num_words\"] = test_data[\"Description\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    ## Number of unique words in the text ##\n",
    "    train_data[\"num_unique_words\"] = train_data[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "    test_data[\"num_unique_words\"] = test_data[\"Description\"].apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    ## Number of characters in the text ##\n",
    "    train_data[\"num_chars\"] = train_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    test_data[\"num_chars\"] = test_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    ## Number of stopwords in the text ##\n",
    "    train_data[\"num_stopwords\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    test_data[\"num_stopwords\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "    \n",
    "    ## Number of punctuations in the text ##\n",
    "    train_data[\"num_punctuations\"] =train_data['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    test_data[\"num_punctuations\"] =test_data['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train_data[\"num_words_upper\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    test_data[\"num_words_upper\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "    \n",
    "    ## Number of title case words in the text ##\n",
    "    train_data[\"num_words_title\"] = train_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    test_data[\"num_words_title\"] = test_data[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "    \n",
    "    ## Average length of the words in the text ##\n",
    "    train_data[\"mean_word_len\"] = train_data[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    test_data[\"mean_word_len\"] = test_data[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    train_data['Description'] = train_data['Description'].fillna(\"null\")\n",
    "    test_data['Description'] = test_data['Description'].fillna(\"null\")\n",
    "    data = pd.concat([train_data,test_data])\n",
    "    data.index=range(len(data))\n",
    "    data_id=data['PetID'].values\n",
    "    \n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    train_data['doc_sent_mag'] = doc_sent_mag\n",
    "    train_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    test_data['doc_sent_mag'] = doc_sent_mag\n",
    "    test_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    print(nl_count)\n",
    "    train_data[ 'vertex_x'] = vertex_xs\n",
    "    train_data['vertex_y'] = vertex_ys\n",
    "    train_data['bounding_confidence'] = bounding_confidences\n",
    "    train_data['bounding_importance'] = bounding_importance_fracs\n",
    "    train_data['dominant_blue'] = dominant_blues\n",
    "    train_data['dominant_green'] = dominant_greens\n",
    "    train_data['dominant_red'] = dominant_reds\n",
    "    train_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    train_data['dominant_score'] = dominant_scores\n",
    "    train_data['label_description'] = label_descriptions\n",
    "    train_data['label_score'] = label_scores\n",
    "    \n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    test_data[ 'vertex_x'] = vertex_xs\n",
    "    test_data['vertex_y'] = vertex_ys\n",
    "    test_data['bounding_confidence'] = bounding_confidences\n",
    "    test_data['bounding_importance'] = bounding_importance_fracs\n",
    "    test_data['dominant_blue'] = dominant_blues\n",
    "    test_data['dominant_green'] = dominant_greens\n",
    "    test_data['dominant_red'] = dominant_reds\n",
    "    test_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    test_data['dominant_score'] = dominant_scores\n",
    "    test_data['label_description'] = label_descriptions\n",
    "    test_data['label_score'] = label_scores\n",
    "    \n",
    "    del  vertex_xs,vertex_ys,bounding_confidences,bounding_importance_fracs,dominant_blues,dominant_greens,dominant_reds,dominant_pixel_fracs,dominant_scores\n",
    "    del label_descriptions,label_scores,doc_sent_mag,doc_sent_score\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['label_description'] =train_data['label_description'].astype(np.str)\n",
    "    train_data['label_description'] =train_data['label_description'].astype('category')\n",
    "    \n",
    "    test_data['label_description'] =test_data['label_description'].astype(np.str)\n",
    "    test_data['label_description'] =test_data['label_description'].astype('category')\n",
    "    \n",
    "    tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "    print(\"TFIDF....\")\n",
    "    tfv.fit(list(train_data['Description'].values)+list(test_data['Description'].values))\n",
    "    X =  tfv.transform(train_data['Description'])\n",
    "    X_test = tfv.transform(test_data['Description'])\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=120)\n",
    "    svd.fit(X)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    print(svd.explained_variance_ratio_)\n",
    "    X = svd.transform(X)\n",
    "    X = pd.DataFrame(X, columns=['nb_{}'.format(i) for i in range(120)])\n",
    "    \n",
    "    X_test = svd.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test, columns=['nb_{}'.format(i) for i in range(120)])\n",
    "    train_data = pd.concat((train_data, X), axis=1)\n",
    "    test_data = pd.concat((test_data, X_test), axis=1)\n",
    "    \n",
    "    del X,X_test\n",
    "    gc.collect()\n",
    "    return train_data,test_data\n",
    "    \n",
    "    \n",
    "train,test=get_feat2()\n",
    "train2=train.copy()\n",
    "test2=test.copy()\n",
    "\n",
    "features = [x for x in train.columns if x not in [\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "label='AdoptionSpeed'\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "#          'max_depth':11,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.4,\n",
    "          'min_split_gain': 0.01,\n",
    "#           'min_child_samples': 30,\n",
    "#           \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "lgb3_train=[r[0] for r in results['train']]\n",
    "lgb3_test=[r[0] for r in results['test']]\n",
    "del train,test\n",
    "gc.collect()\n",
    "t4=time.time()\n",
    "print(\"model4 cost:{} s\".format(t4-t3))\n",
    "def get_feat3():\n",
    "    train_data = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    \n",
    "    # petid列\n",
    "    data = pd.concat([train_data, test_data], axis=0)\n",
    "    del data['AdoptionSpeed']\n",
    "    es = ft.EntitySet(id='data_id')\n",
    "    es = es.entity_from_dataframe(entity_id='PetID', dataframe=data,\n",
    "                                   index='PetID')\n",
    "    \n",
    "    need_deal_columns = ['Age', 'Breed1', 'Breed2', 'Color1', 'Color2', 'Color3', 'Description',\n",
    "           'Dewormed', 'Fee', 'FurLength', 'Gender', 'Health', 'MaturitySize',\n",
    "           'Name', 'PhotoAmt', 'Quantity', 'RescuerID', 'State',\n",
    "           'Sterilized', 'Type', 'Vaccinated', 'VideoAmt']\n",
    "    for i in need_deal_columns:\n",
    "        data_RescuerID = pd.DataFrame()\n",
    "        data_RescuerID[i] = list(data[i].unique())\n",
    "        es = es.entity_from_dataframe(entity_id=i, dataframe=data_RescuerID,\n",
    "                                   index=i)\n",
    "        cr = ft.Relationship( es[i][i],\n",
    "                        es['PetID'][i])\n",
    "        es = es.add_relationship(cr)\n",
    "        \n",
    "    features, feature_names = ft.dfs(entityset=es, target_entity='PetID',\n",
    "                                     max_depth=3,verbose=True)\n",
    "    \n",
    "    features = pd.merge(data[['PetID']], features.reset_index(), on='PetID', how='left')\n",
    "    label_encode = LabelEncoder()\n",
    "    for i in features.columns:\n",
    "        if features[i].dtype ==\"object\":\n",
    "            features[i] = features[i].fillna('未知')\n",
    "            features[i] = list(map(str, features[i]))\n",
    "            features[i] = label_encode.fit_transform(features[i])\n",
    "    features = features[['Breed1.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Description.MODE(PetID.VideoAmt)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Description.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Description.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'State.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'Color2.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Age.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Description.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'RescuerID.MODE(PetID.Health)',\n",
    "     'Color3.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Name)',\n",
    "     'Breed1.MODE(PetID.Type)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Color1)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'State.MODE(PetID.Type)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Health)',\n",
    "     'Color2.MODE(PetID.PhotoAmt)',\n",
    "     'Dewormed.MODE(PetID.RescuerID)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Description)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Color1.NUM_UNIQUE(PetID.State)',\n",
    "     'Color1.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Fee.MODE(PetID.Age)',\n",
    "     'Health.MODE(PetID.Description)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Health)',\n",
    "     'Breed1.MODE(PetID.Breed2)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Color3.MODE(PetID.Type)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Name)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'VideoAmt.MODE(PetID.PhotoAmt)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Name)',\n",
    "     'Health.NUM_UNIQUE(PetID.Age)',\n",
    "     'Age.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Description.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Fee.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Age.MODE(PetID.Color1)',\n",
    "     'State.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Fee)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Name)',\n",
    "     'Vaccinated.MODE(PetID.Dewormed)',\n",
    "     'Color1.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.Type)',\n",
    "     'State.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Health.NUM_UNIQUE(PetID.Breed1)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Color3.MODE(PetID.Breed1)',\n",
    "     'Age.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Color3.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Quantity.MODE(PetID.Color2)',\n",
    "     'Dewormed.MODE(PetID.Type)',\n",
    "     'Color1.MODE(PetID.Gender)',\n",
    "     'Gender.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Breed2.MODE(PetID.Color1)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'VideoAmt.COUNT(PetID)',\n",
    "     'State.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Name)',\n",
    "     'Age.MODE(PetID.FurLength)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'VideoAmt.MODE(PetID.RescuerID)',\n",
    "     'Vaccinated.MODE(PetID.Name)',\n",
    "     'Breed2.MODE(PetID.Gender)',\n",
    "     'MaturitySize.MODE(PetID.Type)',\n",
    "     'Description.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Type.COUNT(PetID)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Health)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'MaturitySize.MODE(PetID.PhotoAmt)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Name)',\n",
    "     'Description.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Description.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Sterilized.MODE(PetID.Vaccinated)',\n",
    "     'Fee.MODE(PetID.Sterilized)',\n",
    "     'Breed2.MODE(PetID.Type)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Name)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Quantity.MODE(PetID.Age)',\n",
    "     'Quantity.MODE(PetID.Dewormed)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Description)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Age)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Gender.MODE(PetID.Name)',\n",
    "     'VideoAmt.MODE(PetID.Vaccinated)',\n",
    "     'Quantity.MODE(PetID.Breed1)',\n",
    "     'Color1.MODE(PetID.Type)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Health)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.State)',\n",
    "     'Breed1.MODE(PetID.Color3)',\n",
    "     'Fee.MODE(PetID.FurLength)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Type)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Health.NUM_UNIQUE(PetID.Breed2)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Color1)',\n",
    "     'State.MODE(PetID.Dewormed)',\n",
    "     'Color1.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Description.NUM_UNIQUE(PetID.Health)',\n",
    "     'Fee.MODE(PetID.Gender)',\n",
    "     'Description.NUM_UNIQUE(PetID.State)',\n",
    "     'Color3.MODE(PetID.Dewormed)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Breed1)',\n",
    "     'PhotoAmt.MODE(PetID.State)',\n",
    "     'Type.NUM_UNIQUE(PetID.Age)',\n",
    "     'Age.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'State.NUM_UNIQUE(PetID.Health)',\n",
    "     'Breed1.MODE(PetID.State)',\n",
    "     'State.MODE(PetID.PhotoAmt)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'Breed1.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Breed2)',\n",
    "     'Description.NUM_UNIQUE(PetID.Type)',\n",
    "     'Vaccinated.MODE(PetID.RescuerID)',\n",
    "     'Health.MODE(PetID.Name)',\n",
    "     'Color3.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Fee.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Name)',\n",
    "     'Health.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'PhotoAmt.MODE(PetID.Breed1)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Color3.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Health.NUM_UNIQUE(PetID.Description)',\n",
    "     'Age.NUM_UNIQUE(PetID.Type)',\n",
    "     'Fee.MODE(PetID.Color1)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Type.NUM_UNIQUE(PetID.Breed2)',\n",
    "     'Breed2.MODE(PetID.Dewormed)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Health.NUM_UNIQUE(PetID.Name)',\n",
    "     'Vaccinated.MODE(PetID.Type)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Description)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'State.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Age.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Type)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Type.NUM_UNIQUE(PetID.Fee)',\n",
    "     'Age.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Color2)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Type.NUM_UNIQUE(PetID.Breed1)',\n",
    "     'Sterilized.MODE(PetID.Name)',\n",
    "     'Description.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Age.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'PhotoAmt.MODE(PetID.Vaccinated)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Age.NUM_UNIQUE(PetID.Gender)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Breed2.MODE(PetID.Color3)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.State)',\n",
    "     'Type.NUM_UNIQUE(PetID.Name)',\n",
    "     'PhotoAmt.MODE(PetID.Age)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Color2.MODE(PetID.Vaccinated)',\n",
    "     'Type.NUM_UNIQUE(PetID.Description)',\n",
    "     'Health.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Health.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Breed2.MODE(PetID.State)',\n",
    "     'MaturitySize.MODE(PetID.Gender)',\n",
    "     'PhotoAmt.MODE(PetID.Gender)',\n",
    "     'Color1.MODE(PetID.PhotoAmt)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'MaturitySize.MODE(PetID.Vaccinated)',\n",
    "     'Breed2.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'VideoAmt.MODE(PetID.Type)',\n",
    "     'Dewormed.MODE(PetID.Vaccinated)',\n",
    "     'FurLength.MODE(PetID.Breed1)',\n",
    "     'Health.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Type.MODE(PetID.RescuerID)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Type.MODE(PetID.Age)',\n",
    "     'Type.MODE(PetID.State)',\n",
    "     'Type.MODE(PetID.Sterilized)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Type.MODE(PetID.Breed2)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Health)',\n",
    "     'Type.MODE(PetID.Breed1)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Type)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Type.MODE(PetID.Color1)',\n",
    "     'Type.MODE(PetID.Health)',\n",
    "     'Type.MODE(PetID.MaturitySize)',\n",
    "     'Type.MODE(PetID.Gender)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.State)',\n",
    "     'Type.MODE(PetID.FurLength)',\n",
    "     'Type.MODE(PetID.Fee)',\n",
    "     'Type.MODE(PetID.VideoAmt)',\n",
    "     'Type.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Type.MODE(PetID.Name)',\n",
    "     'Type.MODE(PetID.Dewormed)',\n",
    "     'Type.MODE(PetID.PhotoAmt)',\n",
    "     'Type.MODE(PetID.Description)',\n",
    "     'Type.MODE(PetID.Color3)',\n",
    "     'Type.MODE(PetID.Quantity)',\n",
    "     'Type.MODE(PetID.Color2)',\n",
    "     'Type.MODE(PetID.Vaccinated)',\n",
    "     'Color1.MODE(PetID.Dewormed)',\n",
    "     'Type.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Type.NUM_UNIQUE(PetID.Color3)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Name)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Health)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Gender)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Fee)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Color3)',\n",
    "     'VideoAmt.MODE(PetID.Sterilized)',\n",
    "     'VideoAmt.MODE(PetID.State)',\n",
    "     'VideoAmt.MODE(PetID.Quantity)',\n",
    "     'VideoAmt.MODE(PetID.MaturitySize)',\n",
    "     'VideoAmt.MODE(PetID.Health)',\n",
    "     'VideoAmt.MODE(PetID.Gender)',\n",
    "     'VideoAmt.MODE(PetID.FurLength)',\n",
    "     'VideoAmt.MODE(PetID.Fee)',\n",
    "     'VideoAmt.MODE(PetID.Dewormed)',\n",
    "     'VideoAmt.MODE(PetID.Color3)',\n",
    "     'VideoAmt.MODE(PetID.Color2)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.State)',\n",
    "     'Breed1.MODE(PetID.Fee)',\n",
    "     'Age.MODE(PetID.Breed2)',\n",
    "     'Age.MODE(PetID.Color3)',\n",
    "     'Age.MODE(PetID.Fee)',\n",
    "     'Age.MODE(PetID.Health)',\n",
    "     'Age.MODE(PetID.MaturitySize)',\n",
    "     'Age.MODE(PetID.Quantity)',\n",
    "     'Age.MODE(PetID.State)',\n",
    "     'Age.MODE(PetID.VideoAmt)',\n",
    "     'Breed1.MODE(PetID.Health)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Breed1.MODE(PetID.Quantity)',\n",
    "     'Breed1.MODE(PetID.VideoAmt)',\n",
    "     'Breed2.MODE(PetID.Fee)',\n",
    "     'Breed2.MODE(PetID.Health)',\n",
    "     'Breed2.MODE(PetID.Quantity)',\n",
    "     'Breed2.MODE(PetID.VideoAmt)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Type)',\n",
    "     'VideoAmt.MODE(PetID.Color1)',\n",
    "     'VideoAmt.MODE(PetID.Breed2)',\n",
    "     'VideoAmt.MODE(PetID.Breed1)',\n",
    "     'Type.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Vaccinated.MODE(PetID.Color3)',\n",
    "     'Vaccinated.MODE(PetID.Color2)',\n",
    "     'Vaccinated.MODE(PetID.Color1)',\n",
    "     'Vaccinated.MODE(PetID.Breed2)',\n",
    "     'Vaccinated.MODE(PetID.Breed1)',\n",
    "     'Vaccinated.MODE(PetID.Age)',\n",
    "     'Type.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Type.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Type.NUM_UNIQUE(PetID.State)',\n",
    "     'Vaccinated.MODE(PetID.Fee)',\n",
    "     'Type.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Type.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'Type.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'Type.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Type.NUM_UNIQUE(PetID.Health)',\n",
    "     'Type.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Type.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Type.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Vaccinated.MODE(PetID.Description)',\n",
    "     'Vaccinated.MODE(PetID.FurLength)',\n",
    "     'VideoAmt.MODE(PetID.Age)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Type)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.State)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Health)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Vaccinated.MODE(PetID.Gender)',\n",
    "     'Vaccinated.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Vaccinated.MODE(PetID.VideoAmt)',\n",
    "     'Vaccinated.MODE(PetID.Sterilized)',\n",
    "     'Vaccinated.MODE(PetID.State)',\n",
    "     'Vaccinated.MODE(PetID.Quantity)',\n",
    "     'Vaccinated.MODE(PetID.PhotoAmt)',\n",
    "     'Vaccinated.MODE(PetID.MaturitySize)',\n",
    "     'Vaccinated.MODE(PetID.Health)',\n",
    "     'VideoAmt.NUM_UNIQUE(PetID.Color2)',\n",
    "     'PhotoAmt.MODE(PetID.Breed2)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'FurLength.MODE(PetID.Age)',\n",
    "     'Fee.MODE(PetID.Health)',\n",
    "     'Fee.MODE(PetID.MaturitySize)',\n",
    "     'Fee.MODE(PetID.Quantity)',\n",
    "     'Fee.MODE(PetID.State)',\n",
    "     'Fee.MODE(PetID.VideoAmt)',\n",
    "     'Fee.NUM_UNIQUE(PetID.Gender)',\n",
    "     'FurLength.MODE(PetID.Breed2)',\n",
    "     'FurLength.MODE(PetID.MaturitySize)',\n",
    "     'FurLength.MODE(PetID.Color1)',\n",
    "     'FurLength.MODE(PetID.Color2)',\n",
    "     'FurLength.MODE(PetID.Color3)',\n",
    "     'FurLength.MODE(PetID.Dewormed)',\n",
    "     'FurLength.MODE(PetID.Fee)',\n",
    "     'FurLength.MODE(PetID.Gender)',\n",
    "     'Fee.MODE(PetID.Dewormed)',\n",
    "     'Fee.MODE(PetID.Color3)',\n",
    "     'Fee.MODE(PetID.Breed2)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Type)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.State)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Health)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Dewormed.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Dewormed.MODE(PetID.VideoAmt)',\n",
    "     'Dewormed.MODE(PetID.Sterilized)',\n",
    "     'Dewormed.MODE(PetID.State)',\n",
    "     'FurLength.MODE(PetID.Health)',\n",
    "     'FurLength.MODE(PetID.PhotoAmt)',\n",
    "     'Dewormed.MODE(PetID.PhotoAmt)',\n",
    "     'Gender.MODE(PetID.Fee)',\n",
    "     'Gender.MODE(PetID.Breed2)',\n",
    "     'Gender.MODE(PetID.Color1)',\n",
    "     'Gender.MODE(PetID.Color2)',\n",
    "     'Gender.MODE(PetID.Color3)',\n",
    "     'Gender.MODE(PetID.Description)',\n",
    "     'Gender.MODE(PetID.Dewormed)',\n",
    "     'Gender.MODE(PetID.FurLength)',\n",
    "     'FurLength.MODE(PetID.Quantity)',\n",
    "     'Gender.MODE(PetID.Health)',\n",
    "     'Gender.MODE(PetID.MaturitySize)',\n",
    "     'Gender.MODE(PetID.PhotoAmt)',\n",
    "     'Gender.MODE(PetID.Quantity)',\n",
    "     'Gender.MODE(PetID.RescuerID)',\n",
    "     'Gender.MODE(PetID.State)',\n",
    "     'Gender.MODE(PetID.Breed1)',\n",
    "     'Gender.MODE(PetID.Age)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Type)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Health)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Gender)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Color3)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Color2)',\n",
    "     'FurLength.NUM_UNIQUE(PetID.Color1)',\n",
    "     'FurLength.MODE(PetID.VideoAmt)',\n",
    "     'FurLength.MODE(PetID.Vaccinated)',\n",
    "     'FurLength.MODE(PetID.Type)',\n",
    "     'FurLength.MODE(PetID.Sterilized)',\n",
    "     'FurLength.MODE(PetID.State)',\n",
    "     'Dewormed.MODE(PetID.Quantity)',\n",
    "     'Dewormed.MODE(PetID.Name)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Color2.MODE(PetID.MaturitySize)',\n",
    "     'Color2.MODE(PetID.Color3)',\n",
    "     'Color2.MODE(PetID.Dewormed)',\n",
    "     'Color2.MODE(PetID.Fee)',\n",
    "     'Color2.MODE(PetID.FurLength)',\n",
    "     'Color2.MODE(PetID.Gender)',\n",
    "     'Color2.MODE(PetID.Health)',\n",
    "     'Color2.MODE(PetID.Quantity)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Color2.MODE(PetID.State)',\n",
    "     'Color2.MODE(PetID.Sterilized)',\n",
    "     'Color2.MODE(PetID.VideoAmt)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Color2.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Color2.MODE(PetID.Breed2)',\n",
    "     'Color2.MODE(PetID.Age)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Type)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Color1.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Health)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Color1.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Color1.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Color1.MODE(PetID.VideoAmt)',\n",
    "     'Color1.MODE(PetID.Sterilized)',\n",
    "     'Color1.MODE(PetID.State)',\n",
    "     'Color1.MODE(PetID.Quantity)',\n",
    "     'Color1.MODE(PetID.MaturitySize)',\n",
    "     'Color1.MODE(PetID.Health)',\n",
    "     'Color1.MODE(PetID.FurLength)',\n",
    "     'Color2.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Type)',\n",
    "     'Dewormed.MODE(PetID.MaturitySize)',\n",
    "     'Dewormed.MODE(PetID.Breed2)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Type)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Color1.MODE(PetID.Color3)',\n",
    "     'Dewormed.MODE(PetID.Age)',\n",
    "     'Dewormed.MODE(PetID.Breed1)',\n",
    "     'Dewormed.MODE(PetID.Color1)',\n",
    "     'Color2.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Dewormed.MODE(PetID.Color3)',\n",
    "     'Dewormed.MODE(PetID.Description)',\n",
    "     'Dewormed.MODE(PetID.Fee)',\n",
    "     'Dewormed.MODE(PetID.FurLength)',\n",
    "     'Dewormed.MODE(PetID.Gender)',\n",
    "     'Dewormed.MODE(PetID.Health)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Gender)',\n",
    "     'Color3.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Color3.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Color3.MODE(PetID.VideoAmt)',\n",
    "     'Color3.MODE(PetID.Vaccinated)',\n",
    "     'Color3.MODE(PetID.Sterilized)',\n",
    "     'Color3.MODE(PetID.State)',\n",
    "     'Color3.MODE(PetID.Quantity)',\n",
    "     'Color3.MODE(PetID.MaturitySize)',\n",
    "     'Color3.MODE(PetID.Health)',\n",
    "     'Color3.MODE(PetID.Gender)',\n",
    "     'Color3.MODE(PetID.FurLength)',\n",
    "     'Color3.MODE(PetID.Fee)',\n",
    "     'Color3.MODE(PetID.Color2)',\n",
    "     'Color3.MODE(PetID.Color1)',\n",
    "     'Color3.MODE(PetID.Breed2)',\n",
    "     'Color3.MODE(PetID.Age)',\n",
    "     'Gender.MODE(PetID.Sterilized)',\n",
    "     'Gender.MODE(PetID.Type)',\n",
    "     'Gender.MODE(PetID.Vaccinated)',\n",
    "     'Quantity.MODE(PetID.VideoAmt)',\n",
    "     'Quantity.MODE(PetID.Gender)',\n",
    "     'Quantity.MODE(PetID.Health)',\n",
    "     'Quantity.MODE(PetID.MaturitySize)',\n",
    "     'Quantity.MODE(PetID.State)',\n",
    "     'Quantity.MODE(PetID.Sterilized)',\n",
    "     'Quantity.MODE(PetID.Vaccinated)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'State.MODE(PetID.Color3)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Type)',\n",
    "     'Quantity.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'RescuerID.NUM_UNIQUE(PetID.State)',\n",
    "     'State.MODE(PetID.Breed2)',\n",
    "     'State.MODE(PetID.Color1)',\n",
    "     'Quantity.MODE(PetID.FurLength)',\n",
    "     'Quantity.MODE(PetID.Fee)',\n",
    "     'Quantity.MODE(PetID.Color1)',\n",
    "     'Quantity.MODE(PetID.Breed2)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Type)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'PhotoAmt.NUM_UNIQUE(PetID.Gender)',\n",
    "     'PhotoAmt.MODE(PetID.VideoAmt)',\n",
    "     'PhotoAmt.MODE(PetID.Sterilized)',\n",
    "     'PhotoAmt.MODE(PetID.Quantity)',\n",
    "     'PhotoAmt.MODE(PetID.MaturitySize)',\n",
    "     'PhotoAmt.MODE(PetID.Health)',\n",
    "     'PhotoAmt.MODE(PetID.FurLength)',\n",
    "     'PhotoAmt.MODE(PetID.Fee)',\n",
    "     'PhotoAmt.MODE(PetID.Dewormed)',\n",
    "     'PhotoAmt.MODE(PetID.Color3)',\n",
    "     'PhotoAmt.MODE(PetID.Color1)',\n",
    "     'State.MODE(PetID.Color2)',\n",
    "     'State.MODE(PetID.Fee)',\n",
    "     'Gender.MODE(PetID.VideoAmt)',\n",
    "     'Sterilized.MODE(PetID.MaturitySize)',\n",
    "     'Sterilized.MODE(PetID.Color3)',\n",
    "     'Sterilized.MODE(PetID.Dewormed)',\n",
    "     'Sterilized.MODE(PetID.Fee)',\n",
    "     'Sterilized.MODE(PetID.FurLength)',\n",
    "     'Sterilized.MODE(PetID.Gender)',\n",
    "     'Sterilized.MODE(PetID.Health)',\n",
    "     'Sterilized.MODE(PetID.PhotoAmt)',\n",
    "     'State.MODE(PetID.FurLength)',\n",
    "     'Sterilized.MODE(PetID.Quantity)',\n",
    "     'Sterilized.MODE(PetID.State)',\n",
    "     'Sterilized.MODE(PetID.Type)',\n",
    "     'Sterilized.MODE(PetID.VideoAmt)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Sterilized.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Sterilized.MODE(PetID.Color2)',\n",
    "     'Sterilized.MODE(PetID.Color1)',\n",
    "     'Sterilized.MODE(PetID.Breed2)',\n",
    "     'Sterilized.MODE(PetID.Breed1)',\n",
    "     'State.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'State.NUM_UNIQUE(PetID.Type)',\n",
    "     'State.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'State.NUM_UNIQUE(PetID.Gender)',\n",
    "     'State.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'State.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'State.NUM_UNIQUE(PetID.Color2)',\n",
    "     'State.MODE(PetID.VideoAmt)',\n",
    "     'State.MODE(PetID.Sterilized)',\n",
    "     'State.MODE(PetID.Quantity)',\n",
    "     'State.MODE(PetID.MaturitySize)',\n",
    "     'State.MODE(PetID.Health)',\n",
    "     'State.MODE(PetID.Gender)',\n",
    "     'Color1.MODE(PetID.Fee)',\n",
    "     'Color1.MODE(PetID.Age)',\n",
    "     'Color1.MODE(PetID.Breed2)',\n",
    "     'Health.MODE(PetID.Quantity)',\n",
    "     'Health.MODE(PetID.Dewormed)',\n",
    "     'Health.MODE(PetID.Fee)',\n",
    "     'Health.MODE(PetID.FurLength)',\n",
    "     'Health.MODE(PetID.Gender)',\n",
    "     'Health.MODE(PetID.MaturitySize)',\n",
    "     'Health.MODE(PetID.PhotoAmt)',\n",
    "     'Health.MODE(PetID.RescuerID)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Health.MODE(PetID.State)',\n",
    "     'Health.MODE(PetID.Sterilized)',\n",
    "     'Health.MODE(PetID.Type)',\n",
    "     'Health.MODE(PetID.Vaccinated)',\n",
    "     'Health.MODE(PetID.VideoAmt)',\n",
    "     'Health.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Health.MODE(PetID.Color3)',\n",
    "     'Health.MODE(PetID.Color2)',\n",
    "     'Health.MODE(PetID.Color1)',\n",
    "     'Health.MODE(PetID.Breed2)',\n",
    "     'Health.MODE(PetID.Breed1)',\n",
    "     'Health.MODE(PetID.Age)',\n",
    "     'Gender.NUM_UNIQUE(PetID.VideoAmt)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Type)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Gender.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Health)',\n",
    "     'Gender.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Color3)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Gender.NUM_UNIQUE(PetID.Color1)',\n",
    "     'Health.NUM_UNIQUE(PetID.Color2)',\n",
    "     'Health.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'Health.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'MaturitySize.MODE(PetID.Health)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Type)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.State)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.PhotoAmt)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Health)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Gender)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.FurLength)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Dewormed)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Color3)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Color2)',\n",
    "     'MaturitySize.NUM_UNIQUE(PetID.Color1)',\n",
    "     'MaturitySize.MODE(PetID.Sterilized)',\n",
    "     'MaturitySize.MODE(PetID.State)',\n",
    "     'MaturitySize.MODE(PetID.Quantity)',\n",
    "     'MaturitySize.MODE(PetID.FurLength)',\n",
    "     'Health.NUM_UNIQUE(PetID.Gender)',\n",
    "     'MaturitySize.MODE(PetID.Fee)',\n",
    "     'MaturitySize.MODE(PetID.Dewormed)',\n",
    "     'MaturitySize.MODE(PetID.Color3)',\n",
    "     'MaturitySize.MODE(PetID.Color2)',\n",
    "     'MaturitySize.MODE(PetID.Color1)',\n",
    "     'MaturitySize.MODE(PetID.Breed2)',\n",
    "     'MaturitySize.MODE(PetID.Age)',\n",
    "     'Health.NUM_UNIQUE(PetID.Vaccinated)',\n",
    "     'Health.NUM_UNIQUE(PetID.Type)',\n",
    "     'Health.NUM_UNIQUE(PetID.Sterilized)',\n",
    "     'Health.NUM_UNIQUE(PetID.State)',\n",
    "     'Health.NUM_UNIQUE(PetID.RescuerID)',\n",
    "     'Health.NUM_UNIQUE(PetID.Quantity)',\n",
    "     'Health.NUM_UNIQUE(PetID.MaturitySize)',\n",
    "     'MaturitySize.MODE(PetID.VideoAmt)']]\n",
    "    \n",
    "    new_columns = []\n",
    "    for i in features.columns:\n",
    "        new_columns.append('featuretools_' + i)\n",
    "    features.columns = new_columns\n",
    "    del data\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data_temp = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data_temp = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    temp_data = pd.concat([train_data_temp, test_data_temp], axis=0)\n",
    "    features['PetID'] = list(temp_data['PetID'])\n",
    "    del train_data_temp,test_data_temp,temp_data\n",
    "    gc.collect()\n",
    "    return features\n",
    "    \n",
    "\n",
    "nurbs=get_feat3()\n",
    "train = pd.merge(train1,nurbs, on='PetID', how='left')\n",
    "test = pd.merge(test1, nurbs, on='PetID', how='left')\n",
    "features = [x for x in train.columns if x not in ['Breed1',\"breed\",\"color\",\"Breed2\",\"State\",\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "\n",
    "label='AdoptionSpeed'\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "#          'max_depth':11,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.4,\n",
    "          'min_split_gain': 0.01,\n",
    "#           'min_child_samples': 30,\n",
    "#           \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "\n",
    "\n",
    "def runCAT(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = Pool(train_X, label=train_y)\n",
    "    d_valid = Pool(test_X, label=test_y)\n",
    "    watchlist = (d_train, d_valid)\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = CatBoostRegressor(iterations=num_rounds, \n",
    "        learning_rate = 0.03,\n",
    "        od_type='Iter',\n",
    "         od_wait=early_stop,\n",
    "        loss_function='RMSE',\n",
    "        eval_metric='RMSE',\n",
    "        bagging_temperature=0.9,                   \n",
    "        random_seed = 2019,\n",
    "        task_type='GPU'\n",
    "                          )\n",
    "    model.fit(d_train,eval_set=d_valid,\n",
    "            use_best_model=True,\n",
    "            verbose=verbose_eval\n",
    "                         )\n",
    "    \n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    log=0#log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "#     pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2)\n",
    "#     pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), 0, coefficients, qwk,log\n",
    "###model 5\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "lgb5_train=[r[0] for r in results['train']]\n",
    "lgb5_test=[r[0] for r in results['test']]\n",
    "t5=time.time()\n",
    "print(\"model5 cost:{} s\".format(t5-t4))\n",
    "\n",
    "train = pd.merge(train2, nurbs, on='PetID', how='left')\n",
    "test = pd.merge(test2, nurbs, on='PetID', how='left')\n",
    "\n",
    "del nurbs\n",
    "gc.collect()\n",
    "###model 6\n",
    "features = [x for x in train.columns if x not in ['label_description',\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "results = run_cv_model(train[features], test[features], train[label], runCAT, params, rmse, 'CAT')\n",
    "\n",
    "cat2_train=[r[0] for r in results['train']]\n",
    "cat2_test=[r[0] for r in results['test']]\n",
    "t6=time.time()\n",
    "print(\"model6 cost:{} s\".format(t6-t5))\n",
    "\n",
    "\n",
    "####model 7\n",
    "features = [x for x in train.columns if x not in [\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "#          'max_depth':11,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.4,\n",
    "          'min_split_gain': 0.01,\n",
    "#           'min_child_samples': 30,\n",
    "#           \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "lgb6_train=[r[0] for r in results['train']]\n",
    "lgb6_test=[r[0] for r in results['test']]\n",
    "del train,test,results\n",
    "gc.collect()\n",
    "t7=time.time()\n",
    "print(\"model7 cost:{} s\".format(t7-t6))\n",
    "\n",
    "def nn1_model(train_img_feat=train_img_feat,test_img_feat=test_img_feat):\n",
    "    train_data = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\n",
    "    \n",
    "    breed=pd.read_csv(\"../input/petfinder-adoption-prediction/breed_labels.csv\")\n",
    "    color = pd.read_csv(\"../input/petfinder-adoption-prediction/color_labels.csv\")\n",
    "    \n",
    "    color_dict = dict(zip(color['ColorID'].values.astype(\"str\"),color['ColorName'].values))\n",
    "    breed_dict = dict(zip(breed['BreedID'].values.astype(\"str\"),breed['BreedName'].values))\n",
    "    \n",
    "    def get_purebreed_feat(df):\n",
    "        if df['Breed2']==0 and df['Breed1']!=307:\n",
    "            return 1\n",
    "        return 0\n",
    "    train_data['purebreed']=train_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    test_data['purebreed']=test_data.apply(lambda x:get_purebreed_feat(x),1)\n",
    "    \n",
    "    train_data['mixed']=train_data['Gender'].apply(lambda x:1 if x==3 else 0)\n",
    "    test_data['mixed']=test_data['Gender'].apply(lambda x:1 if x==3 else 0)\n",
    "    \n",
    "    \n",
    "    train_data['lan_type'] = train_data.Description.map(lambda x:lan_type(x))\n",
    "    train_data['malai_type'] = train_data.Description.map(lambda x:malai_type(x))\n",
    "    \n",
    "    test_data['lan_type'] = test_data.Description.map(lambda x:lan_type(x))\n",
    "    test_data['malai_type'] = test_data.Description.map(lambda x:malai_type(x))\n",
    "    \n",
    "    def name_deal(df):\n",
    "        if \"No Name\" in df:\n",
    "            return np.nan\n",
    "        if df ==\"nan\":\n",
    "            return np.nan\n",
    "        else:\n",
    "            return df\n",
    "    train_data['Name'] = train_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    test_data['Name'] = test_data['Name'].apply(lambda x:name_deal(str(x)),1)\n",
    "    \n",
    "    train_data['Name'],indexer=pd.factorize(train_data['Name'])\n",
    "    test_data['Name'] = indexer.get_indexer(test_data['Name'])\n",
    "    \n",
    "    group = train_data['RescuerID'].values\n",
    "    \n",
    "    rescuer_df=train_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    train_data=pd.merge(train_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    train_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    rescuer_df=test_data.groupby(\"RescuerID\",as_index=False).count()[[\"RescuerID\",\"PetID\"]]\n",
    "    rescuer_df.columns=[\"RescuerID\",\"rescuer_cnt\"]\n",
    "    test_data=pd.merge(test_data,rescuer_df,on=\"RescuerID\",how=\"left\")\n",
    "    test_data.drop(\"RescuerID\",1,inplace=True)\n",
    "    \n",
    "    train_data[\"num_chars\"] = train_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    test_data[\"num_chars\"] = test_data[\"Description\"].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    train_data['Description'] = train_data['Description'].fillna(\"None\")\n",
    "    test_data['Description'] = test_data['Description'].fillna(\"None\")\n",
    "    \n",
    "    train_data[\"Description\"] = train_data[\"Description\"].str.lower()\n",
    "    test_data[\"Description\"] = test_data[\"Description\"].str.lower()\n",
    "    \n",
    "    puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "     '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "     '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "     '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "     '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "    def clean_text(x):\n",
    "        for punct in puncts:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    train_data[\"Description\"] = train_data[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    test_data[\"Description\"] = test_data[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "\n",
    "    def deal_desc(df):\n",
    "        if df['lan_type']==1:\n",
    "            return \"null\"\n",
    "        if df['lan_type']==3:\n",
    "            text=jieba.cut(df['Description'])\n",
    "            text=\" \".join(text)\n",
    "            text=text.replace(\"   \",\" \")\n",
    "            return text\n",
    "        else:\n",
    "            return df['Description']\n",
    "    train_data['Description']=train_data.apply(lambda x:deal_desc(x), 1)\n",
    "    test_data['Description']=test_data.apply(lambda x:deal_desc(x), 1)\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    train_data['doc_sent_mag'] = doc_sent_mag\n",
    "    train_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "    \n",
    "    test_data['doc_sent_mag'] = doc_sent_mag\n",
    "    test_data['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in train_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    print(nl_count)\n",
    "    train_data[ 'vertex_x'] = vertex_xs\n",
    "    train_data['vertex_y'] = vertex_ys\n",
    "    train_data['bounding_confidence'] = bounding_confidences\n",
    "    train_data['bounding_importance'] = bounding_importance_fracs\n",
    "    train_data['dominant_blue'] = dominant_blues\n",
    "    train_data['dominant_green'] = dominant_greens\n",
    "    train_data['dominant_red'] = dominant_reds\n",
    "    train_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    train_data['dominant_score'] = dominant_scores\n",
    "    train_data['label_description'] = label_descriptions\n",
    "    train_data['label_score'] = label_scores\n",
    "    \n",
    "    \n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_descriptions = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    for pet in test_data['PetID'].values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_descriptions.append(label_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_descriptions.append('nothing')\n",
    "            label_scores.append(-1)\n",
    "    \n",
    "    print(nf_count)\n",
    "    test_data[ 'vertex_x'] = vertex_xs\n",
    "    test_data['vertex_y'] = vertex_ys\n",
    "    test_data['bounding_confidence'] = bounding_confidences\n",
    "    test_data['bounding_importance'] = bounding_importance_fracs\n",
    "    test_data['dominant_blue'] = dominant_blues\n",
    "    test_data['dominant_green'] = dominant_greens\n",
    "    test_data['dominant_red'] = dominant_reds\n",
    "    test_data['dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    test_data['dominant_score'] = dominant_scores\n",
    "    test_data['label_description'] = label_descriptions\n",
    "    test_data['label_score'] = label_scores\n",
    "    \n",
    "    del  vertex_xs,vertex_ys,bounding_confidences,bounding_importance_fracs,dominant_blues,dominant_greens,dominant_reds,dominant_pixel_fracs,dominant_scores\n",
    "    del label_descriptions,label_scores,doc_sent_mag,doc_sent_score\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data['label_description'] =train_data['label_description'].astype(np.str)\n",
    "    test_data['label_description'] =test_data['label_description'].astype(np.str)\n",
    "    \n",
    "    def get_text(df):\n",
    "        x=\"\"\n",
    "        if df['Type']==1:\n",
    "            x+=\"dog\"+\" \"\n",
    "        if df['Type']==2:\n",
    "            x+=\"cat\"+\" \"\n",
    "        for i in ['Breed1',\"Breed2\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=breed_dict[str(df[i])]+\" \"\n",
    "        for i in [\"Color1\",\"Color2\",\"Color3\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=color_dict[str(df[i])]+\" \"\n",
    "        x+=df['label_description']+\" \"\n",
    "        x=x+df['Description']\n",
    "        return x\n",
    "    train_data['Description']=train_data.apply(lambda x:get_text(x),1)\n",
    "    test_data['Description']=test_data.apply(lambda x:get_text(x),1)\n",
    "    \n",
    "    text_list = train_data['Description'].values.tolist()\n",
    "    text_list.extend(test_data['Description'].values.tolist())\n",
    "    \n",
    "    documents = text_list\n",
    "    texts = [[word for word in str(document).split(' ') ] for document in documents]\n",
    "    \n",
    "    \n",
    "    \n",
    "    w2v = Word2Vec(texts, size=128, window=7, iter=8, seed=10, workers=2, min_count=3)\n",
    "    w2v.wv.save_word2vec_format('w2v_128.txt')\n",
    "    print(\"w2v model done\")\n",
    "    del w2v\n",
    "    gc.collect()\n",
    "    embed_size = 128 # how big is each word vector\n",
    "    max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
    "    maxlen = 230 # max number of words in a question to use\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    train_X = train_data[\"Description\"].values\n",
    "    test_X = test_data[\"Description\"].values\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "    tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_y = train_data['AdoptionSpeed'].values\n",
    "    \n",
    "    word_index=tokenizer.word_index\n",
    "    features = [x for x in train_data.columns if x not in [\"num_words\",\"num_unique_words\",\"num_stopwords\",\"num_punctuations\",\"mean_word_len\",'label_description',\"Name\", 'PetID', \"Description\", 'AdoptionSpeed']]\n",
    "    \n",
    "    cate_col=['Breed1', 'Breed2', 'Color1', 'Color2', 'Color3', 'State']\n",
    "    onehot_col=['Type','Gender','MaturitySize','MaturitySize','FurLength','Vaccinated',\n",
    "               'Dewormed','Sterilized','Health','purebreed','Color1', 'Color2', 'Color3', ]\n",
    "    num_col=features\n",
    "    \n",
    "   \n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    data = pd.concat([train_data, test_data])\n",
    "    sc.fit(data[num_col])\n",
    "    del data\n",
    "    gc.collect()\n",
    "    train_data[num_col] = sc.transform(train_data[num_col])\n",
    "    test_data[num_col] = sc.transform(test_data[num_col])\n",
    "    train_num_feat = train_data[num_col]\n",
    "    test_num_feat = test_data[num_col]\n",
    "    \n",
    "    train_img_feat.reset_index(inplace=True)\n",
    "    test_img_feat.reset_index(inplace=True)\n",
    "    train_img_feat.columns = [\"PetID\"]+[\"img_\"+str(i) for i in range(train_img_feat.shape[1]-1)]\n",
    "    test_img_feat.columns = [\"PetID\"]+[\"img_\"+str(i) for i in range(train_img_feat.shape[1]-1)]\n",
    "    del train_img_feat['PetID'], test_img_feat['PetID']\n",
    "    train_num_feat = pd.concat([train_num_feat, train_img_feat], axis=1).values\n",
    "    test_num_feat = pd.concat([test_num_feat, test_img_feat], axis=1).values\n",
    "    \n",
    "    embedding_matrix=get_embedding_matrix(word_index)\n",
    "    \n",
    "    def hybrid_model(embedding_matrix):\n",
    "        K.clear_session()\n",
    "        inp_text = Input(shape=(maxlen, ))\n",
    "        emb = Embedding(\n",
    "            input_dim=embedding_matrix.shape[0],\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=False)(inp_text)\n",
    "        x = SpatialDropout1D(rate=0.22)(emb)\n",
    "        x = Bidirectional(CuDNNLSTM(120, return_sequences=True, kernel_initializer=glorot_uniform(seed=123)))(x)  \n",
    "        x1 = Conv1D(filters=100, kernel_size=1, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x2 = Conv1D(filters=90, kernel_size=2, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x3 = Conv1D(filters=30, kernel_size=3, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x4 = Conv1D(filters=10, kernel_size=5, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "    \n",
    "        x1 = GlobalMaxPool1D()(x1)\n",
    "        x2 = GlobalMaxPool1D()(x2)\n",
    "        x3 = GlobalMaxPool1D()(x3)\n",
    "        x4 = GlobalMaxPool1D()(x4)\n",
    "        x5 = AttentionWeightedAverage()(x)\n",
    "        \n",
    "        inp_num = Input(shape=(293, ))\n",
    "        x = concatenate([x1, x2, x3, x4, x5, inp_num])\n",
    "        x = Dense(200, kernel_initializer='glorot_uniform', activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.22)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(200, kernel_initializer='glorot_uniform', activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.22)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        out = Dense(1, kernel_initializer=glorot_uniform(seed=123))(x)\n",
    "    \n",
    "        model = Model(inputs=[inp_text, inp_num], outputs=out)\n",
    "        model.compile(loss='mean_squared_error', optimizer=AdamW(weight_decay=0.02))\n",
    "        return model\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=1017, shuffle=True)\n",
    "    pred_oof=np.zeros((train_X.shape[0], ))\n",
    "    y_test = np.zeros((test_X.shape[0], ))\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    all_coefficients = np.zeros((5, 4))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(train_X, train_y)):\n",
    "        print(\"FOLD | {}/{}\".format(i+1,5))\n",
    "        X_tr, X_vl, X_tr2, X_vl2, y_tr, y_vl = train_X[train_index], train_X[test_index], train_num_feat[\n",
    "            train_index], train_num_feat[test_index], train_y[train_index], train_y[test_index]\n",
    "        #X_tr0 = get_keras_data(X_trall,  cate_col)\n",
    "        #X_tr0['text']=X_tr\n",
    "        #X_tr0['num']=X_tr2\n",
    "        #X_vl0 = get_keras_data(X_vlall,  cate_col)\n",
    "        #X_vl0['text']=X_vl\n",
    "        #X_vl0['num']=X_vl2\n",
    "        filepath=\"weights_best.h5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=0.00001, verbose=2)\n",
    "        earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=2, mode='auto')\n",
    "        callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "        model = hybrid_model(embedding_matrix)\n",
    "        if i == 0:print(model.summary()) \n",
    "        model.fit([X_tr, X_tr2], y_tr, batch_size=128, epochs=20, validation_data=([X_vl, X_vl2], y_vl), verbose=2, callbacks=callbacks,)\n",
    "        model.load_weights(filepath)    \n",
    "        y_pred = np.squeeze(model.predict([X_vl, X_vl2], batch_size=256, verbose=2))\n",
    "        pred_oof[test_index] = y_pred\n",
    "        y_test += np.squeeze(model.predict([test_X, test_num_feat], batch_size=256, verbose=2))/5\n",
    "        optR = OptimizedRounder()\n",
    "        optR.fit(y_pred, y_vl)\n",
    "        len_0 = sum([1 for i in y_vl if i==0])\n",
    "        coefficients = optR.coefficients()\n",
    "        pred_test_y_k = optR.predict(y_pred, coefficients, len_0)\n",
    "        print(\"Valid Counts = \", Counter(y_vl))\n",
    "        print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "        print(\"Coefficients = \", coefficients)\n",
    "        \n",
    "        qwk = cohen_kappa_score(y_vl, pred_test_y_k,weights='quadratic')\n",
    "        cv_score = rmse(y_vl, y_pred)\n",
    "        cv_scores.append(cv_score)\n",
    "        qwk_scores.append(qwk)\n",
    "        all_coefficients[i, :] = coefficients\n",
    "        print( ' cv score {}: RMSE {} QWK {}'.format(i+1, cv_score, qwk))\n",
    "        print(\"##\"*40)\n",
    "        \n",
    "    print('cv mean RMSE score : {}'.format( np.mean(cv_scores)))\n",
    "    print('cv std RMSE score : {}'.format( np.std(cv_scores)))\n",
    "    print('cv mean QWK score : {}'.format( np.mean(qwk_scores)))\n",
    "    print('cv std QWK score : {}'.format( np.std(qwk_scores)))  \n",
    "    \n",
    "    del train_num_feat,test_num_feat,train_X,test_X\n",
    "    gc.collect()\n",
    "    \n",
    "    nn1_train = [r for r in pred_oof]\n",
    "    nn1_test = [r for r in y_test]\n",
    "    \n",
    "    return nn1_train,nn1_test,embedding_matrix,train_img_feat,test_img_feat,train_data,test_data\n",
    "\n",
    "###model 8\n",
    "###nn1\n",
    "nn1_train,nn1_test,embedding_matrix,train_img_feat,test_img_feat,train_data,test_data=nn1_model()\n",
    "t8=time.time()\n",
    "print(\"model8 cost:{} s\".format(t8-t7))\n",
    "####model 9\n",
    "###nn2\n",
    "def nn2_model(train,test,embedding_matrix,train_img_feat,test_img_feat):\n",
    "    \n",
    "    embed_size = 128 # how big is each word vector\n",
    "    max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
    "    maxlen = 220 # max number of words in a question to use\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    train_X = train[\"concat_text\"].values\n",
    "    test_X = test[\"concat_text\"].values\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "    tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "    ## Get the target values\n",
    "    train_y = train['AdoptionSpeed'].values\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    features = [x for x in train.columns if x not in [\"num_words\",\"num_unique_words\",\"num_stopwords\",\"num_punctuations\",\"mean_word_len\",'Breed1',\"breed\",\"color\",\"Breed2\",\"State\",\"concat_text\",'label_description',\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "\n",
    "    num_col = features\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "    data = pd.concat([train, test])\n",
    "    sc.fit(data[num_col])\n",
    "    del data\n",
    "    gc.collect()\n",
    "    train[num_col] = sc.transform(train[num_col])\n",
    "    test[num_col] = sc.transform(test[num_col])\n",
    "    train_num_feat = train[num_col]\n",
    "    test_num_feat = test[num_col]\n",
    "    \n",
    "    train_num_feat = pd.concat([train_num_feat, train_img_feat], axis=1).values\n",
    "    test_num_feat = pd.concat([test_num_feat, test_img_feat], axis=1).values\n",
    "    \n",
    "    \n",
    "    def hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.01):\n",
    "        K.clear_session()\n",
    "        inp_text = Input(shape=(maxlen, ))\n",
    "        emb = Embedding(\n",
    "            input_dim=embedding_matrix.shape[0],\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=False)(inp_text)\n",
    "        x = SpatialDropout1D(rate=sp, seed=1024)(emb)\n",
    "        x = Bidirectional(CuDNNLSTM(128, return_sequences=True, kernel_initializer=glorot_uniform(seed=123), \n",
    "                                    recurrent_initializer=orthogonal(gain=1.0, seed=10000)))(x)\n",
    "        #xx = Bidirectional(CuDNNGRU(60, return_sequences=False, kernel_initializer=glorot_uniform(seed=123)))(x)\n",
    "        #x1 = Conv1D(filters=filters[0], kernel_size=1, kernel_initializer=glorot_uniform(seed=123),\n",
    "        #               padding='same', activation='relu')(x)\n",
    "        c = Conv1D(filters=filters[1], kernel_size=2, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        #x3 = Conv1D(filters=filters[2], kernel_size=3, kernel_initializer=glorot_uniform(seed=123),\n",
    "        #               padding='same', activation='relu')(x)\n",
    "        #x4 = Conv1D(filters=10, kernel_size=5, kernel_initializer=glorot_uniform(seed=123),\n",
    "        #               padding='same', activation='relu')(x)\n",
    "    \n",
    "        #x1 = GlobalMaxPool1D()(x1)\n",
    "        x2 = GlobalMaxPool1D()(c)\n",
    "        x3 = GlobalAvgPool1D()(c)\n",
    "        #x3 = GlobalMaxPool1D()(x3)\n",
    "        #x4 = GlobalMaxPool1D()(x4)\n",
    "        x5 = AttentionWeightedAverage()(x)\n",
    "        \n",
    "        inp_num = Input(shape=(test_num_feat.shape[1], ))\n",
    "        x = concatenate([x2, x3, x5, inp_num])\n",
    "        x = Dense(200, kernel_initializer=glorot_uniform(seed=123), activation=gelu\n",
    "                 )(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.23, seed=1024)(x)\n",
    "        #x = BatchNormalization()(x)\n",
    "        #x = Dense(200, kernel_initializer=glorot_uniform(seed=123), activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        #x = Dropout(0.23, seed=1024)(x)\n",
    "        #x = BatchNormalization()(x)\n",
    "        out = Dense(1, kernel_initializer=glorot_uniform(seed=123))(x)\n",
    "    \n",
    "        model = Model(inputs=[inp_text, inp_num], outputs=out)\n",
    "        model.compile(loss='mean_squared_error', optimizer=AdamW(weight_decay=weight_decay))\n",
    "        #model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "        return model\n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=1017, shuffle=True)\n",
    "    pred_oof=np.zeros((train_X.shape[0], ))\n",
    "    y_test = np.zeros((test_X.shape[0], ))\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    all_coefficients = np.zeros((5, 4))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(train_X, train_y)):\n",
    "        print(\"FOLD | {}/{}\".format(i+1,5))\n",
    "        X_tr, X_vl, X_tr2, X_vl2, y_tr, y_vl = train_X[train_index], train_X[test_index], train_num_feat[\n",
    "            train_index], train_num_feat[test_index], train_y[train_index], train_y[test_index]\n",
    "        \n",
    "        filepath=\"weights_best.h5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=0.00001, verbose=2)\n",
    "        earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=2, mode='auto')\n",
    "        callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "        if i == 0:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        elif i == 1:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        elif i == 2:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        elif i == 3:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        elif i == 4:\n",
    "            model = hybrid_model(embedding_matrix=embedding_matrix, sp=0.22, filters=[96, 100, 30], weight_decay=0.04)\n",
    "        if i == 0:print(model.summary()) \n",
    "        model.fit([X_tr, X_tr2], y_tr, batch_size=128, epochs=20, validation_data=([X_vl, X_vl2], y_vl), verbose=2, callbacks=callbacks,)\n",
    "        model.load_weights(filepath)    \n",
    "        y_pred = np.squeeze(model.predict([X_vl, X_vl2], batch_size=256, verbose=2))\n",
    "        pred_oof[test_index] = y_pred\n",
    "        y_test += np.squeeze(model.predict([test_X, test_num_feat], batch_size=256, verbose=2))/5\n",
    "        optR = OptimizedRounder()\n",
    "        optR.fit(y_pred, y_vl)\n",
    "        len_0 = sum([1 for i in y_vl if i==0])\n",
    "        coefficients = optR.coefficients()\n",
    "        pred_test_y_k = optR.predict(y_pred, coefficients, len_0)\n",
    "        print(\"Valid Counts = \", Counter(y_vl))\n",
    "        print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "        print(\"Coefficients = \", coefficients)\n",
    "        qwk = cohen_kappa_score(y_vl, pred_test_y_k,weights='quadratic')\n",
    "        cv_score = rmse(y_vl, y_pred)\n",
    "        cv_scores.append(cv_score)\n",
    "        qwk_scores.append(qwk)\n",
    "        all_coefficients[i, :] = coefficients\n",
    "        print( ' cv score {}: RMSE {} QWK {}'.format(i+1, cv_score, qwk))\n",
    "        print(\"##\"*40)\n",
    "        \n",
    "    print('cv mean RMSE score : {}'.format( np.mean(cv_scores)))\n",
    "    print('cv std RMSE score : {}'.format( np.std(cv_scores)))\n",
    "    print('cv mean QWK score : {}'.format( np.mean(qwk_scores)))\n",
    "    print('cv std QWK score : {}'.format( np.std(qwk_scores)))\n",
    "    \n",
    "    nn2_train = [r for r in pred_oof]\n",
    "    nn2_test = [r for r in y_test]\n",
    "    del train_X,test_X\n",
    "    gc.collect()\n",
    "    \n",
    "    return nn2_train,nn2_test,train_num_feat,test_num_feat\n",
    "nn2_train,nn2_test,train_num_feat,test_num_feat=nn2_model(train1,test1,embedding_matrix,train_img_feat,test_img_feat)\n",
    "del train_img_feat,test_img_feat\n",
    "gc.collect()\n",
    "t9=time.time()\n",
    "print(\"model9 cost:{} s\".format(t9-t8))\n",
    "####model 10\n",
    "###nn3\n",
    "\n",
    "def nn3_model(train,test,embedding_matrix,train_num_feat,test_num_feat):\n",
    "    maxlen = 200\n",
    "    max_features = None \n",
    "    train_X = train[\"concat_text\"].values\n",
    "    test_X = test[\"concat_text\"].values\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "    tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "    \n",
    "     ## Get the target values\n",
    "    train_y = train['AdoptionSpeed'].values\n",
    "    \n",
    "    def hybrid_model(embedding_matrix):\n",
    "        K.clear_session()\n",
    "        inp_text = Input(shape=(maxlen, ))\n",
    "        emb = Embedding(\n",
    "            input_dim=embedding_matrix.shape[0],\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=False)(inp_text)\n",
    "        x = SpatialDropout1D(rate=0.22)(emb)\n",
    "        x = Bidirectional(CuDNNLSTM(120, return_sequences=True, kernel_initializer=glorot_uniform(seed=123)))(x)  \n",
    "        x1 = Conv1D(filters=96, kernel_size=1, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x2 = Conv1D(filters=90, kernel_size=2, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x3 = Conv1D(filters=30, kernel_size=3, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "        x4 = Conv1D(filters=10, kernel_size=5, kernel_initializer=glorot_uniform(seed=123),\n",
    "                       padding='same', activation='relu')(x)\n",
    "    \n",
    "        x1 = GlobalMaxPool1D()(x1)\n",
    "        x2 = GlobalMaxPool1D()(x2)\n",
    "        x3 = GlobalMaxPool1D()(x3)\n",
    "        x4 = GlobalMaxPool1D()(x4)\n",
    "        x5 = AttentionWeightedAverage()(x)\n",
    "        \n",
    "        inp_num = Input(shape=(test_num_feat.shape[1], ))\n",
    "        x = concatenate([x1, x2, x3, x5, inp_num])\n",
    "        x = Dense(200, kernel_initializer='glorot_uniform', activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.22)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(200, kernel_initializer='glorot_uniform', activation=gelu)(x)\n",
    "        #x = PReLU()(x)\n",
    "        x = Dropout(0.22)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        out = Dense(5, activation=\"softmax\",kernel_initializer=glorot_uniform(seed=123))(x)\n",
    "    \n",
    "        model = Model(inputs=[inp_text, inp_num], outputs=out)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=AdamW(weight_decay=0.02))\n",
    "        return model\n",
    "    \n",
    "   \n",
    "    kfold = StratifiedKFold(n_splits=5, random_state=1017, shuffle=True)\n",
    "    pred_oof=np.zeros((train_X.shape[0], ))\n",
    "    y_test = np.zeros((test_X.shape[0],))\n",
    "    cv_scores = []\n",
    "    qwk_scores = []\n",
    "    all_coefficients = np.zeros((5, 4))\n",
    "    \n",
    "    y_label= to_categorical(train['AdoptionSpeed'])\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(train_X, train_y)):\n",
    "        print(\"FOLD | {}/{}\".format(i+1,5))\n",
    "        X_tr, X_vl, X_tr2, X_vl2, y_tr, y_vl = train_X[train_index], train_X[test_index], train_num_feat[\n",
    "            train_index], train_num_feat[test_index], y_label[train_index], y_label[test_index]\n",
    "        \n",
    "        filepath=\"weights_best.h5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=0.00001, verbose=2)\n",
    "        earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=2, mode='auto')\n",
    "        callbacks = [checkpoint, reduce_lr, earlystopping]\n",
    "        model = hybrid_model(embedding_matrix)\n",
    "        if i == 0:print(model.summary()) \n",
    "        model.fit([X_tr, X_tr2], y_tr, batch_size=128, epochs=20, validation_data=([X_vl, X_vl2], y_vl), verbose=2, callbacks=callbacks,)\n",
    "        model.load_weights(filepath)  \n",
    "        class_list=[0,1,2,3,4]\n",
    "        y_pred = np.squeeze(model.predict([X_vl, X_vl2], batch_size=256, verbose=2))\n",
    "        y_pred=np.array([sum(y_pred[ix]*class_list) for\n",
    "                                   ix in range(len(y_pred[:,0]))]) \n",
    "        pred_oof[test_index] = y_pred\n",
    "        test_temp = np.squeeze(model.predict([test_X, test_num_feat], batch_size=256, verbose=2))\n",
    "    \n",
    "        \n",
    "        test_temp=np.array([sum(test_temp[ix]*class_list) for\n",
    "                                   ix in range(len(test_temp[:,0]))]) \n",
    "        y_test+=np.squeeze(test_temp)/5\n",
    "        y_vl= train_y[test_index]\n",
    "        optR = OptimizedRounder()\n",
    "        optR.fit(y_pred, y_vl)\n",
    "        len_0 = sum([1 for i in y_vl if i==0])\n",
    "        coefficients = optR.coefficients()\n",
    "        pred_test_y_k = optR.predict(y_pred, coefficients, len_0)\n",
    "        print(\"Valid Counts = \", Counter(y_vl))\n",
    "        print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "        print(\"Coefficients = \", coefficients)\n",
    "        qwk = cohen_kappa_score(y_vl, pred_test_y_k,weights='quadratic')\n",
    "        cv_score = rmse(y_vl, y_pred)\n",
    "        cv_scores.append(cv_score)\n",
    "        qwk_scores.append(qwk)\n",
    "        all_coefficients[i, :] = coefficients\n",
    "        print( ' cv score {}: RMSE {} QWK {}'.format(i+1, cv_score, qwk))\n",
    "        print(\"##\"*40)\n",
    "        \n",
    "    print('cv mean RMSE score : {}'.format( np.mean(cv_scores)))\n",
    "    print('cv std RMSE score : {}'.format( np.std(cv_scores)))\n",
    "    print('cv mean QWK score : {}'.format( np.mean(qwk_scores)))\n",
    "    print('cv std QWK score : {}'.format( np.std(qwk_scores)))\n",
    "    \n",
    "    nn3_train = [r for r in pred_oof]\n",
    "    nn3_test = [r for r in y_test]\n",
    "    \n",
    "    del train_X,test_X\n",
    "    gc.collect()\n",
    "    return  nn3_train,nn3_test \n",
    "nn3_train,nn3_test=nn3_model(train1,test1,embedding_matrix,train_num_feat,test_num_feat)\n",
    "del embedding_matrix,train_num_feat,test_num_feat\n",
    "gc.collect()\n",
    "t10=time.time()\n",
    "print(\"model10 cost:{} s\".format(t10-t9))\n",
    "\n",
    "######weak model###############################################\n",
    "data = pd.concat([train_data,test_data])\n",
    "data.index=range(len(data))\n",
    "data_id=data['PetID'].values\n",
    "\n",
    "del train_desc,test_desc\n",
    "gc.collect()  \n",
    "\n",
    "cols = [x for x in train1.columns if x not in ['Breed1',\"breed\",\"color\",\"Breed2\",\"State\",'label_description',\"lan_type\",\"malai_type\",\"Type\",\"concat_text\",\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "\n",
    "train1[cols]=train1[cols].fillna(0)\n",
    "test1[cols]=test1[cols].fillna(0)\n",
    "############################ 切分数据集 ##########################\n",
    "print('开始进行一些前期处理')\n",
    "train_feature = train1[cols].values\n",
    "test_feature = test1[cols].values\n",
    "    # 五则交叉验证\n",
    "n_folds = 5\n",
    "print('处理完毕')\n",
    "df_stack3 = pd.DataFrame()\n",
    "df_stack3['PetID']=data['PetID']\n",
    "for label in [\"AdoptionSpeed\"]:\n",
    "    score = train_data[label]\n",
    "    \n",
    "   \n",
    "    ########################### SGD(随机梯度下降) ################################\n",
    "    # print('sgd stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     sgd = SGDRegressor(random_state=1017,)\n",
    "    #     sgd.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = sgd.predict(train_feature[va])\n",
    "    #     score_te = sgd.predict(test_feature)\n",
    "    #     print('得分' + str(mean_squared_error(score[va], sgd.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0]+= score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "#     df_stack3['tfidf_sgd_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "\n",
    "\n",
    "    ########################### pac(PassiveAggressiveClassifier) ################################\n",
    "    # print('PAC stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     pac = PassiveAggressiveRegressor(random_state=1017)\n",
    "    #     pac.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = pac.predict(train_feature[va])\n",
    "    #     score_te = pac.predict(test_feature)\n",
    "      \n",
    "    #     print('得分' + str(mean_squared_error(score[va], pac.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0] += score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "#     df_stack3['tfidf_pac_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ########################### FTRL ################################\n",
    "    print('MultinomialNB stacking')\n",
    "    stack_train = np.zeros((len(train_data),1))\n",
    "    stack_test = np.zeros((len(test_data),1))\n",
    "    score_va = 0\n",
    "\n",
    "    sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "        clf = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=train_feature.shape[1], iters=50, inv_link=\"identity\", threads=1)\n",
    "        clf.fit(train_feature[tr], score[tr])\n",
    "        score_va = clf.predict(train_feature[va])\n",
    "        score_te = clf.predict(test_feature)\n",
    "      \n",
    "        print('得分' + str(mean_squared_error(score[va], clf.predict(train_feature[va]))))\n",
    "        stack_train[va,0] = score_va\n",
    "        stack_test[:,0] += score_te\n",
    "    stack_test /= n_folds\n",
    "    stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "    df_stack3['tfidf_FTRL_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "    \n",
    "    ########################### ridge(RidgeClassfiy) ################################\n",
    "    print('RidgeClassfiy stacking')\n",
    "    stack_train = np.zeros((len(train_data),1))\n",
    "    stack_test = np.zeros((len(test_data),1))\n",
    "    score_va = 0\n",
    "\n",
    "    sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "        ridge = Ridge(solver=\"sag\", fit_intercept=True, random_state=42, alpha=30) \n",
    "        ridge.fit(train_feature[tr], score[tr])\n",
    "        score_va = ridge.predict(train_feature[va])\n",
    "        score_te = ridge.predict(test_feature)\n",
    "       \n",
    "        print('得分' + str(mean_squared_error(score[va], ridge.predict(train_feature[va]))))\n",
    "        stack_train[va,0] = score_va\n",
    "        stack_test[:,0] += score_te\n",
    "    stack_test /= n_folds\n",
    "    stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "    df_stack3['tfidf_ridge_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "    \n",
    "    ############################ Linersvc(LinerSVC) ################################\n",
    "    # print('LinerSVC stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     lsvc = LinearSVR(random_state=1017)\n",
    "    #     lsvc.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = lsvc.predict(train_feature[va])\n",
    "    #     score_te = lsvc.predict(test_feature)\n",
    "       \n",
    "    #     print('得分' + str(mean_squared_error(score[va], lsvc.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0] += score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "#     df_stack3['tfidf_lsvc_classfiy_{}'.format(\"feat1\")] = stack[:,0]\n",
    "del stack,stack_train, stack_test,train_feature,test_feature\n",
    "gc.collect()   \n",
    "# df_stack.to_csv('graph_tfidf_classfiy.csv', index=None, encoding='utf8')\n",
    "print('tfidf特征已保存\\n')\n",
    "del train1,test1\n",
    "gc.collect()\n",
    "\n",
    "cols = [x for x in train2.columns if x not in ['label_description',\"is_group\",\"Name\",'PetID',\"Description\",'AdoptionSpeed']]\n",
    "train2[cols]=train2[cols].fillna(0)\n",
    "test2[cols]=test2[cols].fillna(0)\n",
    "############################ 切分数据集 ##########################\n",
    "print('开始进行一些前期处理')\n",
    "train_feature = train2[cols].values\n",
    "test_feature = test2[cols].values\n",
    "    # 五则交叉验证\n",
    "n_folds = 5\n",
    "print('处理完毕')\n",
    "df_stack4 = pd.DataFrame()\n",
    "df_stack4['PetID']=data['PetID']\n",
    "for label in [\"AdoptionSpeed\"]:\n",
    "    score = train_data[label]\n",
    "    \n",
    "   \n",
    "    ########################### SGD(随机梯度下降) ################################\n",
    "    # print('sgd stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     sgd = SGDRegressor(random_state=1017,)\n",
    "    #     sgd.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = sgd.predict(train_feature[va])\n",
    "    #     score_te = sgd.predict(test_feature)\n",
    "    #     print('得分' + str(mean_squared_error(score[va], sgd.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0]+= score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "#     df_stack4['tfidf_sgd_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "\n",
    "\n",
    "    ########################### pac(PassiveAggressiveClassifier) ################################\n",
    "    # print('PAC stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     pac = PassiveAggressiveRegressor(random_state=1017)\n",
    "    #     pac.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = pac.predict(train_feature[va])\n",
    "    #     score_te = pac.predict(test_feature)\n",
    "      \n",
    "    #     print('得分' + str(mean_squared_error(score[va], pac.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0] += score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "#     df_stack4['tfidf_pac_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ########################### FTRL ################################\n",
    "    print('MultinomialNB stacking')\n",
    "    stack_train = np.zeros((len(train_data),1))\n",
    "    stack_test = np.zeros((len(test_data),1))\n",
    "    score_va = 0\n",
    "\n",
    "    sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "        clf = FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=train_feature.shape[1], iters=50, inv_link=\"identity\", threads=1)\n",
    "        clf.fit(train_feature[tr], score[tr])\n",
    "        score_va = clf.predict(train_feature[va])\n",
    "        score_te = clf.predict(test_feature)\n",
    "      \n",
    "        print('得分' + str(mean_squared_error(score[va], clf.predict(train_feature[va]))))\n",
    "        stack_train[va,0] = score_va\n",
    "        stack_test[:,0] += score_te\n",
    "    stack_test /= n_folds\n",
    "    stack = np.vstack([stack_train, stack_test])\n",
    "    \n",
    "    df_stack4['tfidf_FTRL_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "    \n",
    "    ########################### ridge(RidgeClassfiy) ################################\n",
    "    print('RidgeClassfiy stacking')\n",
    "    stack_train = np.zeros((len(train_data),1))\n",
    "    stack_test = np.zeros((len(test_data),1))\n",
    "    score_va = 0\n",
    "\n",
    "    sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "        print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "        ridge = Ridge(solver=\"sag\", fit_intercept=True, random_state=42, alpha=30) \n",
    "        ridge.fit(train_feature[tr], score[tr])\n",
    "        score_va = ridge.predict(train_feature[va])\n",
    "        score_te = ridge.predict(test_feature)\n",
    "       \n",
    "        print('得分' + str(mean_squared_error(score[va], ridge.predict(train_feature[va]))))\n",
    "        stack_train[va,0] = score_va\n",
    "        stack_test[:,0] += score_te\n",
    "    stack_test /= n_folds\n",
    "    stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "    df_stack4['tfidf_ridge_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "    \n",
    "    ############################ Linersvc(LinerSVC) ################################\n",
    "    # print('LinerSVC stacking')\n",
    "    # stack_train = np.zeros((len(train_data),1))\n",
    "    # stack_test = np.zeros((len(test_data),1))\n",
    "    # score_va = 0\n",
    "\n",
    "    # sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "    # for i, (tr, va) in enumerate(sk.split(train_feature, score)):\n",
    "    #     print('stack:%d/%d' % ((i + 1), n_folds))\n",
    "    #     lsvc = LinearSVR(random_state=1017)\n",
    "    #     lsvc.fit(train_feature[tr], score[tr])\n",
    "    #     score_va = lsvc.predict(train_feature[va])\n",
    "    #     score_te = lsvc.predict(test_feature)\n",
    "       \n",
    "    #     print('得分' + str(mean_squared_error(score[va], lsvc.predict(train_feature[va]))))\n",
    "    #     stack_train[va,0] = score_va\n",
    "    #     stack_test[:,0] += score_te\n",
    "    # stack_test /= n_folds\n",
    "    # stack = np.vstack([stack_train, stack_test])\n",
    "\n",
    "#     df_stack4['tfidf_lsvc_classfiy_{}'.format(\"feat2\")] = stack[:,0]\n",
    "del stack,stack_train, stack_test,train_feature,test_feature\n",
    "gc.collect()   \n",
    "# df_stack.to_csv('graph_tfidf_classfiy.csv', index=None, encoding='utf8')\n",
    "print('tfidf特征已保存\\n')\n",
    "\n",
    "# wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n",
    "#                                                               \"hash_ngrams_weights\": [1.5, 1.0],\n",
    "#                                                               \"hash_size\": 2 ** 29,\n",
    "#                                                               \"norm\": None,\n",
    "#                                                               \"tf\": 'binary',\n",
    "#                                                               \"idf\": None,\n",
    "#                                                               }), procs=8)\n",
    "# x_train = wb.fit_transform(train2['Description'])\n",
    "# x_test = wb.transform(test2['Description'])\n",
    "\n",
    "################\n",
    "# Remove features with document frequency <=100\n",
    "#@eg:1)\n",
    "#mask = np.array(np.clip(sparse_merge.getnnz(axis=0) - 100, 0, 1), dtype=bool)\n",
    "#sparse_merge = sparse_merge[:, mask]\n",
    "#@eg:2)\n",
    "#mask\n",
    "#mask = np.where(X_param1_train.getnnz(axis=0) > 3)[0]\n",
    "#X_param1_train = X_param1_train[:, mask]\n",
    "################\n",
    "# mask = np.array(np.clip(x_train.getnnz(axis=0) - 3, 0, 1), dtype=bool)\n",
    "# x_train=x_train[:,mask]\n",
    "# x_test=x_test[:,mask]\n",
    "\n",
    "# sk = StratifiedKFold( n_splits=5, random_state=1017,shuffle=True)\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# print(\"FTRL...\")\n",
    "# n_fold=5\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf=  FTRL(alpha=0.01, beta=0.1, L1=0.00001, L2=1.0, D=x_train.shape[1], iters=50, inv_link=\"identity\", threads=1)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['FTRL_pred']=stack_train\n",
    "# test_data['FTRL_pred']=stack_test\n",
    "# print(\"Ridge...\")\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf= Ridge(solver=\"sag\", fit_intercept=True, random_state=42, alpha=30)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['ridge_pred']=stack_train\n",
    "# test_data['ridge_pred']=stack_test\n",
    "\n",
    "# print(\"LinearSVR...\")\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf= LinearSVR(random_state=1017)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['svr_pred']=stack_train\n",
    "# test_data['svr_pred']=stack_test\n",
    "\n",
    "# print(\"pac...\")\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf= PassiveAggressiveRegressor(random_state=1017)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['pac_pred']=stack_train\n",
    "# test_data['pac_pred']=stack_test\n",
    "\n",
    "# print(\"sgd...\")\n",
    "# stack_train = np.zeros((len(train_data)))\n",
    "# stack_test = np.zeros((len(test_data)))\n",
    "# for i, (tr, va) in enumerate(sk.split(x_train, train_data['AdoptionSpeed'])):\n",
    "#     print(\"FOLD | {}/{}\".format(i+1,n_fold))\n",
    "#     clf= SGDRegressor(random_state=1017,)\n",
    "#     clf.fit(x_train[tr],train_data['AdoptionSpeed'][tr])\n",
    "#     score_va = clf.predict(x_train[va])\n",
    "#     score_te = clf.predict(x_test)\n",
    "#     stack_train[va] = score_va\n",
    "#     stack_test += score_te\n",
    "# stack_test /= n_fold\n",
    "\n",
    "# train_data['sgd_pred']=stack_train\n",
    "# test_data['sgd_pred']=stack_test\n",
    "\n",
    "# del x_train,x_test,stack_train,stack_test\n",
    "del train2,test2\n",
    "gc.collect()\n",
    "\n",
    "##3sigma \n",
    "def feat4_model():\n",
    "\n",
    "    def read_sent_json(pet_id,data_source='train'):\n",
    "    \n",
    "        doc_sent_mag = []\n",
    "        doc_sent_score = []\n",
    "        nf_count = 0\n",
    "        for pet in pet_id:\n",
    "            try:\n",
    "                with open(data_source + '_sentiment/' + pet + '.json', 'r') as f:\n",
    "                    sentiment = json.load(f)\n",
    "                doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "                doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "            except FileNotFoundError:\n",
    "                nf_count += 1\n",
    "                doc_sent_mag.append(-np.nan)\n",
    "                doc_sent_score.append(-np.nan)\n",
    "        return doc_sent_mag,doc_sent_score\n",
    "    \n",
    "    def read_meta_json(data_source='train'):\n",
    "    \n",
    "        vertex_xs = []\n",
    "        vertex_ys = []\n",
    "        bounding_confidences = []\n",
    "        bounding_importance_fracs = []\n",
    "        dominant_blues = []\n",
    "        dominant_greens = []\n",
    "        dominant_reds = []\n",
    "        dominant_pixel_fracs = []\n",
    "        dominant_scores = []\n",
    "        label_descriptions = []\n",
    "        label_all_descriptions = []\n",
    "        label_scores = []\n",
    "        file_name = []\n",
    "        nl_count = 0\n",
    "        file_list = os.listdir(data_source+'_metadata/')\n",
    "        for file_i in file_list:\n",
    "            file_name.append(file_i) \n",
    "            with open(data_source + '_metadata/' + file_i, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "            vertex_xs.append(vertex_x)\n",
    "            vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "            vertex_ys.append(vertex_y)\n",
    "            bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "            bounding_confidences.append(bounding_confidence)\n",
    "            bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "            bounding_importance_fracs.append(bounding_importance_frac)\n",
    "            dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('blue',-1)\n",
    "            dominant_blues.append(dominant_blue)\n",
    "            dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('green',-1)\n",
    "            dominant_greens.append(dominant_green)\n",
    "            dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color'].get('red',-1)\n",
    "            dominant_reds.append(dominant_red)\n",
    "            dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "            dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "            dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "            dominant_scores.append(dominant_score)\n",
    "            if data.get('labelAnnotations'):\n",
    "                label_description = data['labelAnnotations'][0]['description']\n",
    "                label_all_description = ' '.join([i['description'].replace(' ','_') for i in data['labelAnnotations']])\n",
    "                label_descriptions.append(label_description)\n",
    "                label_all_descriptions.append(label_all_description)\n",
    "                label_score = data['labelAnnotations'][0]['score']\n",
    "                label_scores.append(label_score)\n",
    "            else:\n",
    "                nl_count += 1\n",
    "                label_all_descriptions.append('nothing')\n",
    "                label_descriptions.append('nothing')\n",
    "                label_scores.append(-1)\n",
    "        out_df = pd.DataFrame({'file_name':file_name,\n",
    "                               'vertex_xs':vertex_xs,\n",
    "                               'vertex_ys':vertex_ys,\n",
    "                               'bounding_confidences':bounding_confidences,\n",
    "                               'bounding_importance_fracs':bounding_importance_fracs,\n",
    "                               'dominant_blues':dominant_blues,\n",
    "                               'dominant_greens':dominant_greens,\n",
    "                               'dominant_reds':dominant_reds,\n",
    "                               'dominant_pixel_fracs':dominant_pixel_fracs,\n",
    "                               'dominant_scores':dominant_scores,\n",
    "                               'label_descriptions':label_descriptions,\n",
    "                               'label_all_descriptions':label_all_descriptions,\n",
    "                               'label_scores':label_scores\n",
    "                              })\n",
    "        out_df['PetID'] = out_df['file_name'].str.split('-').str[0]\n",
    "        out_df['PicID'] =  out_df['file_name'].str.split('[-|.]').str[1]\n",
    "        \n",
    "        return out_df\n",
    "    \n",
    "    breed = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')\n",
    "    state = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')\n",
    "    color = pd.read_csv('../input/petfinder-adoption-prediction/color_labels.csv')\n",
    "    \n",
    "    data_tr = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "    data_te = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n",
    "    \n",
    "    def deal_breed(df):\n",
    "        if df['Breed1']==df['Breed2']:\n",
    "            df['Breed2']=0\n",
    "        if df['Breed1']!=307 & df['Breed2']==307:\n",
    "            temp=df[\"Breed1\"]\n",
    "            df['Breed1']=df['Breed2']\n",
    "            df['Breed2']=temp\n",
    "        return df\n",
    "    data_tr=data_tr.apply(lambda x:deal_breed(x),1)\n",
    "    data_te=data_te.apply(lambda x:deal_breed(x),1)\n",
    "    \n",
    "    breed_dict = dict(zip(breed['BreedID'].values.astype(\"str\"),breed['BreedName'].values))\n",
    "    def get_breed(df):\n",
    "        \n",
    "        x=\"\"\n",
    "        for i in [\"Breed1\",\"Breed2\"]:\n",
    "            if df[i]==0:\n",
    "                continue\n",
    "            x+=breed_dict[str(df[i])]+\" \"\n",
    "        return x\n",
    "    data_tr['breed']=data_tr.apply(lambda x:get_breed(x),1)\n",
    "    data_te['breed']=data_te.apply(lambda x:get_breed(x),1)\n",
    "    \n",
    "    data_tr['Breed1_str']=data_tr['Breed1'].astype(\"str\")\n",
    "    data_tr['Breed1_str']=data_tr['Breed1'].map(breed_dict)\n",
    "    data_tr['Breed1_str']=data_tr['Breed1'].replace(np.nan,\"null\")\n",
    "    \n",
    "    data_te['Breed1_str']=data_te['Breed1'].astype(\"str\")\n",
    "    data_te['Breed1_str']=data_te['Breed1'].map(breed_dict)\n",
    "    data_te['Breed1_str']=data_te['Breed1'].replace(np.nan,\"null\")\n",
    "    \n",
    "    mean_encoder = MeanEncoder( categorical_features=['Breed1_str', 'breed'],target_type ='regression')\n",
    "    \n",
    "    data_tr = mean_encoder.fit_transform(data_tr, data_tr['AdoptionSpeed'])\n",
    "    data_te = mean_encoder.transform(data_te)\n",
    "    \n",
    "    breed_encode_tr = data_tr[['Breed1_str_pred','breed_pred']]\n",
    "    breed_encode_te = data_te[['Breed1_str_pred','breed_pred']]\n",
    "    \n",
    "    data_tr['doc_sent_mag'],data_tr['doc_sent_score'] = read_sent_json(data_tr['PetID'],data_source='../input/petfinder-adoption-prediction/train')\n",
    "    data_te['doc_sent_mag'],data_te['doc_sent_score'] = read_sent_json(data_te['PetID'],data_source='../input/petfinder-adoption-prediction/test')\n",
    "    \n",
    "    meta_tr = read_meta_json(data_source='../input/petfinder-adoption-prediction/train')\n",
    "    meta_te = read_meta_json(data_source='../input/petfinder-adoption-prediction/test')\n",
    "    \n",
    "    num_cols_0 = ['Age','MaturitySize','FurLength','Health','Quantity','Fee','VideoAmt','PhotoAmt']\n",
    "    sent_cols_0 = ['doc_sent_mag','doc_sent_score']\n",
    "    meta_cols_0 = ['vertex_xs', 'vertex_ys', 'bounding_confidences',\n",
    "           'bounding_importance_fracs', 'dominant_blues', 'dominant_greens',\n",
    "           'dominant_reds', 'dominant_pixel_fracs', 'dominant_scores','label_scores'] #'label_descriptions'\n",
    "    cat_cols_0 = ['Type','Gender','Vaccinated','Dewormed','Sterilized']\n",
    "    join_cols_0 = ['Breed1','Breed2','Color1','Color2','Color3','State']\n",
    "    \n",
    "    n_tr = data_tr.shape[0]\n",
    "    n_te = data_te.shape[0]\n",
    "    \n",
    "    def data_process(data,meta_data,data_source='train'):\n",
    "        fea_data = data[['PetID']+num_cols_0+sent_cols_0+cat_cols_0].copy()\n",
    "        \n",
    "        meta_1 = meta_data.query('PicID ==\"1\"')\n",
    "        temp = data[['PetID']].merge(meta_1,how='left',on='PetID')[meta_cols_0].fillna(-1)\n",
    "        fea_data = pd.concat([fea_data,temp],axis=1)[['PetID']+num_cols_0+sent_cols_0+meta_cols_0+cat_cols_0]\n",
    "        \n",
    "    #     temp = meta_1.pivot(index='PetID',columns='label_descriptions',values='label_scores')\n",
    "    #     temp = meta_data.loc[meta_data['PicID'].isin(['1']),:].groupby(['PetID','label_descriptions'])['label_scores'].max().reset_index()\n",
    "    #     temp = temp.pivot(index='PetID',columns='label_descriptions',values='label_scores')\n",
    "    #     temp.columns = [i.replace(' ','_') + '_scores' for i in temp.columns]\n",
    "    #     fea_data = fea_data.merge(temp,how='left',on='PetID')\n",
    "        \n",
    "        \n",
    "        for i in ['Age','MaturitySize','FurLength','Health',\n",
    "                  'Fee','VideoAmt','PhotoAmt',\n",
    "                  'Breed2','Color2','Color3']:\n",
    "            fea_data[i+'_is_0'] = data[i] == 0\n",
    "        for i in ['Fee','VideoAmt','PhotoAmt']:\n",
    "            fea_data[i+'_avg'] = data[i] / data['Quantity']\n",
    "        \n",
    "        dummy_fea = pd.get_dummies(data[cat_cols_0],columns=cat_cols_0)\n",
    "        fea_data = pd.concat([fea_data,dummy_fea],axis=1)\n",
    "        fea_data['color_cnt'] = (data[['Color1','Color2','Color3']] != 0 ).sum(axis=1)\n",
    "        fea_data['no_name'] = data['Name'].isnull() | data['Name'].str.contains('No Name')\n",
    "        \n",
    "        # Description\n",
    "        temp = data['Description']\n",
    "        fea_data['desc_ch'] = temp.str.count(u'[\\u4e00-\\u9fa5]')\n",
    "        fea_data['desc_count'] = temp.str.count(u'[\\u4e00-\\u9fa5]') + temp.replace(u'[\\u4e00-\\u9fa5]','').str.count(' ')\n",
    "        fea_data['desc_ch_rate'] = fea_data['desc_ch'] / fea_data['desc_count']\n",
    "        \n",
    "        # RescuerID\n",
    "        fea_data['rescuer_cnt'] = data.groupby('RescuerID')['PetID'].transform('count')\n",
    "        \n",
    "        temp = data.groupby(['RescuerID','Type'])['PetID'].count().rename('rescuer_type_cnt').reset_index()\n",
    "        temp = temp.pivot(index='RescuerID',columns='Type',values='rescuer_type_cnt').fillna(0)\n",
    "        temp.columns = ['rescuer_type1_cnt','rescuer_type2_cnt']\n",
    "        temp = data[['RescuerID']].merge(temp,how='left',on='RescuerID')\n",
    "        fea_data[['rescuer_type1_cnt','rescuer_type2_cnt']] = temp[['rescuer_type1_cnt','rescuer_type2_cnt']] \n",
    "        fea_data['rescuer_type1_rate']= fea_data['rescuer_type1_cnt'] / fea_data['rescuer_cnt']\n",
    "        \n",
    "    # #     temp = data.groupby(['RescuerID','State'])['PetID'].count() / data_tr.groupby(['State'])['PetID'].count()\n",
    "    # #     temp = temp.rename('Rescuer_State_rate').reset_index().drop('State',axis=1)\n",
    "    # #     fea_data['Rescuer_State_rate']= data_tr[['RescuerID']].merge(temp,how='left',on='RescuerID')['Rescuer_State_rate']\n",
    "        \n",
    "        # Breed\n",
    "        temp = data_tr.groupby(['Breed1'])['PetID'].count().rename('Breed1_cnt').reset_index()\n",
    "        temp = data[['Breed1']].merge(temp,how='left',on='Breed1')   \n",
    "        fea_data['Breed1_cnt'] = temp['Breed1_cnt']\n",
    "        \n",
    "        \n",
    "        data_tr['Fee_avg'] = data_tr['Fee'] / data_tr['Quantity']\n",
    "        temp = data_tr.groupby(['Breed1'])['Fee_avg'].mean().rename('Breed1_Fee_avg').reset_index()\n",
    "        temp = data[['Breed1']].merge(temp,how='left',on='Breed1') \n",
    "        fea_data['Breed1_Fee_avg'] = temp['Breed1_Fee_avg']\n",
    "        fea_data['Breed1_Fee_avg_diff'] = fea_data['Fee_avg'] - fea_data['Breed1_Fee_avg']\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # State\n",
    "        \n",
    "        temp = data_tr.groupby(['State','Type'])['PetID'].count().rename('state_type_cnt').reset_index()\n",
    "        temp = temp.pivot(index='State',columns='Type',values='state_type_cnt').fillna(0)\n",
    "        # temp.columns = ['state_type1_cnt','state_type2_cnt']\n",
    "        temp.columns = ['state_type1_cnt_rank','state_type2_cnt_rank']\n",
    "        temp['state_type1_cnt_rank'] = temp['state_type1_cnt_rank'].rank()\n",
    "        temp['state_type2_cnt_rank'] = temp['state_type2_cnt_rank'].rank()\n",
    "        temp = data[['State']].merge(temp,how='left',on='State')\n",
    "        fea_data[['state_type1_cnt_rank','state_type2_cnt_rank']] = temp[['state_type1_cnt_rank','state_type2_cnt_rank']] \n",
    "        \n",
    "        \n",
    "        \n",
    "        if data_source == 'train':\n",
    "            label = data['AdoptionSpeed'].values\n",
    "        else:\n",
    "            label = None\n",
    "        return fea_data,label\n",
    "    \n",
    "    fea_tr,label_tr = data_process(data_tr,meta_tr,data_source='train')\n",
    "    fea_te,_        = data_process(data_te,meta_te,data_source='test')\n",
    "    \n",
    "    base_fea_cols = [i for i in fea_te.columns if i in fea_tr.columns]\n",
    "    fea_tr = fea_tr[base_fea_cols]\n",
    "    fea_te = fea_te[base_fea_cols]\n",
    "    \n",
    "    print('base_fea_size_tr:',fea_tr.shape)\n",
    "    print('base_fea_size_te:',fea_te.shape)\n",
    "    \n",
    "    def data_process_lr(data,meta_data,data_source='train'):\n",
    "        fea_data = data[['PetID']+num_cols_0+sent_cols_0].copy()\n",
    "        \n",
    "        meta_1 = meta_data.query('PicID ==\"1\"')\n",
    "        temp = data[['PetID']].merge(meta_1,how='left',on='PetID')[meta_cols_0].fillna(0)\n",
    "        fea_data = pd.concat([fea_data,temp],axis=1)[['PetID']+num_cols_0+sent_cols_0+meta_cols_0]\n",
    "        \n",
    "        for i in ['Fee','VideoAmt','PhotoAmt']:\n",
    "            fea_data[i+'_avg'] = data[i] / data['Quantity']\n",
    "        \n",
    "    \n",
    "        fea_data['color_cnt'] = (data[['Color1','Color2','Color3']] != 0 ).sum(axis=1)\n",
    "        \n",
    "        # Description\n",
    "        temp = data['Description']\n",
    "        fea_data['desc_count'] = temp.str.count(u'[\\u4e00-\\u9fa5]') + temp.replace(u'[\\u4e00-\\u9fa5]','').str.count(' ')\n",
    "        \n",
    "        # RescuerID\n",
    "        fea_data['rescuer_cnt'] = data.groupby('RescuerID')['PetID'].transform('count')\n",
    "        \n",
    "        temp = data.groupby(['RescuerID','Type'])['PetID'].count().rename('rescuer_type_cnt').reset_index()\n",
    "        temp = temp.pivot(index='RescuerID',columns='Type',values='rescuer_type_cnt').fillna(0)\n",
    "        temp.columns = ['rescuer_type1_cnt','rescuer_type2_cnt']\n",
    "        temp = data[['RescuerID']].merge(temp,how='left',on='RescuerID')\n",
    "        fea_data[['rescuer_type1_cnt','rescuer_type2_cnt']] = temp[['rescuer_type1_cnt','rescuer_type2_cnt']] \n",
    "        fea_data['rescuer_type1_rate']= fea_data['rescuer_type1_cnt'] / fea_data['rescuer_cnt']\n",
    "        \n",
    "      \n",
    "        # Breed\n",
    "        temp = data_tr.groupby(['Breed1'])['PetID'].count().rename('Breed1_cnt').reset_index()\n",
    "        temp = data[['Breed1']].merge(temp,how='left',on='Breed1')   \n",
    "        fea_data['Breed1_cnt'] = temp['Breed1_cnt']\n",
    "        \n",
    "        \n",
    "        data_tr['Fee_avg'] = data_tr['Fee'] / data_tr['Quantity']\n",
    "        temp = data_tr.groupby(['Breed1'])['Fee_avg'].mean().rename('Breed1_Fee_avg').reset_index()\n",
    "        temp = data[['Breed1']].merge(temp,how='left',on='Breed1') \n",
    "        fea_data['Breed1_Fee_avg'] = temp['Breed1_Fee_avg']\n",
    "        fea_data['Breed1_Fee_avg_diff'] = fea_data['Fee_avg'] - fea_data['Breed1_Fee_avg']\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # State\n",
    "        \n",
    "        temp = data_tr.groupby(['State','Type'])['PetID'].count().rename('state_type_cnt').reset_index()\n",
    "        temp = temp.pivot(index='State',columns='Type',values='state_type_cnt').fillna(0)\n",
    "        # temp.columns = ['state_type1_cnt','state_type2_cnt']\n",
    "        temp.columns = ['state_type1_cnt_rank','state_type2_cnt_rank']\n",
    "        temp['state_type1_cnt_rank'] = temp['state_type1_cnt_rank'].rank()\n",
    "        temp['state_type2_cnt_rank'] = temp['state_type2_cnt_rank'].rank()\n",
    "        temp = data[['State']].merge(temp,how='left',on='State')\n",
    "        fea_data[['state_type1_cnt_rank','state_type2_cnt_rank']] = temp[['state_type1_cnt_rank','state_type2_cnt_rank']] \n",
    "        \n",
    "        dummy_fea = pd.get_dummies(data[cat_cols_0],columns=cat_cols_0)\n",
    "        fea_data = pd.concat([fea_data,dummy_fea],axis=1)   \n",
    "        fea_data['no_name'] = (data['Name'].isnull() | data['Name'].str.contains('No Name')).fillna(1).astype(int)\n",
    "        \n",
    "        \n",
    "        if data_source == 'train':\n",
    "            label = data['AdoptionSpeed'].values\n",
    "        else:\n",
    "            label = None\n",
    "        return fea_data,label\n",
    "    \n",
    "    lr_fea_tr,label_tr = data_process_lr(data_tr,meta_tr,data_source='train')\n",
    "    lr_fea_te,_        = data_process_lr(data_te,meta_te,data_source='test')\n",
    "    \n",
    "    lr_base_fea_cols = [i for i in lr_fea_te.columns if i in lr_fea_tr.columns]\n",
    "    lr_fea_tr = lr_fea_tr[lr_base_fea_cols]\n",
    "    lr_fea_te = lr_fea_te[lr_base_fea_cols]\n",
    "    replace_cols_0 = [\n",
    "     'bounding_importance_fracs',\n",
    "     'dominant_blues',\n",
    "     'dominant_greens',\n",
    "     'dominant_reds',\n",
    "    'label_scores',]\n",
    "    \n",
    "    \n",
    "    lr_fea_tr[replace_cols_0] = lr_fea_tr[replace_cols_0].replace(-1,0)\n",
    "    lr_fea_te[replace_cols_0] = lr_fea_te[replace_cols_0].replace(-1,0)\n",
    "    # fillna log\n",
    "    for i in lr_base_fea_cols[1:35]:\n",
    "        if i in ['doc_sent_mag','doc_sent_score','Breed1_Fee_avg_diff']:\n",
    "            lr_fea_tr[i] = lr_fea_tr[i].fillna(0)\n",
    "            lr_fea_te[i] = lr_fea_te[i].fillna(0)\n",
    "        else:\n",
    "            lr_fea_tr[i] = np.log(lr_fea_tr[i].fillna(0)+1e-5)\n",
    "            lr_fea_te[i] = np.log(lr_fea_te[i].fillna(0)+1e-5)\n",
    "    # standarscaler\n",
    "    scaler = StandardScaler()\n",
    "    # scaler = MinMaxScaler()\n",
    "    lr_fea_tr[lr_base_fea_cols[1:35]] = scaler.fit_transform(lr_fea_tr[lr_base_fea_cols[1:35]])\n",
    "    lr_fea_te[lr_base_fea_cols[1:35]] = scaler.transform(lr_fea_te[lr_base_fea_cols[1:35]])\n",
    "    \n",
    "    \n",
    "    ##############img_feat###############################\n",
    "    train_df = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "    pet_ids = train_df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    \n",
    "    def img_model():\n",
    "        K.clear_session()\n",
    "        inp = Input((img_size, img_size, 3))\n",
    "        x = DenseNet121(\n",
    "                include_top=False, \n",
    "                weights=\"../input/keras-pretrain-model-weights/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\", \n",
    "                input_shape=(img_size, img_size, 3))(inp)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Lambda(lambda x: K.expand_dims(x, axis = -1))(x)\n",
    "        x = AveragePooling1D()(x)\n",
    "        out = Lambda(lambda x: x[:, :, 0])(x)\n",
    "    \n",
    "        model = Model(inp, out)\n",
    "        return model\n",
    "    \n",
    "    extract_model = img_model()\n",
    "    img_features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = extract_model.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            img_features[pet_id] = batch_preds[i]\n",
    "    img_tr = pd.DataFrame.from_dict(img_features, orient='index')\n",
    "    del img_features\n",
    "    gc.collect()\n",
    "    \n",
    "    pca1 = PCA(n_components=30,random_state=42)\n",
    "    image_pca_all_tr = pca1.fit_transform(img_tr)\n",
    "    pca2 = PCA(n_components=80,random_state=42)\n",
    "    \n",
    "    image_pca_lr_tr = pca2.fit_transform(img_tr)\n",
    "    del img_tr\n",
    "    gc.collect()\n",
    "    \n",
    "    test_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n",
    "    pet_ids = test_df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    img_features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = extract_model.predict(batch_images)\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            img_features[pet_id] = batch_preds[i]\n",
    "            \n",
    "    img_te = pd.DataFrame.from_dict(img_features, orient='index')\n",
    "    del img_features,train_df,test_df,pet_ids,extract_model\n",
    "    gc.collect()\n",
    "    image_pca_all_te = pca1.transform(img_te)\n",
    "    image_pca_lr_te = pca2.transform(img_te)\n",
    "    del img_te,pca1,pca2\n",
    "    gc.collect()\n",
    "    \n",
    "    #####################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    desc_vec = TfidfVectorizer(ngram_range=(1,4),\n",
    "                               min_df=3, max_df=0.9, \n",
    "                               strip_accents='unicode', \n",
    "                               use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "    \n",
    "    \n",
    "    desc_tfidf_all_tr = desc_vec.fit_transform(data_tr['Description'].fillna('null').tolist())\n",
    "    desc_tfidf_all_te = desc_vec.transform(data_te['Description'].fillna('null').tolist())\n",
    "    \n",
    "    # tfidf select by chi2 top 10k\n",
    "    tfidf_select = chi2(desc_tfidf_all_tr,label_tr)\n",
    "    tfidf_select_index = (-tfidf_select[0]).argsort()[:10000]\n",
    "    \n",
    "    desc_tfidf_tr = desc_tfidf_all_tr[:,tfidf_select_index]\n",
    "    desc_tfidf_te = desc_tfidf_all_te[:,tfidf_select_index]\n",
    "    \n",
    "    # SVD\n",
    "    svd = TruncatedSVD(n_components=200,random_state=42)\n",
    "    tfidf_svd_tr = svd.fit_transform(desc_tfidf_all_tr)\n",
    "    tfidf_svd_te = svd.transform(desc_tfidf_all_te)\n",
    "    \n",
    "    tfidf_select = chi2(desc_tfidf_all_tr,label_tr)\n",
    "    tfidf_select_index = (-tfidf_select[0]).argsort()[:50000]\n",
    "    \n",
    "    lr_desc_tfidf_tr = desc_tfidf_all_tr[:,tfidf_select_index]\n",
    "    lr_desc_tfidf_te = desc_tfidf_all_te[:,tfidf_select_index]\n",
    "    \n",
    "    join_data_all = pd.concat([data_tr[join_cols_0],data_te[join_cols_0]],axis=0)\n",
    "    # join_data_all = pd.concat([data_tr[cat_cols_0+join_cols_0],data_te[cat_cols_0+join_cols_0]],axis=0)\n",
    "    # for i in cat_cols_0:\n",
    "    #     join_data_all[i] = join_data_all[i].astype(str)+'_'+i\n",
    "    for i in join_cols_0:\n",
    "        if i != 'State':\n",
    "            join_data_all[i] = join_data_all[i].astype(str)+'_'+i[:-1]\n",
    "        else:\n",
    "            join_data_all[i] = join_data_all[i].astype(str)+'_'+i\n",
    "    \n",
    "    join_data_all = join_data_all.apply(lambda x: ' '.join(x), axis=1).tolist()\n",
    "    \n",
    "    # tfidf\n",
    "    join_vec = TfidfVectorizer(ngram_range=(1,1),\n",
    "                               min_df=3, max_df=0.9, \n",
    "                               strip_accents='unicode', \n",
    "                               use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "    # join_all = join_vec.fit_transform(join_data_all)\n",
    "    join_tr = join_vec.fit_transform(join_data_all[:n_tr])\n",
    "    join_te = join_vec.transform(join_data_all[n_tr:])\n",
    "    \n",
    "    # SVD\n",
    "    svd = TruncatedSVD(n_components=50,random_state=42)\n",
    "    # svd_join = svd.fit_transform(join_all)\n",
    "    \n",
    "    join_svd_tr = svd.fit_transform(join_tr)\n",
    "    join_svd_te = svd.transform(join_te)\n",
    "    \n",
    "    meta_desc_all = meta_tr[['PetID','label_all_descriptions']].append(meta_te[['PetID','label_all_descriptions']])\n",
    "    \n",
    "    meta_desc_all = meta_desc_all.groupby('PetID')['label_all_descriptions']\\\n",
    "                    .apply(lambda x: ' '.join(x)).to_frame().reset_index()\n",
    "    \n",
    "    meta_desc_all = data_tr[['PetID']].append(data_te[['PetID']]).merge(meta_desc_all,how='left',on='PetID')['label_all_descriptions']\n",
    "    \n",
    "    meta_desc_vec = TfidfVectorizer(ngram_range=(1,1),\n",
    "                               min_df=10, max_df=0.9, \n",
    "                               strip_accents='unicode', \n",
    "                               use_idf=1,smooth_idf=1, sublinear_tf=1)\n",
    "    # meta_desc_tfidf = meta_desc_vec.fit_transform(meta_desc_all.fillna('nan'))\n",
    "    # meta_desc_tfidf_tr = meta_desc_tfidf[:n_tr,:]\n",
    "    # meta_desc_tfidf_te = meta_desc_tfidf[n_tr:,:]\n",
    "    meta_desc_tfidf_tr = meta_desc_vec.fit_transform(meta_desc_all[:n_tr].fillna('nan'))\n",
    "    meta_desc_tfidf_te = meta_desc_vec.transform(meta_desc_all[n_tr:].fillna('nan'))\n",
    "     \n",
    "    del meta_desc_all,join_data_all\n",
    "    gc.collect()\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    gpf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    lr_tr_fea_all = hstack([lr_fea_tr.values[:,1:].astype(float),lr_desc_tfidf_tr,join_tr,meta_desc_tfidf_tr,image_pca_lr_tr]).tocsr()\n",
    "    lr_te_fea_all = hstack([lr_fea_te.values[:,1:].astype(float),lr_desc_tfidf_te,join_te,meta_desc_tfidf_te,image_pca_lr_te]).tocsr()\n",
    "    del lr_desc_tfidf_tr,lr_desc_tfidf_te,lr_fea_tr,lr_fea_te,image_pca_lr_tr,image_pca_lr_te\n",
    "    gc.collect()\n",
    "    lr_train_predictions = np.zeros((n_tr, 1))\n",
    "    lr_test_predictions = np.zeros((n_te, 1))\n",
    "    lr_train = np.zeros((n_tr, ))\n",
    "    lr_test = np.zeros((n_te, ))\n",
    "    qwk = []\n",
    "    rmse_list = []\n",
    "    for k,(tr_idx,val_idx) in enumerate(gpf.split(data_tr,label_tr,data_tr['RescuerID'])):\n",
    "    # for k,(tr_idx,val_idx) in enumerate(skf.split(data_tr,label_tr)):\n",
    "        if k >= 0:\n",
    "            print('fold_' + str(k) + ' ...')\n",
    "            \n",
    "            fold_tr_fea,fold_tr_label = lr_tr_fea_all[tr_idx,:],label_tr[tr_idx]\n",
    "            fold_val_fea,fold_val_label = lr_tr_fea_all[val_idx,:],label_tr[val_idx]\n",
    "            \n",
    "            for i in [2]:\n",
    "                ridge_model = Ridge(alpha=i,random_state=42)\n",
    "                ridge_model.fit(fold_tr_fea,fold_tr_label)\n",
    "    \n",
    "                pred_val  = ridge_model.predict(fold_val_fea)\n",
    "    \n",
    "                optR = OptimizedRounder()\n",
    "                optR.fit(pred_val, fold_val_label)\n",
    "                valid_p = optR.predict(pred_val, optR.coefficients(),384).astype(int)  \n",
    "                qwk_i = cohen_kappa_score(fold_val_label, valid_p,weights='quadratic')\n",
    "                rmse_i = rmse(fold_val_label,pred_val)\n",
    "                qwk.append(qwk_i)\n",
    "                rmse_list.append(rmse_i)\n",
    "                print('alpha:',i)\n",
    "                print('qwk:',qwk_i)\n",
    "                print(optR.coefficients())\n",
    "                print('train_rmse:',rmse(fold_tr_label,ridge_model.predict(fold_tr_fea)))\n",
    "                print('val_rmse:',rmse_i)\n",
    "                print('----'*5)\n",
    "                \n",
    "                lr_train_predictions[val_idx] = pred_val.reshape(-1,1)\n",
    "                lr_train[val_idx] = np.squeeze(pred_val)\n",
    "                test_preds = ridge_model.predict(lr_te_fea_all)\n",
    "                lr_test_predictions +=test_preds.reshape(-1,1)\n",
    "                lr_test += np.squeeze(test_preds)/5\n",
    "    lr_test_predictions =lr_test_predictions/5\n",
    "    print(qwk,np.mean(qwk),np.std(qwk))\n",
    "    print(rmse_list,np.mean(rmse_list),np.std(rmse_list))\n",
    "    del lr_tr_fea_all,lr_te_fea_all\n",
    "    gc.collect()\n",
    "    params = {'application': 'regression',\n",
    "              'boosting': 'gbdt',\n",
    "              'metric': 'rmse',\n",
    "              'num_leaves': 70,\n",
    "              'max_depth': 8,\n",
    "              'learning_rate': 0.01,\n",
    "              'bagging_fraction': 0.85,\n",
    "              'feature_fraction': 0.8,\n",
    "              'min_split_gain': 0.02,\n",
    "              'min_child_samples': 150,\n",
    "              'min_child_weight': 0.2,\n",
    "              'lambda_l2': 0.05,\n",
    "              'verbosity': -1,\n",
    "              'seed':24}\n",
    "    \n",
    "    tr_fea_all = hstack([fea_tr.values[:,1:].astype(float),desc_tfidf_tr,tfidf_svd_tr,join_tr,join_svd_tr,meta_desc_tfidf_tr,image_pca_all_tr,lr_train_predictions,breed_encode_tr]).tocsr()\n",
    "    te_fea_all = hstack([fea_te.values[:,1:].astype(float),desc_tfidf_te,tfidf_svd_te,join_te,join_svd_te,meta_desc_tfidf_te,image_pca_all_te,lr_test_predictions,breed_encode_te]).tocsr()\n",
    "    del fea_tr,fea_te,desc_tfidf_tr,tfidf_svd_tr,join_tr,join_svd_tr,meta_desc_tfidf_tr,image_pca_all_tr,desc_tfidf_te,tfidf_svd_te,join_te,join_svd_te,meta_desc_tfidf_te,image_pca_all_te,breed_encode_tr,breed_encode_te\n",
    "    gc.collect()\n",
    "    qwk = []\n",
    "    rmse_list = []\n",
    "    # fea_imp = []\n",
    "    train_predictions = np.zeros((n_tr, ))\n",
    "    test_predictions = np.zeros((n_te, ))\n",
    "    \n",
    "    for k,(tr_idx,val_idx) in enumerate(skf.split(data_tr,label_tr)):\n",
    "    # for k,(tr_idx,val_idx) in enumerate(gpf.split(data_tr,label_tr,data_tr['RescuerID'])):\n",
    "        if k >= 0:\n",
    "            print('fold_' + str(k) + ' ...')\n",
    "            \n",
    "            \n",
    "            fold_tr_fea,fold_tr_label = tr_fea_all[tr_idx,:],label_tr[tr_idx]\n",
    "            fold_val_fea,fold_val_label = tr_fea_all[val_idx,:],label_tr[val_idx]\n",
    "            \n",
    "            d_fold_tr = lgb.Dataset(fold_tr_fea, label=fold_tr_label)\n",
    "            d_fold_val = lgb.Dataset(fold_val_fea, label=fold_val_label)\n",
    "            watchlist = [d_fold_tr, d_fold_val]\n",
    "            num_rounds = 10000\n",
    "            verbose_eval = 100\n",
    "            early_stop = 100\n",
    "            model = lgb.train(params,\n",
    "                              train_set=d_fold_tr,\n",
    "                              num_boost_round=num_rounds,\n",
    "                              valid_sets=watchlist,\n",
    "                              verbose_eval=verbose_eval,\n",
    "                              early_stopping_rounds=early_stop)    \n",
    "            pred_val = model.predict(fold_val_fea, num_iteration=model.best_iteration)\n",
    "            optR = OptimizedRounder()\n",
    "            optR.fit(pred_val, fold_val_label)\n",
    "            valid_p = optR.predict(pred_val, optR.coefficients(),384).astype(int)     \n",
    "            qwk_i = cohen_kappa_score(fold_val_label, valid_p,weights='quadratic')\n",
    "            rmse_i = model.best_score['valid_1']['rmse']\n",
    "            qwk.append(qwk_i)\n",
    "            rmse_list.append(rmse_i)\n",
    "            print('qwk:',qwk_i)\n",
    "            print('rmse:',rmse_i)\n",
    "            print('----'*5)\n",
    "            \n",
    "            train_predictions[val_idx] = np.squeeze(pred_val)\n",
    "            test_preds = model.predict(te_fea_all, num_iteration=model.best_iteration)\n",
    "            test_predictions += np.squeeze(test_preds)/5\n",
    "    lgb7_test = [r for r in test_predictions]\n",
    "    \n",
    "    lgb7_train = [r for r in train_predictions]\n",
    "    \n",
    "    \n",
    "    del tr_fea_all,te_fea_all \n",
    "    del data_tr,meta_tr,data_te,meta_te\n",
    "    gc.collect()\n",
    "    \n",
    "    return lr_train,lr_test,lgb7_train,lgb7_test\n",
    "lr_train,lr_test,lgb7_train,lgb7_test = feat4_model()\n",
    "t11=time.time()\n",
    "print(\"model11 cost:{} s\".format(t11-t10))\n",
    "\n",
    "import psutil\n",
    "info = psutil.virtual_memory()\n",
    "print(\"memery used rate:\",info.percent)\n",
    "\n",
    "vv=pd.DataFrame(index=range(len(train_data)))\n",
    "vv['PetID']=train_data['PetID']\n",
    "vv['lgb1']=lgb1_train\n",
    "vv['lgb2']=lgb2_train\n",
    "vv['lgb3']=lgb3_train\n",
    "vv['lr']=lr_train\n",
    "vv['lgb5']=lgb5_train\n",
    "vv['lgb6']=lgb6_train\n",
    "vv['lgb7']=lgb7_train\n",
    "vv['cat1']=cat1_train\n",
    "vv['cat2']=cat2_train\n",
    "vv['nn1']=nn1_train\n",
    "vv['nn2']=nn2_train\n",
    "vv['nn3']=nn3_train\n",
    "# vv['ridge_pred']=train_data['ridge_pred']\n",
    "# vv['FTRL_pred']=train_data['FTRL_pred']\n",
    "# vv['svr_pred']=train_data['svr_pred']\n",
    "# vv['pac_pred']=train_data['pac_pred']\n",
    "# vv['sgd_pred']=train_data['sgd_pred']\n",
    "# vv['breed1_pred']=breed_encode_tr['Breed1_str_pred']\n",
    "# vv['breed_pred']=breed_encode_tr['breed_pred']\n",
    "# vv = pd.merge(vv,df_stack1,on='PetID',how=\"left\")\n",
    "# vv = pd.merge(vv,df_stack2,on='PetID',how=\"left\")\n",
    "vv = pd.merge(vv,df_stack3,on='PetID',how=\"left\")\n",
    "vv = pd.merge(vv,df_stack4,on='PetID',how=\"left\")\n",
    "vv['AdoptionSpeed']=train_data['AdoptionSpeed']\n",
    "\n",
    "col = [x for x in vv.columns if x not in ['PetID','AdoptionSpeed']]\n",
    "print(vv[col].corr())\n",
    "\n",
    "sub=pd.DataFrame(index=range(len(test_data)))\n",
    "sub['PetID']=test_data['PetID']\n",
    "sub['lgb1']=lgb1_test\n",
    "sub['lgb2']=lgb2_test\n",
    "sub['lgb3']=lgb3_test\n",
    "sub['lr']=lr_test\n",
    "sub['lgb5']=lgb5_test\n",
    "sub['lgb6']=lgb6_test\n",
    "sub['lgb7']=lgb7_test\n",
    "sub['cat1']=cat1_test\n",
    "sub['cat2']=cat2_test\n",
    "sub['nn1']=nn1_test\n",
    "sub['nn2']=nn2_test\n",
    "sub['nn3']=nn3_test\n",
    "# sub['ridge_pred']=test_data['ridge_pred']\n",
    "# sub['FTRL_pred']=test_data['FTRL_pred']\n",
    "# sub['svr_pred']=test_data['svr_pred']\n",
    "# sub['pac_pred']=test_data['pac_pred']\n",
    "# sub['sgd_pred']=test_data['sgd_pred']\n",
    "# sub['breed1_pred']=breed_encode_te['Breed1_str_pred']\n",
    "# sub['breed_pred']=breed_encode_te['breed_pred']\n",
    "# sub = pd.merge(sub,df_stack1,on='PetID',how=\"left\")\n",
    "# sub = pd.merge(sub,df_stack2,on='PetID',how=\"left\")\n",
    "sub = pd.merge(sub,df_stack3,on='PetID',how=\"left\")\n",
    "sub = pd.merge(sub,df_stack4,on='PetID',how=\"left\")\n",
    "\n",
    "print(sub[col].corr())\n",
    "\n",
    "del train_data,test_data,df_stack3,df_stack4\n",
    "gc.collect()\n",
    "\n",
    "train=vv\n",
    "test=sub\n",
    "\n",
    "vote = pd.DataFrame(index=range(len(test)))\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "train_predictions = np.array(lgb1_train)*0.3+np.array(lgb7_train)*0.3+np.array(lgb3_train)*0.2+np.array(nn3_train)*0.2\n",
    "blend_train=train_predictions.copy()\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "print(coefficients_)\n",
    "\n",
    "test_predictions =  np.array(lgb1_test)*0.3+np.array(lgb7_test)*0.3+np.array(lgb3_test)*0.2+np.array(nn3_test)*0.2\n",
    "blend_test=test_predictions.copy()\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['blend1']=test_predictions\n",
    "\n",
    "features = [x for x in train.columns if x not in ['PetID','AdoptionSpeed']]\n",
    "\n",
    "label='AdoptionSpeed'\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 80,\n",
    "         'max_depth':9,\n",
    "          'learning_rate': 0.01,\n",
    "          'bagging_fraction': 0.9,\n",
    "           \"bagging_freq\":3,\n",
    "          'feature_fraction': 0.9,\n",
    "          'min_split_gain': 0.01,\n",
    "          'min_child_samples': 150,\n",
    "          \"lambda_l1\": 0.1,\n",
    "          'verbosity': -1,\n",
    "          'early_stop': 100,\n",
    "          'verbose_eval': 200,\n",
    "           \"data_random_seed\":3,\n",
    "#           \"random_state\":1017,\n",
    "          'num_rounds': 10000}\n",
    "\n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, rmse, 'LGB')\n",
    "\n",
    "imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\n",
    "imp=imports.sort_values('importance', ascending=False)\n",
    "print(imp)\n",
    "\n",
    "lgb_train = [r[0] for r in results['train']]\n",
    "lgb_test = [r[0] for r in results['test']]\n",
    "optR = OptimizedRounder()\n",
    "train_predictions =lgb_train\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "\n",
    "test_predictions =  lgb_test\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['lgb']=test_predictions\n",
    "\n",
    "def runBR(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "\n",
    "\n",
    "    model = BayesianRidge()\n",
    "    model.fit(train_X,train_y)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    log=0#log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "#     pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(test_X2)\n",
    "#     pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y2[:,0]))]) \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), 0, coefficients, qwk,log\n",
    "results = run_cv_model(train[features], test[features], train[label], runBR, params, rmse, 'LR')\n",
    "\n",
    "br_train = [r[0] for r in results['train']]\n",
    "br_test = [r[0] for r in results['test']]\n",
    "optR = OptimizedRounder()\n",
    "train_predictions =br_train\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "\n",
    "test_predictions =  br_test\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['br']=test_predictions\n",
    "\n",
    "params={\n",
    "\t'booster':'gbtree',\n",
    "\t'objective': 'reg:linear',\n",
    "    \"tree_method\":\"gpu_hist\",\n",
    "            \"gpu_id\":0,\n",
    "#      'is_unbalance':'True',\n",
    "# \t'scale_pos_weight': 1500.0/13458.0,\n",
    "        'eval_metric': \"rmse\",\n",
    "\t'gamma':0.2,#0.2 is ok\n",
    "\t'max_depth':7,\n",
    "# \t'lambda':20,\n",
    "    # \"alpha\":5,\n",
    "        'subsample':0.9,\n",
    "        'colsample_bytree':0.9 ,\n",
    "        'min_child_weight':3, \n",
    "        'eta': 0.01,\n",
    "    # 'learning_rate':0.01,\n",
    "    \"silent\":1,\n",
    "\t'seed':1024,\n",
    "\t'nthread':12,\n",
    "     'num_rounds': 5000,\n",
    "    'verbose_eval': 200,\n",
    "    'early_stop':100,\n",
    "\n",
    "   \n",
    "    }\n",
    "def runXGB(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = xgb.DMatrix(train_X, label=train_y)\n",
    "    d_valid = xgb.DMatrix(test_X, label=test_y)\n",
    "    watchlist = [(d_train,'train'),\n",
    "    (d_valid,'val')\n",
    "             ]\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = xgb.train(params,\n",
    "                      d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      evals=watchlist,\n",
    "#                       fobj=softkappaObj,\n",
    "                      verbose_eval=verbose_eval,\n",
    "#                       feval=kappa_scorer,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(xgb.DMatrix(test_X),ntree_limit=model.best_ntree_limit)\n",
    "    log=0#log_loss(test_y,pred_test_y)\n",
    "    print(\"log_loss:\",log)\n",
    "    class_list=[0,1,2,3,4]\n",
    "#     pred_test_y=np.array([sum(pred_test_y[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y[:,0]))]) \n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(pred_test_y, test_y)\n",
    "    len_0 = sum([1 for i in test_y if i==0])\n",
    "    coefficients = optR.coefficients()\n",
    "    pred_test_y_k = optR.predict(pred_test_y, coefficients,len_0)\n",
    "   \n",
    "    print(\"Valid Counts = \", Counter(test_y))\n",
    "    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n",
    "    print(\"Coefficients = \", coefficients)\n",
    "    qwk = cohen_kappa_score(test_y, pred_test_y_k,weights='quadratic')\n",
    "    print(\"QWK = \", qwk)\n",
    "    print('Predict 2/2')\n",
    "    pred_test_y2 = model.predict(xgb.DMatrix(test_X2),ntree_limit=model.best_ntree_limit)\n",
    "#     pred_test_y2=np.array([sum(pred_test_y2[ix]*class_list) for\n",
    "#                                ix in range(len(pred_test_y2[:,0]))]) \n",
    "   \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.get_fscore(), coefficients, qwk,log\n",
    "results = run_cv_model(train[features], test[features], train[label], runXGB, params, rmse, 'XGB')\n",
    "\n",
    "xgb_train = [r[0] for r in results['train']]\n",
    "xgb_test = [r[0] for r in results['test']]\n",
    "optR = OptimizedRounder()\n",
    "train_predictions =xgb_train\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "\n",
    "test_predictions =  xgb_test\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['xgb']=test_predictions\n",
    "\n",
    "\n",
    "results = run_cv_model(train[features], test[features], train[label], runCAT, params, rmse, 'CAT')\n",
    "cat_train = [r[0] for r in results['train']]\n",
    "cat_test = [r[0] for r in results['test']]\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "train_predictions =cat_train\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "\n",
    "test_predictions =  cat_test\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['cat']=test_predictions\n",
    "\n",
    "optR = OptimizedRounder()\n",
    "train_predictions = (np.array(lgb_train)+np.array(xgb_train)+np.array(cat_train)+np.array(br_train))/4.0#[r[0] for r in results['train']]##np.array(lgb1_train)*0.7+np.array(lgb2_train)*0.3\n",
    "stack_train = train_predictions.copy()\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "print(coefficients_)\n",
    "\n",
    "test_predictions =  (np.array(lgb_test)+np.array(xgb_test)+np.array(cat_test)+np.array(br_test))/4.0#[r[0] for r in results['test']]#reg.coef_[0] *sub[\"pred1\"]+reg.coef_[1] *sub[\"pred2\"]#np.array(lgb1_test)*0.7+np.array(lgb2_test)*0.3\n",
    "stack_test = test_predictions.copy()\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['blend2']=test_predictions\n",
    "\n",
    "####blend+stacking\n",
    "optR = OptimizedRounder()\n",
    "train_predictions = np.array(blend_train)*0.5+np.array(stack_train)*0.5#[r[0] for r in results['train']]##np.array(lgb1_train)*0.7+np.array(lgb2_train)*0.3\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "print(coefficients_)\n",
    "\n",
    "test_predictions =  np.array(blend_test)*0.5+np.array(stack_test)*0.5#[r[0] for r in results['test']]#reg.coef_[0] *sub[\"pred1\"]+reg.coef_[1] *sub[\"pred2\"]#np.array(lgb1_test)*0.7+np.array(lgb2_test)*0.3\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['blend+stack']=test_predictions\n",
    "vote['blend+stack_copy']=test_predictions\n",
    "\n",
    "test_predictions = vote.mode(1)[0].values.astype(int)\n",
    "test_predictions  = vote['br'].values\n",
    "train_predictions = optR.predict(train_predictions, coefficients_,410).astype(int)\n",
    "print(cohen_kappa_score(train[label], train_predictions,weights='quadratic'))\n",
    "print(rmse(train[label], train_predictions))\n",
    "submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions})\n",
    "print(submission.head())\n",
    "print(submission.AdoptionSpeed.value_counts())\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"done! cost:{} s\".format(time.time()-start_time))\n",
    "    ents()\n",
    "print(coefficients_)\n",
    "\n",
    "test_predictions =  (np.array(lgb_test)+np.array(xgb_test)+np.array(cat_test)+np.array(br_test))/4.0#[r[0] for r in results['test']]#reg.coef_[0] *sub[\"pred1\"]+reg.coef_[1] *sub[\"pred2\"]#np.array(lgb1_test)*0.7+np.array(lgb2_test)*0.3\n",
    "stack_test = test_predictions.copy()\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['blend2']=test_predictions\n",
    "\n",
    "####blend+stacking\n",
    "optR = OptimizedRounder()\n",
    "train_predictions = np.array(blend_train)*0.5+np.array(stack_train)*0.5#[r[0] for r in results['train']]##np.array(lgb1_train)*0.7+np.array(lgb2_train)*0.3\n",
    "boundaries = get_class_bounds(train[label], train_predictions)\n",
    "optR.fit(train_predictions, train[label],boundaries)\n",
    "coefficients_ = optR.coefficients()\n",
    "print(coefficients_)\n",
    "\n",
    "test_predictions =  np.array(blend_test)*0.5+np.array(stack_test)*0.5#[r[0] for r in results['test']]#reg.coef_[0] *sub[\"pred1\"]+reg.coef_[1] *sub[\"pred2\"]#np.array(lgb1_test)*0.7+np.array(lgb2_test)*0.3\n",
    "test_predictions = optR.predict(test_predictions, coefficients_,90).astype(int)\n",
    "print(Counter(test_predictions))\n",
    "vote['blend+stack']=test_predictions\n",
    "vote['blend+stack_copy']=test_predictions\n",
    "\n",
    "test_predictions = vote.mode(1)[0].values.astype(int)\n",
    "test_predictions  = vote['br'].values\n",
    "train_predictions = optR.predict(train_predictions, coefficients_,410).astype(int)\n",
    "print(cohen_kappa_score(train[label], train_predictions,weights='quadratic'))\n",
    "print(rmse(train[label], train_predictions))\n",
    "submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions})\n",
    "print(submission.head())\n",
    "print(submission.AdoptionSpeed.value_counts())\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"done! cost:{} s\".format(time.time()-start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
