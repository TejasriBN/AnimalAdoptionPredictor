{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel,  delayed\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 128\n",
    "pd.options.display.max_columns = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_submission.csv', 'test.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('input/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/train/train.csv')\n",
    "test = pd.read_csv('input/test/test.csv')\n",
    "sample_submission = pd.read_csv('input/test/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_breed = pd.read_csv('input/breed_labels.csv')\n",
    "labels_state = pd.read_csv('input/state_labels.csv')\n",
    "labels_color = pd.read_csv('input/color_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train images files: 58311\n",
      "num of train metadata files: 58311\n",
      "num of train sentiment files: 14442\n",
      "\n",
      "\n",
      "\n",
      "num of test images files: 14465\n",
      "num of test metadata files: 14465\n",
      "num of test sentiment files: 3865\n"
     ]
    }
   ],
   "source": [
    "train_image_files = sorted(glob.glob('input/train_images/*.jpg'))\n",
    "train_metadata_files = sorted(glob.glob('input/train_metadata/*.json'))\n",
    "train_sentiment_files = sorted(glob.glob('input/train_sentiment/*.json'))\n",
    "\n",
    "print('num of train images files: {}'.format(len(train_image_files)))\n",
    "print('num of train metadata files: {}'.format(len(train_metadata_files)))\n",
    "print('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "test_image_files = sorted(glob.glob('input/test_images/*.jpg'))\n",
    "test_metadata_files = sorted(glob.glob('input/test_metadata/*.json'))\n",
    "test_sentiment_files = sorted(glob.glob('input/test_sentiment/*.json'))\n",
    "\n",
    "print('num of test images files: {}'.format(len(test_image_files)))\n",
    "print('num of test metadata files: {}'.format(len(test_metadata_files)))\n",
    "print('num of test sentiment files: {}'.format(len(test_sentiment_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 1)\n",
      "14652\n",
      "fraction of pets with images: 0.977\n"
     ]
    }
   ],
   "source": [
    "#Train Images:\n",
    "train_df_ids = train[['PetID']]\n",
    "print(train_df_ids.shape)\n",
    "\n",
    "train_df_imgs = pd.DataFrame(train_image_files)\n",
    "train_df_imgs.columns = ['image_filename']\n",
    "#print(train_df_imgs)\n",
    "train_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_imgs_pets = train_imgs_pets.apply(lambda x: x.split(\"\\\\\")[-1])\n",
    "\n",
    "#print(\"\\n\\n\",train_imgs_pets)\n",
    "train_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\n",
    "print(len(train_imgs_pets.unique()))\n",
    "\n",
    "pets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / train_df_ids.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14652\n",
      "fraction of pets with metadata: 0.977\n"
     ]
    }
   ],
   "source": [
    "#Train Metadata:\n",
    "\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_metadata = pd.DataFrame(train_metadata_files)\n",
    "train_df_metadata.columns = ['metadata_filename']\n",
    "train_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\n",
    "train_metadata_pets = train_metadata_pets.apply(lambda x: x.split(\"\\\\\")[-1])\n",
    "train_df_metadata = train_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\n",
    "print(len(train_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas /train_df_ids.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14442\n",
      "fraction of pets with sentiment: 0.963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train Sentiment:\n",
    "\n",
    "train_df_ids = train[['PetID']]\n",
    "train_df_sentiment = pd.DataFrame(train_sentiment_files)\n",
    "train_df_sentiment.columns = ['sentiment_filename']\n",
    "train_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "train_sentiment_pets = train_sentiment_pets.apply(lambda x: x.split(\"\\\\\")[-1])\n",
    "train_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\n",
    "print(len(train_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments/train_df_ids.shape[0]))                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3972, 1)\n",
      "3858\n",
      "fraction of pets with images: 0.971\n"
     ]
    }
   ],
   "source": [
    "#Test Images\n",
    "test_df_ids = test[['PetID']]\n",
    "print(test_df_ids.shape)\n",
    "\n",
    "test_df_imgs = pd.DataFrame(test_image_files)\n",
    "test_df_imgs.columns = ['image_filename']\n",
    "test_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/') [-1].split('-')[0])\n",
    "test_imgs_pets = test_imgs_pets.apply(lambda x: x.split(\"\\\\\")[-1])\n",
    "test_df_imgs = test_df_imgs.assign(PetID = test_imgs_pets)\n",
    "print(len(test_imgs_pets.unique()))\n",
    "\n",
    "pets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with images: {:.3f}'.format(pets_with_images / test_df_ids.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3858\n",
      "fraction of pets with metadata: 0.971\n"
     ]
    }
   ],
   "source": [
    "#Test Metadata:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_metadata = pd.DataFrame(test_metadata_files)\n",
    "test_df_metadata.columns = ['metadata_filename']\n",
    "test_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split ('-') [0])\n",
    "test_metadata_pets = test_metadata_pets.apply(lambda x: x.split(\"\\\\\")[-1])\n",
    "test_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\n",
    "print(len(test_metadata_pets.unique()))\n",
    "\n",
    "pets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids ['PetID'].unique ()))\n",
    "print('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / test_df_ids.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3865\n",
      "fraction of pets with sentiment: 0.973\n"
     ]
    }
   ],
   "source": [
    "#Test Sentiment:\n",
    "test_df_ids = test[['PetID']]\n",
    "test_df_sentiment = pd.DataFrame(test_sentiment_files)\n",
    "test_df_sentiment.columns = ['sentiment_filename']\n",
    "test_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\n",
    "test_sentiment_pets = test_sentiment_pets.apply(lambda x: x.split(\"\\\\\")[-1])\n",
    "test_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\n",
    "print(len(test_sentiment_pets.unique()))\n",
    "\n",
    "pets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\n",
    "print('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / test_df_ids.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images and metadata distributions the same? True\n"
     ]
    }
   ],
   "source": [
    "#are distributions the same ?\n",
    "print('images and metadata distributions the same? {}'.format(np.all(test_metadata_pets == test_imgs_pets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PetFinderParser(object):\n",
    "    \n",
    "    def __init__(self, debug=False):\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.sentence_sep = ' '\n",
    "        \n",
    "        # Does not have to be extracted because main DF already contains description\n",
    "        self.extract_sentiment_text = True\n",
    "        \n",
    "    def open_metadata_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load metadata file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r',encoding='utf8') as f:\n",
    "            metadata_file = json.load(f)\n",
    "        return metadata_file\n",
    "            \n",
    "    def open_sentiment_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load sentiment file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'r',encoding='utf8') as f:\n",
    "            sentiment_file = json.load(f)\n",
    "            #print(sentiment_file)\n",
    "        return sentiment_file\n",
    "            \n",
    "    def open_image_file(self, filename):\n",
    "        \"\"\"\n",
    "        Load image file.\n",
    "        \"\"\"\n",
    "        image = np.asarray(Image.open(filename))\n",
    "        return image\n",
    "        \n",
    "    def parse_sentiment_file(self, file):\n",
    "        \"\"\"\n",
    "        Parse sentiment file. Output DF with sentiment features.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_sentiment = file['documentSentiment']\n",
    "        file_entities = [x['name'] for x in file['entities']]\n",
    "        file_entities = self.sentence_sep.join(file_entities)\n",
    "\n",
    "        if self.extract_sentiment_text:\n",
    "            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n",
    "            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n",
    "        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n",
    "        \n",
    "        file_sentences_sentiment = pd.DataFrame.from_dict(\n",
    "            file_sentences_sentiment, orient='columns').sum()\n",
    "        file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n",
    "        \n",
    "        file_sentiment.update(file_sentences_sentiment)\n",
    "        \n",
    "        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n",
    "        if self.extract_sentiment_text:\n",
    "            df_sentiment['text'] = file_sentences_text\n",
    "            \n",
    "        df_sentiment['entities'] = file_entities\n",
    "        df_sentiment = df_sentiment.add_prefix('sentiment_')\n",
    "        \n",
    "        return df_sentiment\n",
    "    \n",
    "    def parse_metadata_file(self, file):\n",
    "        \"\"\"\n",
    "        Parse metadata file. Output DF with metadata features.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_keys = list(file.keys())\n",
    "        \n",
    "        if 'labelAnnotations' in file_keys:\n",
    "            #file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.5)]\n",
    "            file_annots = file['labelAnnotations'][:]\n",
    "            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n",
    "            file_top_desc = [x['description'] for x in file_annots]\n",
    "        else:\n",
    "            file_top_score = np.nan\n",
    "            file_top_desc = ['']\n",
    "        \n",
    "        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n",
    "        file_crops = file['cropHintsAnnotation']['cropHints']\n",
    "\n",
    "        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n",
    "        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n",
    "\n",
    "        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n",
    "        \n",
    "        if 'importanceFraction' in file_crops[0].keys():\n",
    "            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n",
    "        else:\n",
    "            file_crop_importance = np.nan\n",
    "\n",
    "        df_metadata = {\n",
    "            'annots_score': file_top_score,\n",
    "            'color_score': file_color_score,\n",
    "            'color_pixelfrac': file_color_pixelfrac,\n",
    "            'crop_conf': file_crop_conf,\n",
    "            'crop_importance': file_crop_importance,\n",
    "            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n",
    "        }\n",
    "        \n",
    "        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n",
    "        df_metadata = df_metadata.add_prefix('metadata_')\n",
    "        \n",
    "        return df_metadata\n",
    "\n",
    "pet_parser = PetFinderParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for parallel data processing:\n",
    "def extract_additional_features(pet_id, mode='train'):\n",
    "    \n",
    "    sentiment_filename = 'input/{}_sentiment/{}.json'.format(mode, pet_id)\n",
    "    #print(sentiment_filename)\n",
    "    #print(\"st\")\n",
    "    try:\n",
    "        #print(\"in\")\n",
    "        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n",
    "        #print(\"hello  \",sentiment_file)\n",
    "        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n",
    "        #print(\"uu  \",df_sentiment)\n",
    "        df_sentiment['PetID'] = pet_id\n",
    "    except FileNotFoundError:\n",
    "        df_sentiment = []\n",
    "    #print(\"art\")\n",
    "\n",
    "    dfs_metadata = []\n",
    "    metadata_filenames = sorted(glob.glob('input/{}_metadata/{}*.json'.format(mode, pet_id)))\n",
    "    if len(metadata_filenames) > 0:\n",
    "        for f in metadata_filenames:\n",
    "            metadata_file = pet_parser.open_metadata_file(f)\n",
    "            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n",
    "            df_metadata['PetID'] = pet_id\n",
    "            dfs_metadata.append(df_metadata)\n",
    "        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n",
    "    dfs = [df_sentiment, dfs_metadata]\n",
    "    #print(dfs)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   sentiment_magnitude  sentiment_score  sentiment_document_magnitude  \\\n",
       " 0                  0.9              0.9                           0.9   \n",
       " \n",
       "    sentiment_document_score  \\\n",
       " 0                       0.9   \n",
       " \n",
       "                                       sentiment_text sentiment_entities  \\\n",
       " 0  Fili just loves laying around and also loves b...           Fili sun   \n",
       " \n",
       "        PetID  \n",
       " 0  a83d95ead  ,\n",
       "   metadata_annots_score metadata_color_score metadata_color_pixelfrac  \\\n",
       " 0              0.805343             0.073463                 0.014032   \n",
       " \n",
       "   metadata_crop_conf metadata_crop_importance  \\\n",
       " 0                0.8                     0.98   \n",
       " \n",
       "                             metadata_annots_top_desc      PetID  \n",
       " 0  dog dog like mammal puppy dog breed dog breed ...  a83d95ead  ]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_additional_features('a83d95ead',mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:   44.3s\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=6)]: Done 1238 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=6)]: Done 1788 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=6)]: Done 2438 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=6)]: Done 3188 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=6)]: Done 4988 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=6)]: Done 6038 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=6)]: Done 7188 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=6)]: Done 8438 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=6)]: Done 9788 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=6)]: Done 11238 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=6)]: Done 12788 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=6)]: Done 14438 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=6)]: Done 14993 out of 14993 | elapsed: 23.9min finished\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14442, 7) (58311, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  64 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=6)]: Done 364 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=6)]: Done 864 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=6)]: Done 1564 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=6)]: Done 2464 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=6)]: Done 3564 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=6)]: Done 3972 out of 3972 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3865, 7) (14465, 7)\n"
     ]
    }
   ],
   "source": [
    "def pet_parsing(train, test):\n",
    "    \n",
    "    # Unique IDs from train and test:\n",
    "    debug = False\n",
    "    train_pet_ids = train.PetID.unique()\n",
    "    test_pet_ids = test.PetID.unique()\n",
    "\n",
    "    if debug:\n",
    "        train_pet_ids = train_pet_ids[:10]\n",
    "        test_pet_ids = test_pet_ids[:5]\n",
    "\n",
    "    # Train set:\n",
    "    # Parallel processing of data:\n",
    "    dfs_train = Parallel(n_jobs=6, verbose=1)(\n",
    "        delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n",
    "\n",
    "    # Extract processed data and format them as DFs:\n",
    "    train_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\n",
    "    print(train_dfs_sentiment)\n",
    "    train_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n",
    "    print(train_dfs_metadata)\n",
    "\n",
    "    train_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\n",
    "    train_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "    print(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n",
    "\n",
    "    # Test set:\n",
    "    # Parallel processing of data:\n",
    "    dfs_test = Parallel(n_jobs=6, verbose=1)(\n",
    "        delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n",
    "\n",
    "    # Extract processed data and format them as DFs:\n",
    "    test_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\n",
    "    test_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n",
    "\n",
    "    test_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\n",
    "    test_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n",
    "\n",
    "    print(test_dfs_sentiment.shape, test_dfs_metadata.shape)\n",
    "    return train_dfs_sentiment,train_dfs_metadata,test_dfs_sentiment,test_dfs_metadata\n",
    "\n",
    "\n",
    "train_dfs_sentiment, train_dfs_metadata, test_dfs_sentiment, test_dfs_metadata = pet_parsing(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(train_dfs_sentiment[['sentiment_text', 'PetID']], how='left', on='PetID')\n",
    "test = test.merge(test_dfs_sentiment[['sentiment_text', 'PetID']], how='left', on='PetID')\n",
    "\n",
    "train['sentiment_text'] = train['sentiment_text'].apply(lambda x: str(x).replace('.', ' '))\n",
    "train['sentiment_text'] = train['sentiment_text'].apply(lambda x: str(x).replace(',', ' '))\n",
    "\n",
    "test['sentiment_text'] = test['sentiment_text'].apply(lambda x: str(x).replace('.', ' '))\n",
    "test['sentiment_text'] = test['sentiment_text'].apply(lambda x: str(x).replace(',', ' '))\n",
    "\n",
    "# at this moment, just drop \n",
    "train_dfs_sentiment = train_dfs_sentiment.drop(['sentiment_text'], axis=1)\n",
    "test_dfs_sentiment = test_dfs_sentiment.drop(['sentiment_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend aggregates and improve column naming\n",
    "aggregates = ['mean', 'sum', 'median', 'min', 'max']\n",
    "\n",
    "# Train\n",
    "train_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "train_metadata_desc = train_metadata_desc.reset_index()\n",
    "train_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = train_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'metadata'\n",
    "train_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "for i in train_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\n",
    "train_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\n",
    "train_metadata_gr = train_metadata_gr.reset_index()\n",
    "\n",
    "\n",
    "train_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "train_sentiment_desc = train_sentiment_desc.reset_index()\n",
    "train_sentiment_desc[\n",
    "    'sentiment_entities'] = train_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "train_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in train_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\n",
    "train_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\n",
    "train_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\n",
    "train_sentiment_gr = train_sentiment_gr.reset_index()\n",
    "\n",
    "\n",
    "# Test\n",
    "test_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\n",
    "test_metadata_desc = test_metadata_desc.reset_index()\n",
    "test_metadata_desc[\n",
    "    'metadata_annots_top_desc'] = test_metadata_desc[\n",
    "    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'metadata'\n",
    "test_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\n",
    "for i in test_metadata_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\n",
    "test_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\n",
    "test_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in test_metadata_gr.columns.tolist()])\n",
    "test_metadata_gr = test_metadata_gr.reset_index()\n",
    "\n",
    "\n",
    "test_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\n",
    "test_sentiment_desc = test_sentiment_desc.reset_index()\n",
    "test_sentiment_desc[\n",
    "    'sentiment_entities'] = test_sentiment_desc[\n",
    "    'sentiment_entities'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "prefix = 'sentiment'\n",
    "test_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\n",
    "for i in test_sentiment_gr.columns:\n",
    "    if 'PetID' not in i:\n",
    "        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\n",
    "test_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(aggregates)\n",
    "test_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n",
    "            prefix, c[0], c[1].upper()) for c in test_sentiment_gr.columns.tolist()])\n",
    "test_sentiment_gr = test_sentiment_gr.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (14993, 72), test shape (3972, 71) after adding sentiment and metadata features\n"
     ]
    }
   ],
   "source": [
    "# Train merges:\n",
    "train_proc = train.copy()\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_gr, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_metadata_desc, how='left', on='PetID')\n",
    "train_proc = train_proc.merge(\n",
    "    train_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "# Test merges:\n",
    "test_proc = test.copy()\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_gr, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_metadata_desc, how='left', on='PetID')\n",
    "test_proc = test_proc.merge(\n",
    "    test_sentiment_desc, how='left', on='PetID')\n",
    "\n",
    "print(\"Train shape {}, test shape {} after adding sentiment and metadata features\".format(train_proc.shape, test_proc.shape))\n",
    "assert train_proc.shape[0] == train.shape[0]\n",
    "assert test_proc.shape[0] == test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (14993, 76), test shape (3972, 75) after adding breed features\n"
     ]
    }
   ],
   "source": [
    "#ADDING BREED FEATURES\n",
    "\n",
    "\n",
    "train_breed_main = train_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "\n",
    "train_breed_main = train_breed_main.iloc[:, 2:]\n",
    "train_breed_main = train_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "train_breed_second = train_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "train_breed_second = train_breed_second.iloc[:, 2:]\n",
    "train_breed_second = train_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "\n",
    "train_proc = pd.concat(\n",
    "    [train_proc, train_breed_main, train_breed_second], axis=1)\n",
    "\n",
    "\n",
    "test_breed_main = test_proc[['Breed1']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed1', right_on='BreedID',\n",
    "    suffixes=('', '_main_breed'))\n",
    "\n",
    "test_breed_main = test_breed_main.iloc[:, 2:]\n",
    "test_breed_main = test_breed_main.add_prefix('main_breed_')\n",
    "\n",
    "test_breed_second = test_proc[['Breed2']].merge(\n",
    "    labels_breed, how='left',\n",
    "    left_on='Breed2', right_on='BreedID',\n",
    "    suffixes=('', '_second_breed'))\n",
    "\n",
    "test_breed_second = test_breed_second.iloc[:, 2:]\n",
    "test_breed_second = test_breed_second.add_prefix('second_breed_')\n",
    "\n",
    "test_proc = pd.concat(\n",
    "    [test_proc, test_breed_main, test_breed_second], axis=1)\n",
    "\n",
    "print(\"Train shape {}, test shape {} after adding breed features\".format(train_proc.shape, test_proc.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN Structure:\n",
      "Type                                                 0\n",
      "Name                                              1668\n",
      "Age                                                  0\n",
      "Breed1                                               0\n",
      "Breed2                                               0\n",
      "Gender                                               0\n",
      "Color1                                               0\n",
      "Color2                                               0\n",
      "Color3                                               0\n",
      "MaturitySize                                         0\n",
      "FurLength                                            0\n",
      "Vaccinated                                           0\n",
      "Dewormed                                             0\n",
      "Sterilized                                           0\n",
      "Health                                               0\n",
      "Quantity                                             0\n",
      "Fee                                                  0\n",
      "State                                                0\n",
      "RescuerID                                            0\n",
      "VideoAmt                                             0\n",
      "Description                                         13\n",
      "PetID                                                0\n",
      "PhotoAmt                                             0\n",
      "AdoptionSpeed                                     3972\n",
      "sentiment_text                                       0\n",
      "sentiment_sentiment_magnitude_MEAN                 658\n",
      "sentiment_sentiment_magnitude_SUM                  658\n",
      "sentiment_sentiment_magnitude_MEDIAN               658\n",
      "sentiment_sentiment_magnitude_MIN                  658\n",
      "sentiment_sentiment_magnitude_MAX                  658\n",
      "sentiment_sentiment_score_MEAN                     658\n",
      "sentiment_sentiment_score_SUM                      658\n",
      "sentiment_sentiment_score_MEDIAN                   658\n",
      "sentiment_sentiment_score_MIN                      658\n",
      "sentiment_sentiment_score_MAX                      658\n",
      "sentiment_sentiment_document_magnitude_MEAN        658\n",
      "sentiment_sentiment_document_magnitude_SUM         658\n",
      "sentiment_sentiment_document_magnitude_MEDIAN      658\n",
      "sentiment_sentiment_document_magnitude_MIN         658\n",
      "sentiment_sentiment_document_magnitude_MAX         658\n",
      "sentiment_sentiment_document_score_MEAN            658\n",
      "sentiment_sentiment_document_score_SUM             658\n",
      "sentiment_sentiment_document_score_MEDIAN          658\n",
      "sentiment_sentiment_document_score_MIN             658\n",
      "sentiment_sentiment_document_score_MAX             658\n",
      "metadata_metadata_annots_score_MEAN                455\n",
      "metadata_metadata_annots_score_SUM                 455\n",
      "metadata_metadata_annots_score_MEDIAN              455\n",
      "metadata_metadata_annots_score_MIN                 455\n",
      "metadata_metadata_annots_score_MAX                 455\n",
      "metadata_metadata_color_score_MEAN                 455\n",
      "metadata_metadata_color_score_SUM                  455\n",
      "metadata_metadata_color_score_MEDIAN               455\n",
      "metadata_metadata_color_score_MIN                  455\n",
      "metadata_metadata_color_score_MAX                  455\n",
      "metadata_metadata_color_pixelfrac_MEAN             455\n",
      "metadata_metadata_color_pixelfrac_SUM              455\n",
      "metadata_metadata_color_pixelfrac_MEDIAN           455\n",
      "metadata_metadata_color_pixelfrac_MIN              455\n",
      "metadata_metadata_color_pixelfrac_MAX              455\n",
      "metadata_metadata_crop_conf_MEAN                   455\n",
      "metadata_metadata_crop_conf_SUM                    455\n",
      "metadata_metadata_crop_conf_MEDIAN                 455\n",
      "metadata_metadata_crop_conf_MIN                    455\n",
      "metadata_metadata_crop_conf_MAX                    455\n",
      "metadata_metadata_crop_importance_MEAN             456\n",
      "metadata_metadata_crop_importance_SUM              455\n",
      "metadata_metadata_crop_importance_MEDIAN           456\n",
      "metadata_metadata_crop_importance_MIN              456\n",
      "metadata_metadata_crop_importance_MAX              456\n",
      "metadata_annots_top_desc                           455\n",
      "sentiment_entities                                 658\n",
      "main_breed_Type                                      5\n",
      "main_breed_BreedName                                 5\n",
      "second_breed_Type                                13729\n",
      "second_breed_BreedName                           13729\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\n",
    "print('NaN Structure:\\n{}'.format(np.sum(pd.isnull(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tinteger columns:\n",
      "Series([], dtype: object)\n",
      "\n",
      "\tfloat columns:\n",
      "PhotoAmt                                         float64\n",
      "AdoptionSpeed                                    float64\n",
      "sentiment_sentiment_magnitude_MEAN               float64\n",
      "sentiment_sentiment_magnitude_SUM                float64\n",
      "sentiment_sentiment_magnitude_MEDIAN             float64\n",
      "sentiment_sentiment_magnitude_MIN                float64\n",
      "sentiment_sentiment_magnitude_MAX                float64\n",
      "sentiment_sentiment_score_MEAN                   float64\n",
      "sentiment_sentiment_score_SUM                    float64\n",
      "sentiment_sentiment_score_MEDIAN                 float64\n",
      "sentiment_sentiment_score_MIN                    float64\n",
      "sentiment_sentiment_score_MAX                    float64\n",
      "sentiment_sentiment_document_magnitude_MEAN      float64\n",
      "sentiment_sentiment_document_magnitude_SUM       float64\n",
      "sentiment_sentiment_document_magnitude_MEDIAN    float64\n",
      "sentiment_sentiment_document_magnitude_MIN       float64\n",
      "sentiment_sentiment_document_magnitude_MAX       float64\n",
      "sentiment_sentiment_document_score_MEAN          float64\n",
      "sentiment_sentiment_document_score_SUM           float64\n",
      "sentiment_sentiment_document_score_MEDIAN        float64\n",
      "sentiment_sentiment_document_score_MIN           float64\n",
      "sentiment_sentiment_document_score_MAX           float64\n",
      "metadata_metadata_annots_score_MEAN              float64\n",
      "metadata_metadata_annots_score_SUM               float64\n",
      "metadata_metadata_annots_score_MEDIAN            float64\n",
      "metadata_metadata_annots_score_MIN               float64\n",
      "metadata_metadata_annots_score_MAX               float64\n",
      "metadata_metadata_color_score_MEAN               float64\n",
      "metadata_metadata_color_score_SUM                float64\n",
      "metadata_metadata_color_score_MEDIAN             float64\n",
      "metadata_metadata_color_score_MIN                float64\n",
      "metadata_metadata_color_score_MAX                float64\n",
      "metadata_metadata_color_pixelfrac_MEAN           float64\n",
      "metadata_metadata_color_pixelfrac_SUM            float64\n",
      "metadata_metadata_color_pixelfrac_MEDIAN         float64\n",
      "metadata_metadata_color_pixelfrac_MIN            float64\n",
      "metadata_metadata_color_pixelfrac_MAX            float64\n",
      "metadata_metadata_crop_conf_MEAN                 float64\n",
      "metadata_metadata_crop_conf_SUM                  float64\n",
      "metadata_metadata_crop_conf_MEDIAN               float64\n",
      "metadata_metadata_crop_conf_MIN                  float64\n",
      "metadata_metadata_crop_conf_MAX                  float64\n",
      "metadata_metadata_crop_importance_MEAN           float64\n",
      "metadata_metadata_crop_importance_SUM            float64\n",
      "metadata_metadata_crop_importance_MEDIAN         float64\n",
      "metadata_metadata_crop_importance_MIN            float64\n",
      "metadata_metadata_crop_importance_MAX            float64\n",
      "main_breed_Type                                  float64\n",
      "second_breed_Type                                float64\n",
      "dtype: object\n",
      "\n",
      "\tto encode categorical columns:\n",
      "Name                        object\n",
      "RescuerID                   object\n",
      "Description                 object\n",
      "PetID                       object\n",
      "sentiment_text              object\n",
      "metadata_annots_top_desc    object\n",
      "sentiment_entities          object\n",
      "main_breed_BreedName        object\n",
      "second_breed_BreedName      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column_types = X.dtypes\n",
    "\n",
    "int_cols = column_types[column_types == 'int']\n",
    "float_cols = column_types[column_types == 'float']\n",
    "cat_cols = column_types[column_types == 'object']\n",
    "\n",
    "print('\\tinteger columns:\\n{}'.format(int_cols))\n",
    "print('\\n\\tfloat columns:\\n{}'.format(float_cols))\n",
    "print('\\n\\tto encode categorical columns:\\n{}'.format(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original X DF for easier experimentation,\n",
    "# all feature engineering will be performed on this one: \n",
    "X_temp = X.copy()\n",
    "# Select subsets of columns: \n",
    "text_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities','sentiment_text']\n",
    "categorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n",
    "# Names are all unique, so they can be dropped by default\n",
    "# Same goes for PetID, it shouldn't be used as a feature\n",
    "to_drop_columns = ['PetID', 'Name', 'RescuerID']\n",
    "# RescuerID will also be dropped, as a feature based on this column will be extracted independently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teja\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1843: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "# Count RescuerID occurrences:\n",
    "\n",
    "rescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index() \n",
    "rescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n",
    "\n",
    "# Merge as another feature onto main DF: \n",
    "X_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')\n",
    "\n",
    "\n",
    "# Factorize categorical columns:\n",
    "for i in categorical_columns:\n",
    "    X_temp.loc[: , i] = pd.factorize(X_temp.loc[:, i])[0]\n",
    "\n",
    "\n",
    "# Subset text features: \n",
    "X_text = X_temp[text_columns]\n",
    "\n",
    "for i in X_text.columns:\n",
    "    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features from: Description\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teja\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features from: metadata_annots_top_desc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teja\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features from: sentiment_entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teja\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating features from: sentiment_text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teja\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n",
    "\n",
    "n_components = 5\n",
    "text_features = []\n",
    "\n",
    "# Generate text features: \n",
    "for i in X_text.columns:\n",
    "\n",
    "    # Initialize decomposition methods:\n",
    "    print('generating features from: {}'.format(i))\n",
    "    svd_ = TruncatedSVD(\n",
    "    n_components=n_components, random_state=1337)\n",
    "    nmf_ = NMF(\n",
    "    n_components=n_components, random_state=1337)\n",
    "\n",
    "    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n",
    "    svd_col = svd_.fit_transform(tfidf_col)\n",
    "    svd_col = pd.DataFrame(svd_col)\n",
    "    svd_col = svd_col.add_prefix('SVD_{}'.format(i))\n",
    "\n",
    "    nmf_col = nmf_.fit_transform(tfidf_col)\n",
    "    nmf_col = pd.DataFrame(nmf_col)\n",
    "    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n",
    "    text_features.append(svd_col) \n",
    "    text_features.append(nmf_col)\n",
    "\n",
    "# Combine all extracted features:\n",
    "text_features = pd.concat(text_features, axis=1)\n",
    "# Concatenate with main DF:\n",
    "X_temp = pd.concat([X_temp, text_features], axis=1)\n",
    "\n",
    "#Remove raw text columns:\n",
    "for i in X_text.columns:\n",
    "    X_temp = X_temp.drop(i, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (18965, 110)\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns:\n",
    "X_temp = X_temp.drop(to_drop_columns, axis=1)\n",
    "# Check final df shape:\n",
    "print('X shape: {}'.format(X_temp.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14993, 110)\n",
      "X_test shape: (3972, 109)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test again:\n",
    "X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :] \n",
    "X_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n",
    "\n",
    "# Remove missing target column from test: \n",
    "ttest=X_test['AdoptionSpeed']\n",
    "X_test = X_test.drop('AdoptionSpeed', axis=1)\n",
    "\n",
    "\n",
    "print('X_train shape: {}'.format(X_train.shape)) \n",
    "print('X_test shape: {}'.format(X_test.shape))\n",
    "\n",
    "assert X_train.shape[0] == train.shape[0]\n",
    "assert X_test.shape[0] == test.shape[0]\n",
    "\n",
    "\n",
    "# Check if columns between the two DFs are the same: \n",
    "train_cols = X_train.columns.tolist()\n",
    "ttrain=X_train['AdoptionSpeed']\n",
    "train_cols.remove('AdoptionSpeed')\n",
    "\n",
    "test_cols = X_test.columns.tolist() \n",
    "\n",
    "assert np.all(train_cols == test_cols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error \n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "\n",
    "\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None): \n",
    "    \"\"\"\n",
    "       Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b)) \n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b) \n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b) \n",
    "    num_ratings = int(max_rating - min_rating + 1) \n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for J in range(num_ratings)] \n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None): \n",
    "    \"\"\"\n",
    "       Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1) \n",
    "    hist_ratings = [0 for x in range(num_ratings)] \n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "   \n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = quadratic_weighted_kappa(y, X_p)\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def predict_new(self, X, coef):\n",
    "        X_p = np.copy(X)\n",
    "        X_s = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "            X_s[i] = pred\n",
    "        return X_p, X_s\n",
    "        \n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']\n",
    "    \n",
    "def rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATBOOST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cat\n",
    "param = {      \n",
    "     'max_depth': 9, \n",
    "     'learning_rate': 0.01,    \n",
    "     'min_child_samples': 150, \n",
    "     'random_seed': 17}\n",
    "\n",
    "# Additional parameters:\n",
    "early_stop = 500\n",
    "verbose_eval = 100\n",
    "num_rounds = 10000\n",
    "n_splits = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3229, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training CATBOOST:\n",
      "0:\tlearn: 1.1755584\ttest: 1.1755584\ttest1: 1.1755133\tbest: 1.1755133 (0)\ttotal: 424ms\tremaining: 1h 10m 34s\n",
      "100:\tlearn: 1.0800030\ttest: 1.0800030\ttest1: 1.1050035\tbest: 1.1050035 (100)\ttotal: 10.9s\tremaining: 17m 52s\n",
      "200:\tlearn: 1.0329583\ttest: 1.0329583\ttest1: 1.0831814\tbest: 1.0831814 (200)\ttotal: 20.7s\tremaining: 16m 50s\n",
      "300:\tlearn: 1.0011909\ttest: 1.0011909\ttest1: 1.0726452\tbest: 1.0726452 (300)\ttotal: 30.2s\tremaining: 16m 12s\n",
      "400:\tlearn: 0.9763639\ttest: 0.9763639\ttest1: 1.0665922\tbest: 1.0665922 (400)\ttotal: 39.6s\tremaining: 15m 48s\n",
      "500:\tlearn: 0.9538751\ttest: 0.9538751\ttest1: 1.0622005\tbest: 1.0622005 (500)\ttotal: 49s\tremaining: 15m 29s\n",
      "600:\tlearn: 0.9343823\ttest: 0.9343823\ttest1: 1.0589638\tbest: 1.0589638 (600)\ttotal: 58.4s\tremaining: 15m 14s\n",
      "700:\tlearn: 0.9159277\ttest: 0.9159277\ttest1: 1.0563209\tbest: 1.0563209 (700)\ttotal: 1m 7s\tremaining: 15m\n",
      "800:\tlearn: 0.8986903\ttest: 0.8986903\ttest1: 1.0540985\tbest: 1.0540816 (799)\ttotal: 1m 17s\tremaining: 14m 50s\n",
      "900:\tlearn: 0.8816593\ttest: 0.8816593\ttest1: 1.0521045\tbest: 1.0521013 (899)\ttotal: 1m 27s\tremaining: 14m 39s\n",
      "1000:\tlearn: 0.8644105\ttest: 0.8644105\ttest1: 1.0499477\tbest: 1.0499477 (1000)\ttotal: 1m 36s\tremaining: 14m 29s\n",
      "1100:\tlearn: 0.8466489\ttest: 0.8466489\ttest1: 1.0479391\tbest: 1.0479391 (1100)\ttotal: 1m 46s\tremaining: 14m 19s\n",
      "1200:\tlearn: 0.8296533\ttest: 0.8296533\ttest1: 1.0462243\tbest: 1.0462243 (1200)\ttotal: 1m 56s\tremaining: 14m 10s\n",
      "1300:\tlearn: 0.8130632\ttest: 0.8130632\ttest1: 1.0449153\tbest: 1.0448881 (1299)\ttotal: 2m 6s\tremaining: 14m 3s\n",
      "1400:\tlearn: 0.7970355\ttest: 0.7970355\ttest1: 1.0438054\tbest: 1.0437789 (1398)\ttotal: 2m 15s\tremaining: 13m 52s\n",
      "1500:\tlearn: 0.7815876\ttest: 0.7815876\ttest1: 1.0428230\tbest: 1.0428230 (1500)\ttotal: 2m 25s\tremaining: 13m 42s\n",
      "1600:\tlearn: 0.7665998\ttest: 0.7665998\ttest1: 1.0419118\tbest: 1.0419118 (1600)\ttotal: 2m 34s\tremaining: 13m 32s\n",
      "1700:\tlearn: 0.7516707\ttest: 0.7516707\ttest1: 1.0410482\tbest: 1.0410482 (1700)\ttotal: 2m 44s\tremaining: 13m 22s\n",
      "1800:\tlearn: 0.7383854\ttest: 0.7383854\ttest1: 1.0405793\tbest: 1.0405501 (1761)\ttotal: 2m 54s\tremaining: 13m 12s\n",
      "1900:\tlearn: 0.7254022\ttest: 0.7254022\ttest1: 1.0399702\tbest: 1.0399702 (1900)\ttotal: 3m 3s\tremaining: 13m 3s\n",
      "2000:\tlearn: 0.7126617\ttest: 0.7126617\ttest1: 1.0393057\tbest: 1.0392997 (1997)\ttotal: 3m 13s\tremaining: 12m 53s\n",
      "2100:\tlearn: 0.7005517\ttest: 0.7005517\ttest1: 1.0386668\tbest: 1.0386531 (2097)\ttotal: 3m 23s\tremaining: 12m 43s\n",
      "2200:\tlearn: 0.6878159\ttest: 0.6878159\ttest1: 1.0380724\tbest: 1.0380234 (2193)\ttotal: 3m 32s\tremaining: 12m 33s\n",
      "2300:\tlearn: 0.6760445\ttest: 0.6760445\ttest1: 1.0377771\tbest: 1.0377771 (2300)\ttotal: 3m 42s\tremaining: 12m 23s\n",
      "2400:\tlearn: 0.6644323\ttest: 0.6644323\ttest1: 1.0375033\tbest: 1.0375033 (2400)\ttotal: 3m 51s\tremaining: 12m 13s\n",
      "2500:\tlearn: 0.6531388\ttest: 0.6531388\ttest1: 1.0372348\tbest: 1.0372029 (2498)\ttotal: 4m 1s\tremaining: 12m 5s\n",
      "2600:\tlearn: 0.6417816\ttest: 0.6417816\ttest1: 1.0370048\tbest: 1.0369751 (2565)\ttotal: 4m 11s\tremaining: 11m 56s\n",
      "2700:\tlearn: 0.6307572\ttest: 0.6307572\ttest1: 1.0369105\tbest: 1.0368725 (2654)\ttotal: 4m 21s\tremaining: 11m 46s\n",
      "2800:\tlearn: 0.6195685\ttest: 0.6195685\ttest1: 1.0366686\tbest: 1.0366268 (2797)\ttotal: 4m 31s\tremaining: 11m 36s\n",
      "2900:\tlearn: 0.6092838\ttest: 0.6092838\ttest1: 1.0363990\tbest: 1.0363854 (2899)\ttotal: 4m 40s\tremaining: 11m 27s\n",
      "3000:\tlearn: 0.5986105\ttest: 0.5986105\ttest1: 1.0363272\tbest: 1.0362698 (2985)\ttotal: 4m 51s\tremaining: 11m 20s\n",
      "3100:\tlearn: 0.5884248\ttest: 0.5884248\ttest1: 1.0361997\tbest: 1.0361377 (3070)\ttotal: 5m 3s\tremaining: 11m 14s\n",
      "3200:\tlearn: 0.5783355\ttest: 0.5783355\ttest1: 1.0361133\tbest: 1.0359483 (3157)\ttotal: 5m 19s\tremaining: 11m 19s\n",
      "3300:\tlearn: 0.5680800\ttest: 0.5680800\ttest1: 1.0360126\tbest: 1.0359483 (3157)\ttotal: 5m 31s\tremaining: 11m 12s\n",
      "3400:\tlearn: 0.5581062\ttest: 0.5581062\ttest1: 1.0357626\tbest: 1.0357010 (3394)\ttotal: 5m 45s\tremaining: 11m 11s\n",
      "3500:\tlearn: 0.5484953\ttest: 0.5484953\ttest1: 1.0355829\tbest: 1.0355707 (3478)\ttotal: 5m 57s\tremaining: 11m 4s\n",
      "3600:\tlearn: 0.5387753\ttest: 0.5387753\ttest1: 1.0355986\tbest: 1.0355168 (3507)\ttotal: 6m 9s\tremaining: 10m 55s\n",
      "3700:\tlearn: 0.5298533\ttest: 0.5298533\ttest1: 1.0355120\tbest: 1.0354900 (3699)\ttotal: 6m 20s\tremaining: 10m 46s\n",
      "3800:\tlearn: 0.5206418\ttest: 0.5206418\ttest1: 1.0353413\tbest: 1.0352871 (3765)\ttotal: 6m 33s\tremaining: 10m 41s\n",
      "3900:\tlearn: 0.5116182\ttest: 0.5116182\ttest1: 1.0352749\tbest: 1.0352203 (3852)\ttotal: 6m 44s\tremaining: 10m 32s\n",
      "4000:\tlearn: 0.5030852\ttest: 0.5030852\ttest1: 1.0350339\tbest: 1.0350339 (4000)\ttotal: 6m 56s\tremaining: 10m 24s\n",
      "4100:\tlearn: 0.4945804\ttest: 0.4945804\ttest1: 1.0347311\tbest: 1.0347311 (4100)\ttotal: 7m 6s\tremaining: 10m 13s\n",
      "4200:\tlearn: 0.4863277\ttest: 0.4863277\ttest1: 1.0347500\tbest: 1.0347205 (4108)\ttotal: 7m 16s\tremaining: 10m 2s\n",
      "4300:\tlearn: 0.4780711\ttest: 0.4780711\ttest1: 1.0347254\tbest: 1.0347088 (4202)\ttotal: 7m 26s\tremaining: 9m 51s\n",
      "4400:\tlearn: 0.4701080\ttest: 0.4701080\ttest1: 1.0345530\tbest: 1.0345506 (4397)\ttotal: 7m 37s\tremaining: 9m 41s\n",
      "4500:\tlearn: 0.4622896\ttest: 0.4622896\ttest1: 1.0344025\tbest: 1.0343982 (4499)\ttotal: 7m 48s\tremaining: 9m 32s\n",
      "4600:\tlearn: 0.4547510\ttest: 0.4547510\ttest1: 1.0342754\tbest: 1.0342105 (4567)\ttotal: 8m 2s\tremaining: 9m 25s\n",
      "4700:\tlearn: 0.4472500\ttest: 0.4472500\ttest1: 1.0342757\tbest: 1.0341877 (4646)\ttotal: 8m 14s\tremaining: 9m 17s\n",
      "4800:\tlearn: 0.4394337\ttest: 0.4394337\ttest1: 1.0342345\tbest: 1.0341877 (4646)\ttotal: 8m 27s\tremaining: 9m 9s\n",
      "4900:\tlearn: 0.4322889\ttest: 0.4322889\ttest1: 1.0342036\tbest: 1.0341877 (4646)\ttotal: 8m 38s\tremaining: 8m 59s\n",
      "5000:\tlearn: 0.4245914\ttest: 0.4245914\ttest1: 1.0341785\tbest: 1.0341290 (4986)\ttotal: 8m 50s\tremaining: 8m 50s\n",
      "5100:\tlearn: 0.4176951\ttest: 0.4176951\ttest1: 1.0340815\tbest: 1.0340649 (5097)\ttotal: 9m 1s\tremaining: 8m 40s\n",
      "5200:\tlearn: 0.4107370\ttest: 0.4107370\ttest1: 1.0342341\tbest: 1.0340649 (5097)\ttotal: 9m 13s\tremaining: 8m 30s\n",
      "5300:\tlearn: 0.4041932\ttest: 0.4041932\ttest1: 1.0341604\tbest: 1.0340649 (5097)\ttotal: 9m 26s\tremaining: 8m 22s\n",
      "5400:\tlearn: 0.3974954\ttest: 0.3974954\ttest1: 1.0340898\tbest: 1.0340649 (5097)\ttotal: 9m 37s\tremaining: 8m 12s\n",
      "5500:\tlearn: 0.3910541\ttest: 0.3910541\ttest1: 1.0339359\tbest: 1.0339354 (5496)\ttotal: 9m 49s\tremaining: 8m 1s\n",
      "5600:\tlearn: 0.3850779\ttest: 0.3850779\ttest1: 1.0338462\tbest: 1.0338424 (5599)\ttotal: 10m\tremaining: 7m 51s\n",
      "5700:\tlearn: 0.3788658\ttest: 0.3788658\ttest1: 1.0338890\tbest: 1.0338324 (5669)\ttotal: 10m 12s\tremaining: 7m 41s\n",
      "5800:\tlearn: 0.3729715\ttest: 0.3729715\ttest1: 1.0338277\tbest: 1.0337847 (5782)\ttotal: 10m 23s\tremaining: 7m 31s\n",
      "5900:\tlearn: 0.3670538\ttest: 0.3670538\ttest1: 1.0336746\tbest: 1.0336709 (5898)\ttotal: 10m 34s\tremaining: 7m 20s\n",
      "6000:\tlearn: 0.3615026\ttest: 0.3615026\ttest1: 1.0337286\tbest: 1.0336707 (5902)\ttotal: 10m 45s\tremaining: 7m 10s\n",
      "6100:\tlearn: 0.3558649\ttest: 0.3558649\ttest1: 1.0336665\tbest: 1.0336618 (6090)\ttotal: 10m 57s\tremaining: 6m 59s\n",
      "6200:\tlearn: 0.3498626\ttest: 0.3498626\ttest1: 1.0337057\tbest: 1.0336523 (6106)\ttotal: 11m 11s\tremaining: 6m 51s\n",
      "6300:\tlearn: 0.3440740\ttest: 0.3440740\ttest1: 1.0336136\tbest: 1.0335874 (6291)\ttotal: 11m 22s\tremaining: 6m 40s\n",
      "6400:\tlearn: 0.3383975\ttest: 0.3383975\ttest1: 1.0336469\tbest: 1.0335509 (6369)\ttotal: 11m 33s\tremaining: 6m 30s\n",
      "6500:\tlearn: 0.3331300\ttest: 0.3331300\ttest1: 1.0335949\tbest: 1.0335496 (6486)\ttotal: 11m 44s\tremaining: 6m 19s\n",
      "6600:\tlearn: 0.3275289\ttest: 0.3275289\ttest1: 1.0335576\tbest: 1.0335393 (6556)\ttotal: 11m 56s\tremaining: 6m 8s\n",
      "6700:\tlearn: 0.3221551\ttest: 0.3221551\ttest1: 1.0336141\tbest: 1.0335393 (6556)\ttotal: 12m 7s\tremaining: 5m 58s\n",
      "6800:\tlearn: 0.3171119\ttest: 0.3171119\ttest1: 1.0337309\tbest: 1.0335393 (6556)\ttotal: 12m 18s\tremaining: 5m 47s\n",
      "6900:\tlearn: 0.3120718\ttest: 0.3120718\ttest1: 1.0338225\tbest: 1.0335393 (6556)\ttotal: 12m 31s\tremaining: 5m 37s\n",
      "7000:\tlearn: 0.3070563\ttest: 0.3070563\ttest1: 1.0338665\tbest: 1.0335393 (6556)\ttotal: 12m 45s\tremaining: 5m 27s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 1.03353926\n",
      "bestIteration = 6556\n",
      "\n",
      "Shrink model to first 6557 iterations.\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3357, 2.0: 3229, 3.0: 2608, 1.0: 2472, 0.0: 328})\n",
      "training CATBOOST:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.1754429\ttest: 1.1754429\ttest1: 1.1757869\tbest: 1.1757869 (0)\ttotal: 120ms\tremaining: 20m\n",
      "100:\tlearn: 1.0824862\ttest: 1.0824862\ttest1: 1.1021921\tbest: 1.1021921 (100)\ttotal: 11.7s\tremaining: 19m 9s\n",
      "200:\tlearn: 1.0370081\ttest: 1.0370081\ttest1: 1.0772242\tbest: 1.0772242 (200)\ttotal: 23.6s\tremaining: 19m 10s\n",
      "300:\tlearn: 1.0061536\ttest: 1.0061536\ttest1: 1.0657092\tbest: 1.0657092 (300)\ttotal: 35.6s\tremaining: 19m 6s\n",
      "400:\tlearn: 0.9817983\ttest: 0.9817983\ttest1: 1.0587067\tbest: 1.0587067 (400)\ttotal: 48.8s\tremaining: 19m 29s\n",
      "500:\tlearn: 0.9604542\ttest: 0.9604542\ttest1: 1.0536828\tbest: 1.0536828 (500)\ttotal: 1m 6s\tremaining: 20m 54s\n",
      "600:\tlearn: 0.9406990\ttest: 0.9406990\ttest1: 1.0496281\tbest: 1.0496281 (600)\ttotal: 1m 58s\tremaining: 30m 56s\n",
      "700:\tlearn: 0.9231610\ttest: 0.9231610\ttest1: 1.0466066\tbest: 1.0466066 (700)\ttotal: 3m 3s\tremaining: 40m 35s\n",
      "800:\tlearn: 0.9055424\ttest: 0.9055424\ttest1: 1.0436994\tbest: 1.0436994 (800)\ttotal: 4m 20s\tremaining: 49m 50s\n",
      "900:\tlearn: 0.8884364\ttest: 0.8884364\ttest1: 1.0413497\tbest: 1.0413380 (899)\ttotal: 5m 13s\tremaining: 52m 41s\n",
      "1000:\tlearn: 0.8712821\ttest: 0.8712821\ttest1: 1.0395160\tbest: 1.0395160 (1000)\ttotal: 6m 13s\tremaining: 56m\n",
      "1100:\tlearn: 0.8540216\ttest: 0.8540216\ttest1: 1.0370707\tbest: 1.0370707 (1100)\ttotal: 6m 40s\tremaining: 53m 58s\n",
      "1200:\tlearn: 0.8370168\ttest: 0.8370168\ttest1: 1.0349432\tbest: 1.0349432 (1200)\ttotal: 6m 59s\tremaining: 51m 10s\n",
      "1300:\tlearn: 0.8204366\ttest: 0.8204366\ttest1: 1.0334990\tbest: 1.0334990 (1300)\ttotal: 7m 11s\tremaining: 48m 7s\n",
      "1400:\tlearn: 0.8041394\ttest: 0.8041394\ttest1: 1.0324226\tbest: 1.0323927 (1397)\ttotal: 7m 26s\tremaining: 45m 40s\n",
      "1500:\tlearn: 0.7884938\ttest: 0.7884938\ttest1: 1.0315831\tbest: 1.0315592 (1489)\ttotal: 7m 37s\tremaining: 43m 11s\n",
      "1600:\tlearn: 0.7732670\ttest: 0.7732670\ttest1: 1.0305498\tbest: 1.0305299 (1598)\ttotal: 7m 47s\tremaining: 40m 53s\n",
      "1700:\tlearn: 0.7581438\ttest: 0.7581438\ttest1: 1.0297480\tbest: 1.0297218 (1698)\ttotal: 7m 57s\tremaining: 38m 50s\n",
      "1800:\tlearn: 0.7436727\ttest: 0.7436727\ttest1: 1.0287607\tbest: 1.0287607 (1800)\ttotal: 8m 9s\tremaining: 37m 9s\n",
      "1900:\tlearn: 0.7297592\ttest: 0.7297592\ttest1: 1.0279113\tbest: 1.0278938 (1898)\ttotal: 8m 21s\tremaining: 35m 36s\n",
      "2000:\tlearn: 0.7157690\ttest: 0.7157690\ttest1: 1.0271670\tbest: 1.0271670 (2000)\ttotal: 8m 34s\tremaining: 34m 16s\n",
      "2100:\tlearn: 0.7033614\ttest: 0.7033614\ttest1: 1.0265837\tbest: 1.0265785 (2089)\ttotal: 8m 45s\tremaining: 32m 57s\n",
      "2200:\tlearn: 0.6902284\ttest: 0.6902284\ttest1: 1.0261232\tbest: 1.0261232 (2200)\ttotal: 8m 57s\tremaining: 31m 45s\n",
      "2300:\tlearn: 0.6776345\ttest: 0.6776345\ttest1: 1.0258338\tbest: 1.0258208 (2298)\ttotal: 9m 9s\tremaining: 30m 40s\n",
      "2400:\tlearn: 0.6656000\ttest: 0.6656000\ttest1: 1.0254257\tbest: 1.0253786 (2386)\ttotal: 9m 21s\tremaining: 29m 37s\n",
      "2500:\tlearn: 0.6535815\ttest: 0.6535815\ttest1: 1.0248765\tbest: 1.0248685 (2499)\ttotal: 9m 32s\tremaining: 28m 37s\n",
      "2600:\tlearn: 0.6424371\ttest: 0.6424371\ttest1: 1.0243943\tbest: 1.0243237 (2577)\ttotal: 9m 46s\tremaining: 27m 49s\n",
      "2700:\tlearn: 0.6304626\ttest: 0.6304626\ttest1: 1.0244278\tbest: 1.0241939 (2668)\ttotal: 9m 58s\tremaining: 26m 58s\n",
      "2800:\tlearn: 0.6190825\ttest: 0.6190825\ttest1: 1.0240077\tbest: 1.0239758 (2782)\ttotal: 10m 10s\tremaining: 26m 9s\n",
      "2900:\tlearn: 0.6087240\ttest: 0.6087240\ttest1: 1.0238461\tbest: 1.0238457 (2898)\ttotal: 10m 22s\tremaining: 25m 24s\n",
      "3000:\tlearn: 0.5983369\ttest: 0.5983369\ttest1: 1.0236778\tbest: 1.0235632 (2991)\ttotal: 10m 33s\tremaining: 24m 37s\n",
      "3100:\tlearn: 0.5885348\ttest: 0.5885348\ttest1: 1.0236566\tbest: 1.0235632 (2991)\ttotal: 10m 44s\tremaining: 23m 54s\n",
      "3200:\tlearn: 0.5788135\ttest: 0.5788135\ttest1: 1.0235711\tbest: 1.0234932 (3158)\ttotal: 10m 56s\tremaining: 23m 14s\n",
      "3300:\tlearn: 0.5690634\ttest: 0.5690634\ttest1: 1.0236815\tbest: 1.0234932 (3158)\ttotal: 11m 7s\tremaining: 22m 34s\n",
      "3400:\tlearn: 0.5592027\ttest: 0.5592027\ttest1: 1.0237019\tbest: 1.0234932 (3158)\ttotal: 11m 18s\tremaining: 21m 56s\n",
      "3500:\tlearn: 0.5495990\ttest: 0.5495990\ttest1: 1.0234737\tbest: 1.0234250 (3478)\ttotal: 11m 30s\tremaining: 21m 22s\n",
      "3600:\tlearn: 0.5402401\ttest: 0.5402401\ttest1: 1.0233784\tbest: 1.0232431 (3561)\ttotal: 11m 41s\tremaining: 20m 46s\n",
      "3700:\tlearn: 0.5311425\ttest: 0.5311425\ttest1: 1.0232069\tbest: 1.0231876 (3689)\ttotal: 11m 52s\tremaining: 20m 12s\n",
      "3800:\tlearn: 0.5224494\ttest: 0.5224494\ttest1: 1.0231961\tbest: 1.0230862 (3774)\ttotal: 12m 4s\tremaining: 19m 41s\n",
      "3900:\tlearn: 0.5140235\ttest: 0.5140235\ttest1: 1.0230813\tbest: 1.0230139 (3888)\ttotal: 12m 15s\tremaining: 19m 9s\n",
      "4000:\tlearn: 0.5054157\ttest: 0.5054157\ttest1: 1.0232815\tbest: 1.0230139 (3888)\ttotal: 12m 28s\tremaining: 18m 42s\n",
      "4100:\tlearn: 0.4970656\ttest: 0.4970656\ttest1: 1.0232933\tbest: 1.0230139 (3888)\ttotal: 12m 40s\tremaining: 18m 14s\n",
      "4200:\tlearn: 0.4890691\ttest: 0.4890691\ttest1: 1.0233938\tbest: 1.0230139 (3888)\ttotal: 12m 51s\tremaining: 17m 44s\n",
      "4300:\tlearn: 0.4809496\ttest: 0.4809496\ttest1: 1.0234181\tbest: 1.0230139 (3888)\ttotal: 13m 2s\tremaining: 17m 17s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 1.023013906\n",
      "bestIteration = 3888\n",
      "\n",
      "Shrink model to first 3889 iterations.\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3357, 2.0: 3230, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training CATBOOST:\n",
      "0:\tlearn: 1.1756234\ttest: 1.1756234\ttest1: 1.1759589\tbest: 1.1759589 (0)\ttotal: 196ms\tremaining: 32m 37s\n",
      "100:\tlearn: 1.0796739\ttest: 1.0796739\ttest1: 1.1090385\tbest: 1.1090385 (100)\ttotal: 50.7s\tremaining: 1h 22m 51s\n",
      "200:\tlearn: 1.0332534\ttest: 1.0332534\ttest1: 1.0888728\tbest: 1.0888728 (200)\ttotal: 1m 42s\tremaining: 1h 23m 40s\n",
      "300:\tlearn: 1.0008018\ttest: 1.0008018\ttest1: 1.0800263\tbest: 1.0800263 (300)\ttotal: 2m 44s\tremaining: 1h 28m 26s\n",
      "400:\tlearn: 0.9756604\ttest: 0.9756604\ttest1: 1.0746512\tbest: 1.0746512 (400)\ttotal: 3m 44s\tremaining: 1h 29m 39s\n",
      "500:\tlearn: 0.9533099\ttest: 0.9533099\ttest1: 1.0705924\tbest: 1.0705924 (500)\ttotal: 4m 9s\tremaining: 1h 18m 47s\n",
      "600:\tlearn: 0.9339441\ttest: 0.9339441\ttest1: 1.0674024\tbest: 1.0674024 (600)\ttotal: 4m 29s\tremaining: 1h 10m 12s\n",
      "700:\tlearn: 0.9158220\ttest: 0.9158220\ttest1: 1.0650399\tbest: 1.0650399 (700)\ttotal: 4m 50s\tremaining: 1h 4m 19s\n",
      "800:\tlearn: 0.8980709\ttest: 0.8980709\ttest1: 1.0630215\tbest: 1.0630215 (800)\ttotal: 5m 14s\tremaining: 1h 10s\n",
      "900:\tlearn: 0.8818259\ttest: 0.8818259\ttest1: 1.0610439\tbest: 1.0610439 (900)\ttotal: 5m 27s\tremaining: 55m 10s\n",
      "1000:\tlearn: 0.8654040\ttest: 0.8654040\ttest1: 1.0590528\tbest: 1.0590528 (1000)\ttotal: 5m 39s\tremaining: 50m 48s\n",
      "1100:\tlearn: 0.8486487\ttest: 0.8486487\ttest1: 1.0576684\tbest: 1.0576684 (1100)\ttotal: 5m 53s\tremaining: 47m 35s\n",
      "1200:\tlearn: 0.8331436\ttest: 0.8331436\ttest1: 1.0562835\tbest: 1.0562794 (1199)\ttotal: 6m 5s\tremaining: 44m 41s\n",
      "1300:\tlearn: 0.8163244\ttest: 0.8163244\ttest1: 1.0552795\tbest: 1.0552795 (1300)\ttotal: 6m 17s\tremaining: 42m 7s\n",
      "1400:\tlearn: 0.8005017\ttest: 0.8005017\ttest1: 1.0542532\tbest: 1.0542362 (1390)\ttotal: 6m 29s\tremaining: 39m 50s\n",
      "1500:\tlearn: 0.7847235\ttest: 0.7847235\ttest1: 1.0534405\tbest: 1.0534405 (1500)\ttotal: 6m 39s\tremaining: 37m 44s\n",
      "1600:\tlearn: 0.7704301\ttest: 0.7704301\ttest1: 1.0527996\tbest: 1.0527996 (1600)\ttotal: 6m 50s\tremaining: 35m 51s\n",
      "1700:\tlearn: 0.7555799\ttest: 0.7555799\ttest1: 1.0524326\tbest: 1.0523767 (1698)\ttotal: 7m\tremaining: 34m 13s\n",
      "1800:\tlearn: 0.7412466\ttest: 0.7412466\ttest1: 1.0515667\tbest: 1.0515667 (1800)\ttotal: 7m 11s\tremaining: 32m 44s\n",
      "1900:\tlearn: 0.7273668\ttest: 0.7273668\ttest1: 1.0508051\tbest: 1.0508051 (1900)\ttotal: 7m 21s\tremaining: 31m 21s\n",
      "2000:\tlearn: 0.7140447\ttest: 0.7140447\ttest1: 1.0501610\tbest: 1.0501610 (2000)\ttotal: 7m 31s\tremaining: 30m 4s\n",
      "2100:\tlearn: 0.7005216\ttest: 0.7005216\ttest1: 1.0497312\tbest: 1.0497312 (2100)\ttotal: 7m 41s\tremaining: 28m 54s\n",
      "2200:\tlearn: 0.6876510\ttest: 0.6876510\ttest1: 1.0494901\tbest: 1.0494636 (2193)\ttotal: 7m 51s\tremaining: 27m 49s\n",
      "2300:\tlearn: 0.6751467\ttest: 0.6751467\ttest1: 1.0493560\tbest: 1.0492735 (2248)\ttotal: 8m\tremaining: 26m 49s\n",
      "2400:\tlearn: 0.6634643\ttest: 0.6634643\ttest1: 1.0488838\tbest: 1.0488838 (2400)\ttotal: 8m 10s\tremaining: 25m 52s\n",
      "2500:\tlearn: 0.6514219\ttest: 0.6514219\ttest1: 1.0486200\tbest: 1.0486151 (2498)\ttotal: 8m 20s\tremaining: 25m\n",
      "2600:\tlearn: 0.6399765\ttest: 0.6399765\ttest1: 1.0485018\tbest: 1.0483139 (2568)\ttotal: 8m 30s\tremaining: 24m 11s\n",
      "2700:\tlearn: 0.6286934\ttest: 0.6286934\ttest1: 1.0481990\tbest: 1.0481806 (2686)\ttotal: 8m 40s\tremaining: 23m 25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800:\tlearn: 0.6167541\ttest: 0.6167541\ttest1: 1.0480440\tbest: 1.0480395 (2799)\ttotal: 8m 49s\tremaining: 22m 42s\n",
      "2900:\tlearn: 0.6055150\ttest: 0.6055150\ttest1: 1.0479715\tbest: 1.0479210 (2848)\ttotal: 9m 2s\tremaining: 22m 6s\n",
      "3000:\tlearn: 0.5943366\ttest: 0.5943366\ttest1: 1.0480145\tbest: 1.0478711 (2914)\ttotal: 9m 12s\tremaining: 21m 28s\n",
      "3100:\tlearn: 0.5837883\ttest: 0.5837883\ttest1: 1.0478083\tbest: 1.0478083 (3100)\ttotal: 9m 22s\tremaining: 20m 52s\n",
      "3200:\tlearn: 0.5735166\ttest: 0.5735166\ttest1: 1.0476053\tbest: 1.0476053 (3200)\ttotal: 9m 35s\tremaining: 20m 23s\n",
      "3300:\tlearn: 0.5632116\ttest: 0.5632116\ttest1: 1.0475042\tbest: 1.0474865 (3299)\ttotal: 9m 46s\tremaining: 19m 50s\n",
      "3400:\tlearn: 0.5534020\ttest: 0.5534020\ttest1: 1.0474624\tbest: 1.0473659 (3356)\ttotal: 9m 56s\tremaining: 19m 18s\n",
      "3500:\tlearn: 0.5435509\ttest: 0.5435509\ttest1: 1.0472740\tbest: 1.0472657 (3492)\ttotal: 10m 7s\tremaining: 18m 47s\n",
      "3600:\tlearn: 0.5343478\ttest: 0.5343478\ttest1: 1.0472499\tbest: 1.0472292 (3596)\ttotal: 10m 19s\tremaining: 18m 20s\n",
      "3700:\tlearn: 0.5250883\ttest: 0.5250883\ttest1: 1.0472653\tbest: 1.0472065 (3612)\ttotal: 10m 30s\tremaining: 17m 53s\n",
      "3800:\tlearn: 0.5160942\ttest: 0.5160942\ttest1: 1.0470244\tbest: 1.0469971 (3799)\ttotal: 10m 40s\tremaining: 17m 24s\n",
      "3900:\tlearn: 0.5071745\ttest: 0.5071745\ttest1: 1.0469778\tbest: 1.0469746 (3897)\ttotal: 10m 50s\tremaining: 16m 56s\n",
      "4000:\tlearn: 0.4981963\ttest: 0.4981963\ttest1: 1.0469297\tbest: 1.0469053 (3984)\ttotal: 10m 59s\tremaining: 16m 29s\n",
      "4100:\tlearn: 0.4897312\ttest: 0.4897312\ttest1: 1.0470615\tbest: 1.0469053 (3984)\ttotal: 11m 9s\tremaining: 16m 3s\n",
      "4200:\tlearn: 0.4811573\ttest: 0.4811573\ttest1: 1.0470360\tbest: 1.0469053 (3984)\ttotal: 11m 19s\tremaining: 15m 38s\n",
      "4300:\tlearn: 0.4734038\ttest: 0.4734038\ttest1: 1.0468748\tbest: 1.0468748 (4300)\ttotal: 11m 29s\tremaining: 15m 13s\n",
      "4400:\tlearn: 0.4656240\ttest: 0.4656240\ttest1: 1.0469739\tbest: 1.0468514 (4334)\ttotal: 11m 39s\tremaining: 14m 49s\n",
      "4500:\tlearn: 0.4586062\ttest: 0.4586062\ttest1: 1.0469344\tbest: 1.0468514 (4334)\ttotal: 11m 50s\tremaining: 14m 28s\n",
      "4600:\tlearn: 0.4506376\ttest: 0.4506376\ttest1: 1.0468510\tbest: 1.0468477 (4573)\ttotal: 12m 1s\tremaining: 14m 6s\n",
      "4700:\tlearn: 0.4429067\ttest: 0.4429067\ttest1: 1.0469235\tbest: 1.0467620 (4622)\ttotal: 12m 10s\tremaining: 13m 43s\n",
      "4800:\tlearn: 0.4361240\ttest: 0.4361240\ttest1: 1.0468448\tbest: 1.0467620 (4622)\ttotal: 12m 20s\tremaining: 13m 22s\n",
      "4900:\tlearn: 0.4287442\ttest: 0.4287442\ttest1: 1.0468795\tbest: 1.0467330 (4814)\ttotal: 12m 30s\tremaining: 13m\n",
      "5000:\tlearn: 0.4210569\ttest: 0.4210569\ttest1: 1.0467501\tbest: 1.0467330 (4814)\ttotal: 12m 40s\tremaining: 12m 40s\n",
      "5100:\tlearn: 0.4137966\ttest: 0.4137966\ttest1: 1.0465766\tbest: 1.0465275 (5096)\ttotal: 12m 50s\tremaining: 12m 19s\n",
      "5200:\tlearn: 0.4067238\ttest: 0.4067238\ttest1: 1.0465911\tbest: 1.0465275 (5096)\ttotal: 12m 59s\tremaining: 11m 59s\n",
      "5300:\tlearn: 0.4003582\ttest: 0.4003582\ttest1: 1.0466305\tbest: 1.0465275 (5096)\ttotal: 13m 9s\tremaining: 11m 40s\n",
      "5400:\tlearn: 0.3938779\ttest: 0.3938779\ttest1: 1.0465719\tbest: 1.0465275 (5096)\ttotal: 13m 19s\tremaining: 11m 20s\n",
      "5500:\tlearn: 0.3871908\ttest: 0.3871908\ttest1: 1.0465903\tbest: 1.0465275 (5096)\ttotal: 13m 29s\tremaining: 11m 1s\n",
      "5600:\tlearn: 0.3809686\ttest: 0.3809686\ttest1: 1.0465188\tbest: 1.0464928 (5553)\ttotal: 13m 38s\tremaining: 10m 43s\n",
      "5700:\tlearn: 0.3747906\ttest: 0.3747906\ttest1: 1.0465895\tbest: 1.0464928 (5553)\ttotal: 13m 48s\tremaining: 10m 25s\n",
      "5800:\tlearn: 0.3688486\ttest: 0.3688486\ttest1: 1.0464981\tbest: 1.0464332 (5748)\ttotal: 13m 59s\tremaining: 10m 7s\n",
      "5900:\tlearn: 0.3625698\ttest: 0.3625698\ttest1: 1.0465505\tbest: 1.0464332 (5748)\ttotal: 14m 9s\tremaining: 9m 50s\n",
      "6000:\tlearn: 0.3565082\ttest: 0.3565082\ttest1: 1.0465879\tbest: 1.0464332 (5748)\ttotal: 14m 19s\tremaining: 9m 32s\n",
      "6100:\tlearn: 0.3509323\ttest: 0.3509323\ttest1: 1.0466455\tbest: 1.0464332 (5748)\ttotal: 14m 29s\tremaining: 9m 15s\n",
      "6200:\tlearn: 0.3453317\ttest: 0.3453317\ttest1: 1.0466560\tbest: 1.0464332 (5748)\ttotal: 14m 38s\tremaining: 8m 58s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 1.046433242\n",
      "bestIteration = 5748\n",
      "\n",
      "Shrink model to first 5749 iterations.\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training CATBOOST:\n",
      "0:\tlearn: 1.1755594\ttest: 1.1755594\ttest1: 1.1757394\tbest: 1.1757394 (0)\ttotal: 105ms\tremaining: 17m 27s\n",
      "100:\tlearn: 1.0802603\ttest: 1.0802603\ttest1: 1.1054199\tbest: 1.1054199 (100)\ttotal: 11s\tremaining: 17m 55s\n",
      "200:\tlearn: 1.0349378\ttest: 1.0349378\ttest1: 1.0834908\tbest: 1.0834908 (200)\ttotal: 21.4s\tremaining: 17m 22s\n",
      "300:\tlearn: 1.0043262\ttest: 1.0043262\ttest1: 1.0740641\tbest: 1.0740641 (300)\ttotal: 31.8s\tremaining: 17m 3s\n",
      "400:\tlearn: 0.9797214\ttest: 0.9797214\ttest1: 1.0689244\tbest: 1.0689244 (400)\ttotal: 43.5s\tremaining: 17m 21s\n",
      "500:\tlearn: 0.9580426\ttest: 0.9580426\ttest1: 1.0649078\tbest: 1.0649078 (500)\ttotal: 54.1s\tremaining: 17m 6s\n",
      "600:\tlearn: 0.9389317\ttest: 0.9389317\ttest1: 1.0623895\tbest: 1.0623787 (599)\ttotal: 1m 4s\tremaining: 16m 49s\n",
      "700:\tlearn: 0.9193930\ttest: 0.9193930\ttest1: 1.0601234\tbest: 1.0601214 (698)\ttotal: 1m 16s\tremaining: 16m 55s\n",
      "800:\tlearn: 0.9021249\ttest: 0.9021249\ttest1: 1.0583529\tbest: 1.0583529 (800)\ttotal: 1m 27s\tremaining: 16m 39s\n",
      "900:\tlearn: 0.8852169\ttest: 0.8852169\ttest1: 1.0568223\tbest: 1.0568055 (899)\ttotal: 1m 37s\tremaining: 16m 25s\n",
      "1000:\tlearn: 0.8673725\ttest: 0.8673725\ttest1: 1.0548068\tbest: 1.0548068 (1000)\ttotal: 1m 49s\tremaining: 16m 22s\n",
      "1100:\tlearn: 0.8501084\ttest: 0.8501084\ttest1: 1.0530216\tbest: 1.0529929 (1097)\ttotal: 1m 59s\tremaining: 16m 9s\n",
      "1200:\tlearn: 0.8328665\ttest: 0.8328665\ttest1: 1.0520166\tbest: 1.0520042 (1197)\ttotal: 2m 10s\tremaining: 15m 55s\n",
      "1300:\tlearn: 0.8153068\ttest: 0.8153068\ttest1: 1.0513720\tbest: 1.0513720 (1300)\ttotal: 2m 23s\tremaining: 15m 57s\n",
      "1400:\tlearn: 0.7991180\ttest: 0.7991180\ttest1: 1.0505906\tbest: 1.0505770 (1392)\ttotal: 2m 35s\tremaining: 15m 54s\n",
      "1500:\tlearn: 0.7842010\ttest: 0.7842010\ttest1: 1.0500120\tbest: 1.0499342 (1492)\ttotal: 2m 58s\tremaining: 16m 49s\n",
      "1600:\tlearn: 0.7686284\ttest: 0.7686284\ttest1: 1.0492261\tbest: 1.0492083 (1598)\ttotal: 3m 49s\tremaining: 20m 2s\n",
      "1700:\tlearn: 0.7538819\ttest: 0.7538819\ttest1: 1.0488942\tbest: 1.0488154 (1658)\ttotal: 4m 58s\tremaining: 24m 15s\n",
      "1800:\tlearn: 0.7396974\ttest: 0.7396974\ttest1: 1.0484190\tbest: 1.0484190 (1800)\ttotal: 5m 47s\tremaining: 26m 22s\n",
      "1900:\tlearn: 0.7258390\ttest: 0.7258390\ttest1: 1.0481682\tbest: 1.0481682 (1900)\ttotal: 6m 11s\tremaining: 26m 24s\n",
      "2000:\tlearn: 0.7124266\ttest: 0.7124266\ttest1: 1.0479743\tbest: 1.0479741 (1998)\ttotal: 6m 24s\tremaining: 25m 38s\n",
      "2100:\tlearn: 0.6990961\ttest: 0.6990961\ttest1: 1.0475894\tbest: 1.0475672 (2097)\ttotal: 6m 36s\tremaining: 24m 50s\n",
      "2200:\tlearn: 0.6872235\ttest: 0.6872235\ttest1: 1.0472744\tbest: 1.0472744 (2200)\ttotal: 6m 48s\tremaining: 24m 8s\n",
      "2300:\tlearn: 0.6746266\ttest: 0.6746266\ttest1: 1.0469719\tbest: 1.0469719 (2300)\ttotal: 7m\tremaining: 23m 26s\n",
      "2400:\tlearn: 0.6626464\ttest: 0.6626464\ttest1: 1.0468604\tbest: 1.0467216 (2345)\ttotal: 7m 10s\tremaining: 22m 43s\n",
      "2500:\tlearn: 0.6505081\ttest: 0.6505081\ttest1: 1.0468936\tbest: 1.0467216 (2345)\ttotal: 7m 23s\tremaining: 22m 9s\n",
      "2600:\tlearn: 0.6389848\ttest: 0.6389848\ttest1: 1.0465818\tbest: 1.0465618 (2594)\ttotal: 7m 34s\tremaining: 21m 32s\n",
      "2700:\tlearn: 0.6274713\ttest: 0.6274713\ttest1: 1.0464095\tbest: 1.0463237 (2691)\ttotal: 7m 44s\tremaining: 20m 55s\n",
      "2800:\tlearn: 0.6162783\ttest: 0.6162783\ttest1: 1.0463654\tbest: 1.0462473 (2745)\ttotal: 7m 55s\tremaining: 20m 23s\n",
      "2900:\tlearn: 0.6052826\ttest: 0.6052826\ttest1: 1.0462626\tbest: 1.0462188 (2894)\ttotal: 8m 7s\tremaining: 19m 53s\n",
      "3000:\tlearn: 0.5947901\ttest: 0.5947901\ttest1: 1.0461880\tbest: 1.0461880 (3000)\ttotal: 8m 18s\tremaining: 19m 22s\n",
      "3100:\tlearn: 0.5844051\ttest: 0.5844051\ttest1: 1.0460791\tbest: 1.0460539 (3022)\ttotal: 8m 28s\tremaining: 18m 51s\n",
      "3200:\tlearn: 0.5735661\ttest: 0.5735661\ttest1: 1.0457905\tbest: 1.0457306 (3194)\ttotal: 8m 39s\tremaining: 18m 22s\n",
      "3300:\tlearn: 0.5631754\ttest: 0.5631754\ttest1: 1.0458863\tbest: 1.0457306 (3194)\ttotal: 8m 49s\tremaining: 17m 54s\n",
      "3400:\tlearn: 0.5531507\ttest: 0.5531507\ttest1: 1.0457361\tbest: 1.0457060 (3366)\ttotal: 9m\tremaining: 17m 27s\n",
      "3500:\tlearn: 0.5435432\ttest: 0.5435432\ttest1: 1.0456988\tbest: 1.0456510 (3469)\ttotal: 9m 11s\tremaining: 17m 3s\n",
      "3600:\tlearn: 0.5340554\ttest: 0.5340554\ttest1: 1.0458495\tbest: 1.0456510 (3469)\ttotal: 9m 22s\tremaining: 16m 38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700:\tlearn: 0.5248054\ttest: 0.5248054\ttest1: 1.0459560\tbest: 1.0456510 (3469)\ttotal: 9m 32s\tremaining: 16m 14s\n",
      "3800:\tlearn: 0.5156341\ttest: 0.5156341\ttest1: 1.0460069\tbest: 1.0456510 (3469)\ttotal: 9m 42s\tremaining: 15m 50s\n",
      "3900:\tlearn: 0.5067766\ttest: 0.5067766\ttest1: 1.0458173\tbest: 1.0456510 (3469)\ttotal: 9m 53s\tremaining: 15m 28s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 1.045650966\n",
      "bestIteration = 3469\n",
      "\n",
      "Shrink model to first 3470 iterations.\n",
      "\n",
      "y_tr distribution: Counter({4.0: 3358, 2.0: 3230, 3.0: 2607, 1.0: 2472, 0.0: 328})\n",
      "training CATBOOST:\n",
      "0:\tlearn: 1.1755580\ttest: 1.1755580\ttest1: 1.1756849\tbest: 1.1756849 (0)\ttotal: 113ms\tremaining: 18m 51s\n",
      "100:\tlearn: 1.0809374\ttest: 1.0809374\ttest1: 1.1028394\tbest: 1.1028394 (100)\ttotal: 10.7s\tremaining: 17m 31s\n",
      "200:\tlearn: 1.0351906\ttest: 1.0351906\ttest1: 1.0799425\tbest: 1.0799425 (200)\ttotal: 21.6s\tremaining: 17m 30s\n",
      "300:\tlearn: 1.0041350\ttest: 1.0041350\ttest1: 1.0699397\tbest: 1.0699397 (300)\ttotal: 31.2s\tremaining: 16m 46s\n",
      "400:\tlearn: 0.9795621\ttest: 0.9795621\ttest1: 1.0634130\tbest: 1.0634130 (400)\ttotal: 40.9s\tremaining: 16m 19s\n",
      "500:\tlearn: 0.9586298\ttest: 0.9586298\ttest1: 1.0587604\tbest: 1.0587604 (500)\ttotal: 51.8s\tremaining: 16m 21s\n",
      "600:\tlearn: 0.9400319\ttest: 0.9400319\ttest1: 1.0550204\tbest: 1.0550204 (600)\ttotal: 1m 2s\tremaining: 16m 22s\n",
      "700:\tlearn: 0.9222399\ttest: 0.9222399\ttest1: 1.0524818\tbest: 1.0524818 (700)\ttotal: 1m 14s\tremaining: 16m 22s\n",
      "800:\tlearn: 0.9048683\ttest: 0.9048683\ttest1: 1.0501421\tbest: 1.0501421 (800)\ttotal: 1m 24s\tremaining: 16m 13s\n",
      "900:\tlearn: 0.8890410\ttest: 0.8890410\ttest1: 1.0478027\tbest: 1.0478027 (900)\ttotal: 1m 35s\tremaining: 16m 1s\n",
      "1000:\tlearn: 0.8727273\ttest: 0.8727273\ttest1: 1.0456439\tbest: 1.0456439 (1000)\ttotal: 1m 45s\tremaining: 15m 49s\n",
      "1100:\tlearn: 0.8572901\ttest: 0.8572901\ttest1: 1.0439270\tbest: 1.0439270 (1100)\ttotal: 1m 56s\tremaining: 15m 42s\n",
      "1200:\tlearn: 0.8408237\ttest: 0.8408237\ttest1: 1.0417429\tbest: 1.0417429 (1200)\ttotal: 2m 7s\tremaining: 15m 30s\n",
      "1300:\tlearn: 0.8242388\ttest: 0.8242388\ttest1: 1.0402760\tbest: 1.0402760 (1300)\ttotal: 2m 17s\tremaining: 15m 19s\n",
      "1400:\tlearn: 0.8091112\ttest: 0.8091112\ttest1: 1.0389806\tbest: 1.0389747 (1399)\ttotal: 2m 27s\tremaining: 15m 7s\n",
      "1500:\tlearn: 0.7941463\ttest: 0.7941463\ttest1: 1.0379274\tbest: 1.0379274 (1500)\ttotal: 2m 38s\tremaining: 14m 55s\n",
      "1600:\tlearn: 0.7784840\ttest: 0.7784840\ttest1: 1.0370700\tbest: 1.0370641 (1598)\ttotal: 2m 49s\tremaining: 14m 47s\n",
      "1700:\tlearn: 0.7638599\ttest: 0.7638599\ttest1: 1.0367198\tbest: 1.0366837 (1694)\ttotal: 2m 59s\tremaining: 14m 37s\n",
      "1800:\tlearn: 0.7499553\ttest: 0.7499553\ttest1: 1.0361252\tbest: 1.0361252 (1800)\ttotal: 3m 9s\tremaining: 14m 22s\n",
      "1900:\tlearn: 0.7369017\ttest: 0.7369017\ttest1: 1.0352874\tbest: 1.0352766 (1883)\ttotal: 3m 19s\tremaining: 14m 8s\n",
      "2000:\tlearn: 0.7238239\ttest: 0.7238239\ttest1: 1.0345682\tbest: 1.0345682 (2000)\ttotal: 3m 28s\tremaining: 13m 54s\n",
      "2100:\tlearn: 0.7116671\ttest: 0.7116671\ttest1: 1.0341097\tbest: 1.0340780 (2086)\ttotal: 3m 38s\tremaining: 13m 41s\n",
      "2200:\tlearn: 0.6988594\ttest: 0.6988594\ttest1: 1.0335491\tbest: 1.0335491 (2200)\ttotal: 3m 48s\tremaining: 13m 28s\n",
      "2300:\tlearn: 0.6871740\ttest: 0.6871740\ttest1: 1.0331847\tbest: 1.0331637 (2299)\ttotal: 3m 57s\tremaining: 13m 15s\n",
      "2400:\tlearn: 0.6744814\ttest: 0.6744814\ttest1: 1.0329417\tbest: 1.0328707 (2394)\ttotal: 4m 7s\tremaining: 13m 4s\n",
      "2500:\tlearn: 0.6634693\ttest: 0.6634693\ttest1: 1.0326424\tbest: 1.0326323 (2497)\ttotal: 4m 18s\tremaining: 12m 53s\n",
      "2600:\tlearn: 0.6522452\ttest: 0.6522452\ttest1: 1.0323939\tbest: 1.0323760 (2555)\ttotal: 4m 27s\tremaining: 12m 41s\n",
      "2700:\tlearn: 0.6412291\ttest: 0.6412291\ttest1: 1.0321407\tbest: 1.0320223 (2678)\ttotal: 4m 45s\tremaining: 12m 51s\n",
      "2800:\tlearn: 0.6304122\ttest: 0.6304122\ttest1: 1.0319332\tbest: 1.0319180 (2799)\ttotal: 4m 58s\tremaining: 12m 46s\n",
      "2900:\tlearn: 0.6198215\ttest: 0.6198215\ttest1: 1.0315944\tbest: 1.0315944 (2900)\ttotal: 5m 9s\tremaining: 12m 36s\n",
      "3000:\tlearn: 0.6087148\ttest: 0.6087148\ttest1: 1.0316006\tbest: 1.0313392 (2942)\ttotal: 5m 21s\tremaining: 12m 30s\n",
      "3100:\tlearn: 0.5983199\ttest: 0.5983199\ttest1: 1.0313649\tbest: 1.0313342 (3089)\ttotal: 5m 32s\tremaining: 12m 20s\n",
      "3200:\tlearn: 0.5879976\ttest: 0.5879976\ttest1: 1.0312728\tbest: 1.0312512 (3196)\ttotal: 5m 42s\tremaining: 12m 7s\n",
      "3300:\tlearn: 0.5781398\ttest: 0.5781398\ttest1: 1.0311652\tbest: 1.0311299 (3292)\ttotal: 5m 53s\tremaining: 11m 56s\n",
      "3400:\tlearn: 0.5681498\ttest: 0.5681498\ttest1: 1.0312693\tbest: 1.0311299 (3292)\ttotal: 6m 4s\tremaining: 11m 47s\n",
      "3500:\tlearn: 0.5587893\ttest: 0.5587893\ttest1: 1.0311938\tbest: 1.0311298 (3468)\ttotal: 6m 14s\tremaining: 11m 34s\n",
      "3600:\tlearn: 0.5498613\ttest: 0.5498613\ttest1: 1.0309861\tbest: 1.0309834 (3597)\ttotal: 6m 24s\tremaining: 11m 22s\n",
      "3700:\tlearn: 0.5412428\ttest: 0.5412428\ttest1: 1.0308647\tbest: 1.0308633 (3694)\ttotal: 6m 33s\tremaining: 11m 10s\n",
      "3800:\tlearn: 0.5323246\ttest: 0.5323246\ttest1: 1.0308635\tbest: 1.0308541 (3799)\ttotal: 6m 43s\tremaining: 10m 58s\n",
      "3900:\tlearn: 0.5241726\ttest: 0.5241726\ttest1: 1.0308954\tbest: 1.0308541 (3799)\ttotal: 6m 53s\tremaining: 10m 45s\n",
      "4000:\tlearn: 0.5154194\ttest: 0.5154194\ttest1: 1.0308699\tbest: 1.0307660 (3974)\ttotal: 7m 2s\tremaining: 10m 33s\n",
      "4100:\tlearn: 0.5069412\ttest: 0.5069412\ttest1: 1.0308177\tbest: 1.0307660 (3974)\ttotal: 7m 12s\tremaining: 10m 22s\n",
      "4200:\tlearn: 0.4984935\ttest: 0.4984935\ttest1: 1.0306614\tbest: 1.0306017 (4182)\ttotal: 7m 22s\tremaining: 10m 10s\n",
      "4300:\tlearn: 0.4901191\ttest: 0.4901191\ttest1: 1.0306019\tbest: 1.0305523 (4294)\ttotal: 7m 31s\tremaining: 9m 58s\n",
      "4400:\tlearn: 0.4821220\ttest: 0.4821220\ttest1: 1.0305815\tbest: 1.0304976 (4378)\ttotal: 7m 41s\tremaining: 9m 47s\n",
      "4500:\tlearn: 0.4737731\ttest: 0.4737731\ttest1: 1.0306780\tbest: 1.0304976 (4378)\ttotal: 7m 51s\tremaining: 9m 35s\n",
      "4600:\tlearn: 0.4655277\ttest: 0.4655277\ttest1: 1.0307581\tbest: 1.0304976 (4378)\ttotal: 8m\tremaining: 9m 24s\n",
      "4700:\tlearn: 0.4580429\ttest: 0.4580429\ttest1: 1.0308835\tbest: 1.0304976 (4378)\ttotal: 8m 10s\tremaining: 9m 12s\n",
      "4800:\tlearn: 0.4506683\ttest: 0.4506683\ttest1: 1.0307172\tbest: 1.0304976 (4378)\ttotal: 8m 20s\tremaining: 9m 1s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 1.030497552\n",
      "bestIteration = 4378\n",
      "\n",
      "Shrink model to first 4379 iterations.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_splits, random_state=1337,shuffle=True)\n",
    "\n",
    "oof_train = np.zeros((X_train.shape[0])) \n",
    "oof_test = np.zeros((X_test.shape[0], n_splits))\n",
    "\n",
    "i = 0\n",
    "for train_index, valid_index in kfold.split(X_train, X_train['AdoptionSpeed'].values):\n",
    "    X_tr = X_train.iloc[train_index, :] \n",
    "    X_val = X_train.iloc[valid_index, :]\n",
    "\n",
    "    y_tr = X_tr['AdoptionSpeed'].values\n",
    "    X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "    y_val = X_val['AdoptionSpeed'].values\n",
    "    X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n",
    "\n",
    "    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n",
    "\n",
    "    d_train = cat.Pool(X_tr, label=y_tr) \n",
    "    d_valid = cat.Pool(X_val, label=y_val) \n",
    "    watchlist = [d_train, d_valid]\n",
    "\n",
    "    print('training CATBOOST:')\n",
    "    model = cat.train(params=param,\n",
    "                      dtrain=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      eval_set=watchlist,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "\n",
    "    val_pred = model.predict(X_val)\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    oof_train[valid_index] = val_pred \n",
    "    oof_test[:, i] = test_pred\n",
    "\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.31116698 2.37225324 2.59317579 ... 2.27056177 2.82209911 2.46547447]\n"
     ]
    }
   ],
   "source": [
    "print(oof_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   6.,    7.,   52.,  620., 2662., 4335., 3660., 2382., 1076.,\n",
       "         193.]),\n",
       " array([0.02366823, 0.43542916, 0.84719009, 1.25895102, 1.67071195,\n",
       "        2.08247287, 2.4942338 , 2.90599473, 3.31775566, 3.72951659,\n",
       "        4.14127752]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Distribution of Trained Set\n",
    "plt.hist(oof_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Counts =  Counter({4.0: 4197, 2.0: 4037, 3.0: 3259, 1.0: 3090, 0.0: 410})\n",
      "Predicted Counts =  Counter({2.0: 4469, 4.0: 3752, 3.0: 3474, 1.0: 3292, 0.0: 6})\n",
      "Coefficients =  [0.45976316 2.07765722 2.50284342 2.8898671 ]\n",
      "QWK =  0.45584146703935413\n"
     ]
    }
   ],
   "source": [
    "#Compute Quadratic Weighted Kappa based on OOF train predictions: \n",
    "\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(oof_train, X_train['AdoptionSpeed'].values) \n",
    "coefficients = optR.coefficients()\n",
    "pred_test_y_k = optR.predict(oof_train, coefficients)\n",
    "print(\"\\nValid Counts = \", Counter(X_train['AdoptionSpeed'].values)) \n",
    "print(\"Predicted Counts = \", Counter(pred_test_y_k)) \n",
    "print(\"Coefficients = \", coefficients)\n",
    "qwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\n",
    "print(\"QWK = \", qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pred distribution: Counter({2: 4469, 4: 3752, 3: 3474, 1: 3292, 0: 6})\n",
      "test pred distribution: Counter({2.0: 1133, 4.0: 1077, 3.0: 953, 1.0: 809})\n"
     ]
    }
   ],
   "source": [
    "coefficients_ = coefficients.copy()\n",
    "train_predictions = optR.predict(oof_train, coefficients_).astype(int) \n",
    "print('train pred distribution: {}'.format(Counter(train_predictions)))\n",
    "\n",
    "test_predictions = optR.predict(oof_test.mean(axis=1), coefficients_) \n",
    "print('test pred distribution: {}'.format(Counter(test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Distribution:\n",
      "0.0    0.027346\n",
      "1.0    0.206096\n",
      "2.0    0.269259\n",
      "3.0    0.217368\n",
      "4.0    0.279931\n",
      "Name: AdoptionSpeed, dtype: float64\n",
      "\n",
      "Train Predicted Distribution:\n",
      "0    0.000400\n",
      "1    0.219569\n",
      "2    0.298072\n",
      "3    0.231708\n",
      "4    0.250250\n",
      "dtype: float64\n",
      "\n",
      "Test Predicted Distribution:\n",
      "1.0    0.203676\n",
      "2.0    0.285247\n",
      "3.0    0.239930\n",
      "4.0    0.271148\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\n",
    "print(\"\\nTrain Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())\n",
    "print(\"\\nTest Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pred distribution: Counter({4: 4118, 2: 4094, 3: 3108, 1: 3076, 0: 597})\n",
      "test pred distribution: Counter({4.0: 1190, 2.0: 1037, 3.0: 840, 1.0: 804, 0.0: 101})\n"
     ]
    }
   ],
   "source": [
    "#Manually adjusted coefficients: \n",
    "\n",
    "coefficients_ = coefficients.copy()\n",
    "\n",
    "coefficients_[0]=1.645\n",
    "coefficients_[1]=2.115\n",
    "coefficients_[3]=2.84\n",
    "\n",
    "train_predictions = optR.predict(oof_train, coefficients_).astype(int) \n",
    "print('train pred distribution: {}'.format(Counter(train_predictions)))\n",
    "\n",
    "test_predictions = optR.predict(oof_test.mean(axis=1), coefficients_) \n",
    "print('test pred distribution: {}'.format(Counter(test_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Distribution:\n",
      "0.0    0.027346\n",
      "1.0    0.206096\n",
      "2.0    0.269259\n",
      "3.0    0.217368\n",
      "4.0    0.279931\n",
      "Name: AdoptionSpeed, dtype: float64\n",
      "\n",
      "Train Predicted Distribution:\n",
      "0    0.039819\n",
      "1    0.205162\n",
      "2    0.273061\n",
      "3    0.207297\n",
      "4    0.274662\n",
      "dtype: float64\n",
      "\n",
      "Test Predicted Distribution:\n",
      "0.0    0.025428\n",
      "1.0    0.202417\n",
      "2.0    0.261078\n",
      "3.0    0.211480\n",
      "4.0    0.299597\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Distribution inspection of original target and predicted train and test:\n",
    "\n",
    "print(\"True Distribution:\")\n",
    "print(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\n",
    "print(\"\\nTrain Predicted Distribution:\")\n",
    "print(pd.value_counts(train_predictions, normalize=True).sort_index())\n",
    "print(\"\\nTest Predicted Distribution:\")\n",
    "print(pd.value_counts(test_predictions, normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission:\n",
    "\n",
    "submission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\n",
    "submission.head()\n",
    "submission.to_csv('submissioncat.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
